diff --git a/.env.example b/.env.example
deleted file mode 100644
index 1f97ebf..0000000
--- a/.env.example
+++ /dev/null
@@ -1,38 +0,0 @@
-# Broker
-ALPACA_API_KEY="PK64Y4IYEKAOU67MCSAGYESYFO"
-ALPACA_API_SECRET="A3G3nwAWAjTHWox5mRYN3RYv2dgL1KjdZpFMpsH8cJdS"
-ALPACA_BASE_URL=https://paper-api.alpaca.markets
-
-# Azure Storage (choose one)
-AZURE_STORAGE_CONNECTION_STRING=
-# or use MSI + container names
-AZURE_STORAGE_ACCOUNT=
-AZURE_STORAGE_CONTAINER_DATA=trader-data
-AZURE_STORAGE_CONTAINER_MODELS=trader-models
-
-# PostgreSQL (Azure Database for PostgreSQL – Flexible Server)
-PGHOST=
-PGPORT=5432
-PGDATABASE=trader
-PGUSER=
-PGPASSWORD=
-PGSSLMODE=require
-
-# Runtime
-TZ=America/Los_Angeles
-TRADING_ENABLED=true
-PAPER_TRADING=true
-
-# Risk
-MAX_RISK_PER_TRADE=0.01
-DAILY_DRAWDOWN_HALT=0.05
-CONCENTRATION_MANUAL_GATE=0.50
-
-# Watchlist thresholds
-PRICE_MIN=1.0
-PRICE_MAX=10.0
-GAP_MIN_PCT=5.0
-RVOL_MIN=3.0
-SPREAD_MAX_PCT_PRE=0.75
-DOLLAR_VOL_MIN_PRE=1000000
-MAX_WATCHLIST=15
\ No newline at end of file
diff --git a/.gitignore b/.gitignore
index 672dabe..30db6d4 100644
--- a/.gitignore
+++ b/.gitignore
@@ -27,3 +27,7 @@ data/
 *.csv
 *.zip
 *.tar.gz
+
+# Runtime artefacts
+ai-trader-logs/
+*.log
diff --git a/.tool-versions b/.tool-versions
index 234139b..d3dfd03 100644
--- a/.tool-versions
+++ b/.tool-versions
@@ -1 +1 @@
-python 3.13
+python-3.13.9
diff --git a/README.md b/README.md
index 98ec039..c548f33 100644
--- a/README.md
+++ b/README.md
@@ -20,6 +20,7 @@
 - **Trading Agent Suite:** Policy, sizing, execution, and journaling agents with PDT & drawdown gates.
 - **Backtesting:** Breakout strategy engine with metrics, CSV exports, and debug snapshots.
 - **Integrations:** Alpaca market/execution data, PostgreSQL persistence, Azure Blob storage, Telegram bot.
+- **Probabilistic Market Data Layer:** Unified DAL that normalizes Alpaca, Alpha Vantage, and Finnhub feeds (HTTP + WebSocket) and emits Kalman-filtered probabilistic signals with parquet/Postgres persistence.
 
 ## Architecture
 
@@ -55,7 +56,7 @@ All modules are import-safe, follow snake_case for files/functions, and use stru
    export PYTHONPATH=.
    ```
 
-3. Create a `.env` file in the repo root with broker, storage, database, and Telegram credentials (see `app/config.py` for required keys). Never check secrets into source control.
+3. Create a `.env` file in the repo root with broker, storage, database, Telegram, and market data credentials (see `app/settings.py`). Never check secrets into source control.
 4. Launch the FastAPI app locally:
 
    ```bash
@@ -83,6 +84,8 @@ python3 -m app.backtest.run_breakout --symbol AAPL --start 2021-01-01 --debug
 # optional
 #   --min-notional <USD>
 #   --debug-entries   # emit CSV snapshots for inspection
+#   --use-probabilistic --dal-vendor finnhub --regime-aware-sizing
+#     # pull MarketDataDAL signals/regimes and scale risk via latest regime snapshot
 ```
 
 ## Operations & Observability
@@ -109,6 +112,40 @@ Logs rotate daily and retain seven days by default.
        -d '{"url":"<APP_SERVICE_URL>/telegram/webhook","secret_token":"<TELEGRAM_WEBHOOK_SECRET>"}'
   ```
 
+### Market Data DAL
+
+The probabilistic data abstraction layer (`app/dal/`) consolidates vendor access, Kalman filtering, caching, and streaming:
+
+- **Vendors:** Alpaca (HTTP + streaming), Alpha Vantage (HTTP), Finnhub (HTTP + streaming). Additional vendors plug in via the `VendorClient` interface.
+- **Normalization:** `Bars`/`SignalFrame` schemas enforce UTC timestamps, corporate-action aware pricing, and deterministic replay.
+- **Probabilistic signals:** Each fetch/stream run produces Kalman-filtered price, velocity, and state uncertainty.
+- **Persistence:** Parquet snapshots stored under `artifacts/marketdata/cache/` (configurable) with optional metadata in Postgres (`market_data_snapshots`).
+- **Streaming manager:** Converts Alpaca/Finnhub WebSocket ticks into normalized frames with automatic gap backfill via HTTP.
+
+Set the following environment variables to activate non-Alpaca vendors:
+
+```bash
+ALPHAVANTAGE_API_KEY=...   # retrieved from https://www.alphavantage.co/
+FINNHUB_API_KEY=...        # retrieved from https://finnhub.io/
+```
+
+Instantiate the DAL from your module or notebook:
+
+```python
+from app.dal.manager import MarketDataDAL
+
+dal = MarketDataDAL()
+batch = dal.fetch_bars("AAPL", interval="1Min", vendor="finnhub",
+                       start=..., end=...)
+print(batch.bars.symbol, len(batch.signals), batch.regimes[-1].regime)
+
+# async streaming example
+async for payload in dal.stream_bars(["AAPL", "MSFT"], vendor="alpaca"):
+    print(payload.signal.price, payload.regime.regime)
+```
+
+Unit/regression tests covering the DAL live under `tests/dal/`. Running `pytest -q` exercises both the fetch and streaming pipelines (using stub vendors for deterministic behavior).
+
 ## Additional Docs
 
 - `ARCHITECTURE.md` — design deep dive
diff --git a/ai-trader-logs b/ai-trader-logs
new file mode 100644
index 0000000..8137037
--- /dev/null
+++ b/ai-trader-logs
@@ -0,0 +1,122 @@
+2025-11-04 09:42:54.221 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage exception boom; retrying in 0.00s (1/2)
+2025-11-04 09:42:54.221 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage -> 429 retrying in 0.00s (2/2)
+2025-11-04 09:42:54.234 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 09:42:54.234 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 09:42:54.239 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | HTTP GET https://example.com/test exception: boom; retrying in 0.0s (attempt 1/4)
+2025-11-04 09:42:54.360 | INFO     | req=919c3f86dc61499dacb47c1059ff6480 | env=test | ver=1.6.6 | sha=unknown | request method=GET path=/health/live status=200 duration_ms=1.06
+2025-11-04 09:42:54.366 | INFO     | req=850680ba56aa44689fcc1e59107feb3c | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=None (ignored)
+2025-11-04 09:42:54.366 | INFO     | req=850680ba56aa44689fcc1e59107feb3c | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=2.04
+2025-11-04 09:42:54.372 | INFO     | req=3f6d5edde522425f80f8fdba7cfd9120 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=999 (ignored)
+2025-11-04 09:42:54.373 | INFO     | req=3f6d5edde522425f80f8fdba7cfd9120 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=3.66
+2025-11-04 09:42:54.463 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo history fetch failed status=500 symbol=AAPL start=2024-01-01 end=2024-01-10 error=nope
+2025-11-04 09:42:54.465 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo request error attempt=1 url=https://example.com error=boom
+2025-11-04 09:42:54.466 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo throttled status=429 attempt=2 url=https://example.com
+2025-11-04 11:22:38.839 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage exception boom; retrying in 0.00s (1/2)
+2025-11-04 11:22:38.839 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage -> 429 retrying in 0.00s (2/2)
+2025-11-04 11:22:38.850 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 11:22:38.850 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 11:22:38.855 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | HTTP GET https://example.com/test exception: boom; retrying in 0.0s (attempt 1/4)
+2025-11-04 11:22:38.976 | INFO     | req=f9d7312d41b6476f9b06e286b414bd03 | env=test | ver=1.6.6 | sha=unknown | request method=GET path=/health/live status=200 duration_ms=1.07
+2025-11-04 11:22:38.981 | INFO     | req=99f397f1b3d347169de969822bf2355e | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=None (ignored)
+2025-11-04 11:22:38.982 | INFO     | req=99f397f1b3d347169de969822bf2355e | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=2.19
+2025-11-04 11:22:38.986 | INFO     | req=dd6c95d51c22440ba264b1be271d578b | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=999 (ignored)
+2025-11-04 11:22:38.987 | INFO     | req=dd6c95d51c22440ba264b1be271d578b | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.80
+2025-11-04 11:22:39.082 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo history fetch failed status=500 symbol=AAPL start=2024-01-01 end=2024-01-10 error=nope
+2025-11-04 11:22:39.084 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo request error attempt=1 url=https://example.com error=boom
+2025-11-04 11:22:39.084 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo throttled status=429 attempt=2 url=https://example.com
+2025-11-04 13:47:07.917 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage exception boom; retrying in 0.00s (1/2)
+2025-11-04 13:47:07.918 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage -> 429 retrying in 0.00s (2/2)
+2025-11-04 13:47:07.926 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 13:47:07.927 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 13:47:07.930 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | HTTP GET https://example.com/test exception: boom; retrying in 0.0s (attempt 1/4)
+2025-11-04 13:47:08.042 | INFO     | req=42a1ad60166a406b9f558d3112cb2aa0 | env=test | ver=1.6.6 | sha=unknown | request method=GET path=/health/live status=200 duration_ms=1.11
+2025-11-04 13:47:08.048 | INFO     | req=d5a315a2e25d410f9663925850e47318 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=None (ignored)
+2025-11-04 13:47:08.048 | INFO     | req=d5a315a2e25d410f9663925850e47318 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.13
+2025-11-04 13:47:08.051 | INFO     | req=6039a8a668e947838307842c22f61609 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=999 (ignored)
+2025-11-04 13:47:08.051 | INFO     | req=6039a8a668e947838307842c22f61609 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.01
+2025-11-04 13:47:08.147 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo history fetch failed status=500 symbol=AAPL start=2024-01-01 end=2024-01-10 error=nope
+2025-11-04 13:47:08.150 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo request error attempt=1 url=https://example.com error=boom
+2025-11-04 13:47:08.150 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo throttled status=429 attempt=2 url=https://example.com
+2025-11-04 14:17:51.207 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage exception boom; retrying in 0.00s (1/2)
+2025-11-04 14:17:51.207 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage -> 429 retrying in 0.00s (2/2)
+2025-11-04 14:17:53.049 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:17:53.049 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:17:53.053 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | HTTP GET https://example.com/test exception: boom; retrying in 0.0s (attempt 1/4)
+2025-11-04 14:17:53.171 | INFO     | req=d3738bc23c014a2793505465f6efd3af | env=test | ver=1.6.6 | sha=unknown | request method=GET path=/health/live status=200 duration_ms=1.06
+2025-11-04 14:17:53.176 | INFO     | req=510602ab74cf4434bc8882b5039840a8 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=None (ignored)
+2025-11-04 14:17:53.176 | INFO     | req=510602ab74cf4434bc8882b5039840a8 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.16
+2025-11-04 14:17:53.179 | INFO     | req=b7eefefa5b5b4a519d66886cf8220d39 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=999 (ignored)
+2025-11-04 14:17:53.180 | INFO     | req=b7eefefa5b5b4a519d66886cf8220d39 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=0.93
+2025-11-04 14:17:53.269 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo history fetch failed status=500 symbol=AAPL start=2024-01-01 end=2024-01-10 error=nope
+2025-11-04 14:17:53.271 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo request error attempt=1 url=https://example.com error=boom
+2025-11-04 14:17:53.271 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo throttled status=429 attempt=2 url=https://example.com
+2025-11-04 14:26:00.554 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage exception boom; retrying in 0.00s (1/2)
+2025-11-04 14:26:00.555 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage -> 429 retrying in 0.00s (2/2)
+2025-11-04 14:26:01.924 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:26:01.924 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:26:01.931 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | HTTP GET https://example.com/test exception: boom; retrying in 0.0s (attempt 1/4)
+2025-11-04 14:26:02.065 | INFO     | req=f8161d910c664c68847ad5962a70056b | env=test | ver=1.6.6 | sha=unknown | request method=GET path=/health/live status=200 duration_ms=1.04
+2025-11-04 14:26:02.071 | INFO     | req=a2f28bf84ce848628ff2f69f637fd144 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=None (ignored)
+2025-11-04 14:26:02.071 | INFO     | req=a2f28bf84ce848628ff2f69f637fd144 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.40
+2025-11-04 14:26:02.074 | INFO     | req=cb57567fb666409c887631db051fd486 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=999 (ignored)
+2025-11-04 14:26:02.074 | INFO     | req=cb57567fb666409c887631db051fd486 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=0.80
+2025-11-04 14:26:02.314 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo history fetch failed status=500 symbol=AAPL start=2024-01-01 end=2024-01-10 error=nope
+2025-11-04 14:26:02.316 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo request error attempt=1 url=https://example.com error=boom
+2025-11-04 14:26:02.316 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo throttled status=429 attempt=2 url=https://example.com
+2025-11-04 14:27:47.892 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage exception boom; retrying in 0.00s (1/2)
+2025-11-04 14:27:47.893 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage -> 429 retrying in 0.00s (2/2)
+2025-11-04 14:27:47.959 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:27:47.959 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:27:47.962 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | HTTP GET https://example.com/test exception: boom; retrying in 0.0s (attempt 1/4)
+2025-11-04 14:27:48.065 | INFO     | req=8b08b224fc4541ae9e9b72786ddd715c | env=test | ver=1.6.6 | sha=unknown | request method=GET path=/health/live status=200 duration_ms=1.05
+2025-11-04 14:27:48.070 | INFO     | req=0370b7f0cdb24c379b651294098c54cc | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=None (ignored)
+2025-11-04 14:27:48.070 | INFO     | req=0370b7f0cdb24c379b651294098c54cc | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.39
+2025-11-04 14:27:48.074 | INFO     | req=a55cbcd59d9048c9879a21e23af94795 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=999 (ignored)
+2025-11-04 14:27:48.074 | INFO     | req=a55cbcd59d9048c9879a21e23af94795 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=0.97
+2025-11-04 14:27:48.160 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo history fetch failed status=500 symbol=AAPL start=2024-01-01 end=2024-01-10 error=nope
+2025-11-04 14:27:48.163 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo request error attempt=1 url=https://example.com error=boom
+2025-11-04 14:27:48.163 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo throttled status=429 attempt=2 url=https://example.com
+2025-11-04 14:28:31.753 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage exception boom; retrying in 0.00s (1/2)
+2025-11-04 14:28:31.754 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage -> 429 retrying in 0.00s (2/2)
+2025-11-04 14:28:31.827 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:28:31.827 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:28:31.830 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | HTTP GET https://example.com/test exception: boom; retrying in 0.0s (attempt 1/4)
+2025-11-04 14:28:31.978 | INFO     | req=2b0c7a020ab04acfa6587afe980debad | env=test | ver=1.6.6 | sha=unknown | request method=GET path=/health/live status=200 duration_ms=0.82
+2025-11-04 14:28:31.983 | INFO     | req=1a687a3cbf2e4ad1816b5673bb92019c | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=None (ignored)
+2025-11-04 14:28:31.984 | INFO     | req=1a687a3cbf2e4ad1816b5673bb92019c | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.15
+2025-11-04 14:28:31.987 | INFO     | req=dd02595cae9e40a8bf7f7184c40adfbd | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=999 (ignored)
+2025-11-04 14:28:31.988 | INFO     | req=dd02595cae9e40a8bf7f7184c40adfbd | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.11
+2025-11-04 14:28:32.076 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo history fetch failed status=500 symbol=AAPL start=2024-01-01 end=2024-01-10 error=nope
+2025-11-04 14:28:32.078 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo request error attempt=1 url=https://example.com error=boom
+2025-11-04 14:28:32.079 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo throttled status=429 attempt=2 url=https://example.com
+2025-11-04 14:31:27.366 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage exception boom; retrying in 0.00s (1/2)
+2025-11-04 14:31:27.367 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | [Telegram] HTTP POST sendMessage -> 429 retrying in 0.00s (2/2)
+2025-11-04 14:31:27.420 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:31:27.420 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
+2025-11-04 14:31:27.423 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | HTTP GET https://example.com/test exception: boom; retrying in 0.0s (attempt 1/4)
+2025-11-04 14:31:27.525 | INFO     | req=b0e55fb03abe4bf6bb6508286e31d203 | env=test | ver=1.6.6 | sha=unknown | request method=GET path=/health/live status=200 duration_ms=0.88
+2025-11-04 14:31:27.532 | INFO     | req=a5694357abcd4de2b6348a8aabcfb4a9 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=None (ignored)
+2025-11-04 14:31:27.532 | INFO     | req=a5694357abcd4de2b6348a8aabcfb4a9 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.32
+2025-11-04 14:31:27.535 | INFO     | req=208554dbf983495eb3d2df828cd10db2 | env=test | ver=1.6.6 | sha=unknown | [tg] unauthorized user id=999 (ignored)
+2025-11-04 14:31:27.536 | INFO     | req=208554dbf983495eb3d2df828cd10db2 | env=test | ver=1.6.6 | sha=unknown | request method=POST path=/telegram/webhook status=200 duration_ms=1.07
+2025-11-04 14:31:27.627 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo history fetch failed status=500 symbol=AAPL start=2024-01-01 end=2024-01-10 error=nope
+2025-11-04 14:31:27.630 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo request error attempt=1 url=https://example.com error=boom
+2025-11-04 14:31:27.630 | WARNING  | req=- | env=test | ver=1.6.6 | sha=unknown | yahoo throttled status=429 attempt=2 url=https://example.com
+2025-11-04 15:30:06.118 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Fetching daily history for AAPL: 2021-01-01 → 2021-01-05
+2025-11-04 15:30:06.121 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Probabilistic DAL fetch ok vendor=hybrid interval=1Day bars=5 signals=5 regimes=5
+2025-11-04 15:30:06.130 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Regime-aware sizing applied: regime=trend_down uncertainty=0.0800 scale=0.420
+2025-11-04 15:30:06.131 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | [AAPL] equity metrics: {'sharpe': 1.0, 'sortino': 1.0}
+2025-11-04 15:30:06.131 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Skipping save of equity curve due to BACKTEST_NO_SAVE=1
+2025-11-04 15:30:06.133 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Fetching daily history for AAPL: 2021-01-01 → 2021-01-05
+2025-11-04 15:30:06.133 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Probabilistic DAL fetch ok vendor=hybrid interval=1Day bars=5 signals=5 regimes=5
+2025-11-04 15:30:06.135 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | [AAPL] equity metrics: {'sharpe': 1.0, 'sortino': 1.0}
+2025-11-04 15:30:06.136 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Skipping save of equity curve due to BACKTEST_NO_SAVE=1
+2025-11-04 15:30:14.573 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Fetching daily history for AAPL: 2021-01-01 → 2021-01-05
+2025-11-04 15:30:14.575 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Probabilistic DAL fetch ok vendor=hybrid interval=1Day bars=5 signals=5 regimes=5
+2025-11-04 15:30:14.578 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Regime-aware sizing applied: regime=trend_down uncertainty=0.0800 scale=0.420
+2025-11-04 15:30:14.578 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | [AAPL] equity metrics: {'sharpe': 1.0, 'sortino': 1.0}
+2025-11-04 15:30:14.578 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Skipping save of equity curve due to BACKTEST_NO_SAVE=1
+2025-11-04 15:30:14.579 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Fetching daily history for AAPL: 2021-01-01 → 2021-01-05
+2025-11-04 15:30:14.580 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Probabilistic DAL fetch ok vendor=hybrid interval=1Day bars=5 signals=5 regimes=5
+2025-11-04 15:30:14.581 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | [AAPL] equity metrics: {'sharpe': 1.0, 'sortino': 1.0}
+2025-11-04 15:30:14.581 | INFO     | req=- | env=test | ver=1.6.6 | sha=unknown | Skipping save of equity curve due to BACKTEST_NO_SAVE=1
diff --git a/ai-trader-logs/test.log b/ai-trader-logs/test.log
deleted file mode 100644
index 21a0bd9..0000000
--- a/ai-trader-logs/test.log
+++ /dev/null
@@ -1,28 +0,0 @@
-2025-11-01 19:15:06.425 | DEBUG    | app.backtest.metrics:equity_stats:149 - [metrics] 2024-01-01 00:00:00→2024-04-29 00:00:00 n=120 cagr=0.0000 tot=0.0000 vol=0.0000 sharpe=0.000 sortino=0.000 maxDD=0.0000 len=0 mar=0.000
-2025-11-01 19:15:06.427 | DEBUG    | app.backtest.metrics:equity_stats:149 - [metrics] 2024-01-01 00:00:00→2024-09-08 00:00:00 n=252 cagr=1.7459 tot=1.0020 vol=0.0138 sharpe=50.493 sortino=0.000 maxDD=0.0000 len=0 mar=0.000
-2025-11-01 19:15:06.428 | WARNING  | app.data.data_client:batch_latest_ohlcv:214 - alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
-2025-11-01 19:15:06.428 | WARNING  | app.data.data_client:batch_latest_ohlcv:214 - alpaca snapshots returned empty payload; falling back to Yahoo for 1 symbols
-2025-11-01 19:15:06.620 | INFO     | app.main:request_logging_middleware:71 - request method=GET path=/health/live status=200 duration_ms=1.63 request_id=237d42bb1c3a49dcaad2da675806dd60
-2025-11-01 19:15:06.623 | DEBUG    | app.adapters.notifiers.telegram:build_client_from_env:489 - [Telegram] Using FakeTelegramClient (test mode)
-2025-11-01 19:15:06.624 | DEBUG    | app.api.routes.telegram:_dump_webhook_debug:102 - [tg-webhook:enter] DEBUG
-  hdr_primary: <empty>
-  hdr_legacy : <empty>
-  env_secret : <empty>
-  test_mode  : True
-  allow_empty: True
-  ENV        : test
-  BOT_TOKEN? : True
-  PAYLOAD    : {"message":{"chat":{"id":1},"text":"/ping"}}
-2025-11-01 19:15:06.624 | INFO     | app.api.routes.telegram:webhook:422 - [tg] unauthorized user id=None (ignored)
-2025-11-01 19:15:06.624 | INFO     | app.main:request_logging_middleware:71 - request method=POST path=/telegram/webhook status=200 duration_ms=1.68 request_id=51d8ea35e09d4977a66112bf6cf6d80a
-2025-11-01 19:15:06.627 | DEBUG    | app.adapters.notifiers.telegram:build_client_from_env:489 - [Telegram] Using FakeTelegramClient (test mode)
-2025-11-01 19:15:06.627 | DEBUG    | app.api.routes.telegram:_dump_webhook_debug:102 - [tg-webhook:enter] DEBUG
-  hdr_primary: <empty>
-  hdr_legacy : <empty>
-  env_secret : lS**********************************************************TAGa
-  test_mode  : True
-  allow_empty: True
-  ENV        : test
-  BOT_TOKEN? : True
-  PAYLOAD    : {"update_id":1001,"message":{"message_id":111,"from":{"id":999,"is_bot":false,"first_name":"Test"},"chat":{"id":42,"type":"private"},"date":1700000000,"text":"/ping"}}
-2025-11-01 19:15:06.628 | INFO     | app.main:request_logging_middleware:71 - request method=POST path=/telegram/webhook status=200 duration_ms=1.29 request_id=3c5b414943824960af69dd69ffb39338
diff --git a/app/__init__.py b/app/__init__.py
index e049acb..78f23e7 100644
--- a/app/__init__.py
+++ b/app/__init__.py
@@ -1,7 +1,35 @@
+import os
+import logging
 from pathlib import Path
 
 from dotenv import load_dotenv
+import sentry_sdk
+from sentry_sdk.integrations.fastapi import FastApiIntegration
+from sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration
+from sentry_sdk.integrations.logging import LoggingIntegration
 
 __version__ = "1.6.6"
 
+# Load environment variables early so SENTRY_DSN is available for local/dev runs
 load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")
+
+# Initialize Sentry as early as possible (only if DSN is provided)
+_dsn = os.getenv("SENTRY_DSN")
+if _dsn:
+    sentry_sdk.init(
+        dsn=_dsn,
+        integrations=[
+            FastApiIntegration(),
+            SqlalchemyIntegration(),
+            LoggingIntegration(level=logging.INFO, event_level=logging.ERROR),
+        ],
+        traces_sample_rate=1.0,
+        profiles_sample_rate=0.0,
+        environment=os.getenv("APP_ENV", "prod"),
+        release=os.getenv("APP_VERSION", "unknown"),
+    )
+else:
+    logging.getLogger(__name__).info("Sentry DSN not set; Sentry disabled")
+
+from loguru import logger
+logger.info("startup: log pipeline ready")
diff --git a/app/adapters/db/postgres.py b/app/adapters/db/postgres.py
index 5b644b6..cc3ddf2 100644
--- a/app/adapters/db/postgres.py
+++ b/app/adapters/db/postgres.py
@@ -5,8 +5,6 @@ Postgres engine/session helpers with safe DSN building, minimal logging, and
 resilient health checks. Prefers DATABASE_URL when set; otherwise builds from
 individual PG* env vars with sslmode=require by default (works for Azure FS).
 """
-
-import os
 import time
 import contextlib
 from typing import Optional
@@ -17,7 +15,7 @@ from sqlalchemy import create_engine, text
 from sqlalchemy.engine import Engine
 from sqlalchemy.orm import Session, sessionmaker, declarative_base
 
-from app.utils import env as ENV
+from app.settings import get_database_settings
 
 Base = declarative_base()
 
@@ -33,8 +31,7 @@ def get_db_url() -> Optional[str]:
     Returns:
         Optional[str]: The database DSN if found, otherwise None.
     """
-    # Prefer production DSN; fall back to TEST_DATABASE_URL (e.g., CI) if present
-    return os.getenv("DATABASE_URL") or os.getenv("TEST_DATABASE_URL")
+    return get_database_settings().primary_dsn
 
 
 def _dsn_from_env() -> Optional[str]:
@@ -42,8 +39,7 @@ def _dsn_from_env() -> Optional[str]:
     DEPRECATED: Use get_db_url() instead.
     Retrieves the database DSN from environment variables.
     """
-    # Prefer production DSN; fall back to TEST_DATABASE_URL (e.g., CI) if present
-    return os.getenv("DATABASE_URL") or os.getenv("TEST_DATABASE_URL")
+    return get_db_url()
 
 
 def _sanitize_dsn(dsn: str) -> str:
diff --git a/app/adapters/notifiers/telegram.py b/app/adapters/notifiers/telegram.py
index 33d243d..43feea8 100644
--- a/app/adapters/notifiers/telegram.py
+++ b/app/adapters/notifiers/telegram.py
@@ -6,8 +6,11 @@ from typing import Any, Dict, Iterable, Optional, Set, List
 
 import requests
 from loguru import logger
+from html import escape as html_escape
 
+from app.settings import get_telegram_settings
 from app.utils import env as ENV
+from app.utils.http import compute_backoff_delay
 
 API_BASE = "https://api.telegram.org"
 
@@ -32,6 +35,17 @@ def test_outbox_clear() -> None:
     _TEST_OUTBOX.clear()
 
 
+MDV2_SPECIAL = r"_*[]()~`>#+-=|{}.!"
+def escape_mdv2(text: str) -> str:
+    if not text:
+        return ""
+    # Escape backslash first, then specials
+    text = text.replace("\\", "\\\\")
+    for ch in MDV2_SPECIAL:
+        text = text.replace(ch, f"\\{ch}")
+    return text
+
+
 class FakeTelegramClient:
     """
     A test double for the TelegramClient.
@@ -58,24 +72,13 @@ class FakeTelegramClient:
     ) -> bool:
         """
         Sends a message, splitting it into chunks if necessary.
-
-        Args:
-            chat_id (int | str): The chat ID to send the message to.
-            text (str): The message text.
-            parse_mode (Optional[str]): The parse mode for the message.
-            mode (Optional[str]): The parse mode for the message.
-            chunk_size (int): The maximum chunk size.
-            retries (int): The number of retries.
-
-        Returns:
-            bool: True if the message was sent successfully, False otherwise.
         """
         for part in _split_chunks(text, limit=chunk_size):
             _TEST_OUTBOX.append(
                 {
                     "chat_id": chat_id,
                     "text": part,
-                    "parse_mode": parse_mode or mode or "Markdown",
+                    "parse_mode": parse_mode or mode or "MarkdownV2",
                 }
             )
         return True
@@ -201,7 +204,9 @@ class TelegramClient:
         bot_token: str,
         allowed_users: Set[int] | None = None,
         webhook_secret: str | None = None,
-        timeout: int = 10,
+        timeout: int | None = None,
+        retries: Optional[int] = None,
+        backoff: Optional[float] = None,
     ) -> None:
         """
         Initializes the TelegramClient.
@@ -211,11 +216,27 @@ class TelegramClient:
             allowed_users (Set[int] | None): A set of allowed user IDs.
             webhook_secret (str | None): The webhook secret.
             timeout (int): The request timeout.
+            retries (Optional[int]): Max retry attempts for outbound HTTP.
+            backoff (Optional[float]): Backoff factor between retries.
         """
         self.base = f"{API_BASE}/bot{bot_token}" if bot_token else ""
         self.allowed = allowed_users or set()
         self.secret = webhook_secret or ""
-        self.timeout = timeout
+        settings = get_telegram_settings()
+        default_timeout = (
+            timeout if timeout is not None else settings.timeout_secs
+        )
+        self.timeout = int(default_timeout)
+        self.retries = (
+            int(retries)
+            if retries is not None
+            else getattr(ENV, "HTTP_RETRIES", 2)
+        )
+        self.backoff = (
+            float(backoff)
+            if backoff is not None
+            else getattr(ENV, "HTTP_BACKOFF", 1.5)
+        )
 
     def is_allowed(self, chat_id: int | str) -> bool:
         """
@@ -250,20 +271,11 @@ class TelegramClient:
         self,
         chat_id: int | str,
         text: str,
-        parse_mode: Optional[str] = "Markdown",
+        parse_mode: Optional[str] = None,
         disable_preview: bool = True,
     ) -> bool:
         """
         Sends a text message.
-
-        Args:
-            chat_id (int | str): The chat ID to send the message to.
-            text (str): The message text.
-            parse_mode (Optional[str]): The parse mode for the message.
-            disable_preview (bool): Whether to disable the link preview.
-
-        Returns:
-            bool: True if the message was sent successfully, False otherwise.
         """
         return self._send(
             chat_id, text, parse_mode=parse_mode, disable_preview=disable_preview
@@ -274,17 +286,9 @@ class TelegramClient:
     ) -> bool:
         """
         Sends a Markdown message.
-
-        Args:
-            chat_id (int | str): The chat ID to send the message to.
-            text (str): The message text.
-            disable_preview (bool): Whether to disable the link preview.
-
-        Returns:
-            bool: True if the message was sent successfully, False otherwise.
         """
         return self._send(
-            chat_id, text, parse_mode="Markdown", disable_preview=disable_preview
+            chat_id, text, parse_mode="MarkdownV2", disable_preview=disable_preview
         )
 
     def send_html(
@@ -322,16 +326,20 @@ class TelegramClient:
         if not self.base:
             logger.warning("[Telegram] Missing bot token; skipping document send")
             return False
-        url = f"{self.base}/sendDocument"
         try:
             with open(file_path, "rb") as doc:
                 files = {"document": doc}
                 data: Dict[str, Any] = {"chat_id": _coerce_chat_id(chat_id)}
                 if caption:
                     data["caption"] = caption
-                resp = requests.post(
-                    url, data=data, files=files, timeout=self.timeout
+                resp = self._request(
+                    "POST",
+                    "sendDocument",
+                    data=data,
+                    files=files,
                 )
+            if not resp:
+                return False
             if 200 <= resp.status_code < 300:
                 logger.info(
                     "[Telegram] Document sent to {}: {}", chat_id, file_path
@@ -354,31 +362,21 @@ class TelegramClient:
         parse_mode: Optional[str] = None,
         mode: Optional[str] = None,
         chunk_size: int = 3500,
-        retries: int = 2,
+        retries: Optional[int] = None,
         **_ignore,
     ) -> bool:
         """
         Sends a long message in chunks.
-
-        Args:
-            chat_id (int | str): The chat ID to send the message to.
-            text (str): The message text.
-            parse_mode (Optional[str]): The parse mode for the message.
-            mode (Optional[str]): The parse mode for the message.
-            chunk_size (int): The maximum chunk size.
-            retries (int): The number of retries.
-
-        Returns:
-            bool: True if the message was sent successfully, False otherwise.
         """
-        eff_mode = parse_mode or mode or "Markdown"
+        eff_mode = parse_mode or mode or "MarkdownV2"
         ok = True
+        chunk_retries = max(0, self.retries if retries is None else int(retries))
         for part in _split_chunks(text, limit=chunk_size):
-            for attempt in range(retries + 1):
+            for attempt in range(chunk_retries + 1):
                 if self._send(chat_id, part, parse_mode=eff_mode):
                     break
-                if attempt < retries:
-                    delay = 1.5 * (attempt + 1)
+                if attempt < chunk_retries:
+                    delay = compute_backoff_delay(attempt, self.backoff, None)
                     logger.warning(
                         "[Telegram] Retry {} sending chunk to {} in {:.1f}s",
                         attempt + 1,
@@ -400,9 +398,10 @@ class TelegramClient:
         if not self.base:
             logger.warning("[Telegram] Missing bot token; cannot ping")
             return False
-        url = f"{self.base}/getMe"
         try:
-            resp = requests.get(url, timeout=self.timeout)
+            resp = self._request("GET", "getMe")
+            if not resp:
+                return False
             if 200 <= resp.status_code < 300:
                 data = (
                     resp.json()
@@ -447,7 +446,6 @@ class TelegramClient:
             return False
         if len(text) > 4096:
             text = text[:4096]
-        url = f"{self.base}/sendMessage"
         payload: Dict[str, Any] = {
             "chat_id": _coerce_chat_id(chat_id),
             "text": text,
@@ -456,27 +454,126 @@ class TelegramClient:
         if parse_mode:
             payload["parse_mode"] = parse_mode
         try:
-            resp = requests.post(url, json=payload, timeout=self.timeout)
-            if 200 <= resp.status_code < 300:
-                logger.debug(
-                    "[Telegram] Sent {} chars to {}", len(text), chat_id
-                )
-                return True
-            if resp.status_code == 429:
-                try:
-                    retry_after = int(resp.headers.get("Retry-After", "2"))
-                except Exception:
-                    retry_after = 2
-                logger.warning(
-                    "[Telegram] Rate-limited (429). Sleeping {}s", retry_after
-                )
-                time.sleep(retry_after)
+            resp = self._request("POST", "sendMessage", json=payload)
+            if resp is None:
                 return False
-            logger.warning("[Telegram] HTTP {}: {}", resp.status_code, resp.text)
+            # Non-2xx: log and bail (429 handled below)
+            if not (200 <= resp.status_code < 300):
+                if resp.status_code == 429:
+                    try:
+                        retry_after = int(resp.headers.get("Retry-After", "2"))
+                    except Exception:
+                        retry_after = 2
+                    logger.warning("[Telegram] Rate-limited (429). Sleeping {}s", retry_after)
+                    time.sleep(max(0, retry_after))
+                    return False
+                logger.warning("[Telegram] HTTP {}: {}", resp.status_code, resp.text)
+                return False
+            # 2xx but Telegram-level error (ok:false)
+            try:
+                if resp.headers.get("content-type", "").startswith("application/json"):
+                    body = resp.json() or {}
+                    if body.get("ok") is False:
+                        logger.warning(
+                            "[Telegram] API error {}: {}",
+                            body.get("error_code"),
+                            body.get("description"),
+                        )
+                        return False
+            except Exception:
+                pass
+            logger.debug("[Telegram] Sent {} chars to {}", len(text), chat_id)
+            return True
         except Exception as e:
             logger.error("[Telegram] Send error: {}", e)
         return False
 
+    def _request(
+        self,
+        method: str,
+        endpoint: str,
+        *,
+        retries: Optional[int] = None,
+        backoff: Optional[float] = None,
+        timeout: Optional[float] = None,
+        **kwargs,
+    ) -> Optional[requests.Response]:
+        if not self.base:
+            logger.warning("[Telegram] Missing bot token; cannot make request")
+            return None
+
+        attempt_retries = (
+            int(retries) if retries is not None else max(0, self.retries)
+        )
+        backoff_factor = (
+            float(backoff) if backoff is not None else float(self.backoff)
+        )
+        timeout_value = (
+            float(timeout) if timeout is not None else float(self.timeout)
+        )
+
+        url = f"{self.base}/{endpoint}"
+        last_exc: Exception | None = None
+
+        for attempt in range(attempt_retries + 1):
+            try:
+                resp = requests.request(
+                    method.upper(),
+                    url,
+                    timeout=timeout_value,
+                    **kwargs,
+                )
+            except requests.RequestException as exc:
+                last_exc = exc
+                if attempt < attempt_retries:
+                    delay = compute_backoff_delay(attempt, backoff_factor, None)
+                    logger.warning(
+                        "[Telegram] HTTP {} {} exception {}; retrying in {:.2f}s ({}/{})",
+                        method.upper(),
+                        endpoint,
+                        exc,
+                        delay,
+                        attempt + 1,
+                        attempt_retries,
+                    )
+                    time.sleep(delay)
+                    continue
+                logger.error(
+                    "[Telegram] HTTP {} {} failed after retries: {}",
+                    method.upper(),
+                    endpoint,
+                    exc,
+                )
+                return None
+
+            retryable = resp.status_code in {408, 425, 429, 500, 502, 503, 504}
+            if retryable and attempt < attempt_retries:
+                delay = compute_backoff_delay(
+                    attempt, backoff_factor, resp.headers.get("Retry-After")
+                )
+                logger.warning(
+                    "[Telegram] HTTP {} {} -> {} retrying in {:.2f}s ({}/{})",
+                    method.upper(),
+                    endpoint,
+                    resp.status_code,
+                    delay,
+                    attempt + 1,
+                    attempt_retries,
+                )
+                time.sleep(delay)
+                continue
+
+            return resp
+
+        if last_exc:
+            logger.error(
+                "[Telegram] HTTP {} {} exhausted retries: {}",
+                method.upper(),
+                endpoint,
+                last_exc,
+            )
+        return None
+
 
 def build_client_from_env() -> TelegramClient | FakeTelegramClient:
     """
@@ -485,14 +582,16 @@ def build_client_from_env() -> TelegramClient | FakeTelegramClient:
     Returns:
         TelegramClient | FakeTelegramClient: A Telegram client.
     """
-    if os.getenv("PYTEST_CURRENT_TEST") or os.getenv("TELEGRAM_FAKE") == "1":
-        logger.debug("[Telegram] Using FakeTelegramClient (test mode)")
+    settings = get_telegram_settings()
+
+    if os.getenv("PYTEST_CURRENT_TEST") or settings.fake_mode:
+        logger.info("[Telegram] Using FakeTelegramClient (test mode)")
         return FakeTelegramClient()
 
-    token = ENV.TELEGRAM_BOT_TOKEN
-    allowed = ENV.TELEGRAM_ALLOWED_USER_IDS
-    secret = ENV.TELEGRAM_WEBHOOK_SECRET
-    timeout = ENV.TELEGRAM_TIMEOUT_SECS
+    token = settings.bot_token or ""
+    allowed = set(settings.allowed_user_ids)
+    secret = settings.webhook_secret or ""
+    timeout = settings.timeout_secs
 
     if not token:
         logger.warning(
@@ -510,6 +609,8 @@ def build_client_from_env() -> TelegramClient | FakeTelegramClient:
         allowed_users=allowed,
         webhook_secret=secret,
         timeout=timeout,
+        retries=getattr(ENV, "HTTP_RETRIES", 2),
+        backoff=getattr(ENV, "HTTP_BACKOFF", 1.5),
     )
 
 
@@ -517,30 +618,44 @@ def format_watchlist_message(
     session: str, items: Iterable[dict], title: str = "AI Trader • Watchlist"
 ) -> str:
     """
-    Formats a watchlist message.
-
-    Args:
-        session (str): The trading session.
-        items (Iterable[dict]): A list of watchlist items.
-        title (str): The message title.
-
-    Returns:
-        str: The formatted message.
+    Formats a watchlist message for MarkdownV2, escaping dynamic fields.
     """
-    header = f"*{title}* — _{session}_\n"
+    header = f"*{escape_mdv2(title)}* — _{escape_mdv2(session)}_\n\n"
     lines = []
     for it in items:
-        sym = it.get("symbol", "?")
+        sym = escape_mdv2(str(it.get("symbol", "?")))
         last = it.get("last")
-        src = it.get("price_source", "")
+        src = escape_mdv2(str(it.get("price_source", "")))
         vol = (it.get("ohlcv") or {}).get("v")
         last_s = f"${last:,.2f}" if isinstance(last, (int, float)) and last else "$0.00"
         vol_s = f"{vol:,}" if isinstance(vol, int) and vol is not None else "0"
-        suffix = f"  `{src}`" if src else ""
+        suffix = f"  {src}" if src else ""
         lines.append(f"{sym:<6} {last_s:>10}  vol {vol_s}{suffix}")
     return header + ("\n".join(lines) if lines else "_No candidates._")
 
 
+# HTML-safe version for Telegram to avoid MarkdownV2 parsing issues
+def format_watchlist_message_html(
+    session: str, items: Iterable[dict], title: str = "AI Trader • Watchlist"
+) -> str:
+    """
+    HTML-safe version that uses <pre> to avoid MarkdownV2 entity parsing issues.
+    """
+    header = f"<b>{html_escape(title)}</b> — <i>{html_escape(session)}</i>\n"
+    lines: List[str] = []
+    for it in items:
+        sym = html_escape(str(it.get("symbol", "?")))
+        last = it.get("last")
+        src = html_escape(str(it.get("price_source", "")))
+        vol = (it.get("ohlcv") or {}).get("v")
+        last_s = f"${last:,.2f}" if isinstance(last, (int, float)) and last is not None else "$0.00"
+        vol_s = f"{vol:,}" if isinstance(vol, int) and vol is not None else "0"
+        suffix = f"  {src}" if src else ""
+        lines.append(f"{sym:<6} {last_s:>10}  vol {vol_s}{suffix}")
+    body = "\n".join(lines) if lines else "No candidates."
+    return header + "<pre>" + html_escape(body) + "</pre>"
+
+
 def send_watchlist(
     session: str,
     items: Iterable[dict],
@@ -571,5 +686,6 @@ def send_watchlist(
         )
         return False
     client = build_client_from_env()
-    msg = format_watchlist_message(session, items, title=title)
-    return client.smart_send(target, msg, mode="Markdown", chunk_size=3500, retries=2)
+    # Use HTML + &lt;pre&gt; to avoid MarkdownV2 parse errors on decimals and punctuation.
+    msg = format_watchlist_message_html(session, items, title=title)
+    return client.smart_send(target, msg, mode="HTML", chunk_size=3500, retries=2)
diff --git a/app/adapters/telemetry/__init__.py b/app/adapters/telemetry/__init__.py
deleted file mode 100644
index d0d65e9..0000000
--- a/app/adapters/telemetry/__init__.py
+++ /dev/null
@@ -1,49 +0,0 @@
-"""
-Telemetry adapters (logging, metrics, tracing).
-
-Provides:
-- setup_logging()
-- logger
-- setup_tracing(service_name: str)
-"""
-
-import os
-import sys
-
-from loguru import logger
-
-from .loguru import PropagateHandler
-
-
-# --- Logging setup ---
-
-
-def setup_logging() -> None:
-    """Initialize application-wide logging with level from LOG_LEVEL env var and detailed format including module and line number."""
-    level = os.getenv("LOG_LEVEL", "INFO")
-    logger.remove()
-    logger.add(sys.stdout, colorize=True, level=level.upper())
-    logger.add(PropagateHandler(), format="{message}")
-
-
-# --- Tracing (future integration placeholder) ---
-
-
-def setup_tracing(service_name: str) -> None:
-    """
-    Stub for OpenTelemetry tracing integration.
-    Future: configure OTLP exporter to Azure Monitor or Grafana Cloud.
-    """
-    os.environ.setdefault("AI_TRADER_OTEL_SERVICE_NAME", service_name)
-    logger.info(f"Tracing initialized for {service_name}")
-    # To extend: integrate OpenTelemetry SDK here, e.g.,
-    # from opentelemetry import trace
-    # from opentelemetry.sdk.trace import TracerProvider
-    # from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter
-    # trace.set_tracer_provider(TracerProvider())
-    # tracer = trace.get_tracer(__name__)
-    # span_processor = BatchSpanProcessor(ConsoleSpanExporter())
-    # trace.get_tracer_provider().add_span_processor(span_processor)
-
-
-__all__ = ["setup_logging", "logger", "setup_tracing"]
diff --git a/app/adapters/telemetry/logging.py b/app/adapters/telemetry/logging.py
deleted file mode 100644
index bd5c371..0000000
--- a/app/adapters/telemetry/logging.py
+++ /dev/null
@@ -1,24 +0,0 @@
-import logging
-from typing import Optional
-
-from .loguru import configure_loguru
-
-
-def configure_logging(
-    level: Optional[str] = None, json: Optional[bool] = None
-) -> logging.Logger:
-    """
-    Configures process-wide logging.
-
-    Args:
-        level (Optional[str]): The log level.
-        json (Optional[bool]): Whether to use JSON formatting.
-
-    Returns:
-        logging.Logger: The configured root logger.
-    """
-    configure_loguru()
-    return logging.getLogger()
-
-
-__all__ = ["configure_logging"]
diff --git a/app/adapters/telemetry/loguru.py b/app/adapters/telemetry/loguru.py
deleted file mode 100644
index c86814e..0000000
--- a/app/adapters/telemetry/loguru.py
+++ /dev/null
@@ -1,21 +0,0 @@
-import logging
-import sys
-from pathlib import Path
-
-from loguru import logger
-
-
-class PropagateHandler(logging.Handler):
-    def emit(self, record):
-        logging.getLogger(record.name).handle(record)
-
-
-def configure_loguru():
-    logger.remove()
-    logger.add(sys.stdout, colorize=True, level="INFO")
-    logger.add(PropagateHandler(), format="{message}")
-
-
-def configure_test_logging(log_path: Path):
-    log_path.mkdir(parents=True, exist_ok=True)
-    logger.add(log_path / "test.log", rotation="10 MB", retention="10 days")
diff --git a/app/agent/probabilistic/__init__.py b/app/agent/probabilistic/__init__.py
new file mode 100644
index 0000000..378cb57
--- /dev/null
+++ b/app/agent/probabilistic/__init__.py
@@ -0,0 +1,11 @@
+"""Probabilistic signal and regime agents."""
+
+from .signal_filter import SignalFilteringAgent, FilterConfig
+from .regime import RegimeAnalysisAgent, RegimeSnapshot
+
+__all__ = [
+    "SignalFilteringAgent",
+    "FilterConfig",
+    "RegimeAnalysisAgent",
+    "RegimeSnapshot",
+]
diff --git a/app/agent/probabilistic/regime.py b/app/agent/probabilistic/regime.py
new file mode 100644
index 0000000..a10e10a
--- /dev/null
+++ b/app/agent/probabilistic/regime.py
@@ -0,0 +1,101 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from datetime import datetime
+from typing import Iterable, List, Optional
+
+import numpy as np
+
+from app.dal.schemas import SignalFrame
+
+
+@dataclass(slots=True)
+class RegimeSnapshot:
+    symbol: str
+    timestamp: Optional[datetime]
+    regime: str
+    volatility: float
+    uncertainty: float
+    momentum: float
+
+
+class RegimeAnalysisAgent:
+    """Classify probabilistic regimes based on filtered signals and uncertainty."""
+
+    def __init__(
+        self,
+        *,
+        window: int = 20,
+        high_vol_threshold: float = 0.02,
+        low_vol_threshold: float = 0.005,
+        uncertainty_threshold: float = 0.05,
+        momentum_threshold: float = 0.001,
+    ) -> None:
+        if window < 2:
+            raise ValueError("window must be >= 2")
+        self.window = window
+        self.high_vol_threshold = high_vol_threshold
+        self.low_vol_threshold = low_vol_threshold
+        self.uncertainty_threshold = uncertainty_threshold
+        self.momentum_threshold = momentum_threshold
+
+    def classify(self, frames: Iterable[SignalFrame]) -> List[RegimeSnapshot]:
+        frames_list = list(frames)
+        if not frames_list:
+            return []
+
+        prices = np.array([
+            frame.filtered_price if frame.filtered_price is not None else frame.price
+            for frame in frames_list
+        ])
+        returns = np.diff(np.log(prices + 1e-12), prepend=np.log(prices[0] + 1e-12))
+        momentum = np.convolve(returns, np.ones(self.window) / self.window, mode="same")
+
+        vol = self._rolling_std(returns, self.window)
+        snapshots: List[RegimeSnapshot] = []
+
+        for idx, frame in enumerate(frames_list):
+            current_vol = vol[idx]
+            current_uncertainty = frame.uncertainty
+            current_momentum = momentum[idx]
+
+            if current_uncertainty > self.uncertainty_threshold:
+                regime = "uncertain"
+            elif current_vol >= self.high_vol_threshold:
+                regime = "high_volatility"
+            elif current_vol <= self.low_vol_threshold:
+                if current_momentum >= self.momentum_threshold:
+                    regime = "trend_up"
+                elif current_momentum <= -self.momentum_threshold:
+                    regime = "trend_down"
+                else:
+                    regime = "calm"
+            else:
+                regime = "sideways"
+
+            snapshots.append(
+                RegimeSnapshot(
+                    symbol=frame.symbol,
+                    timestamp=frame.timestamp,
+                    regime=regime,
+                    volatility=float(current_vol),
+                    uncertainty=float(current_uncertainty),
+                    momentum=float(current_momentum),
+                )
+            )
+
+        return snapshots
+
+    def _rolling_std(self, data: np.ndarray, window: int) -> np.ndarray:
+        if len(data) < window:
+            std = float(np.std(data)) if data.size else 0.0
+            return np.full_like(data, std)
+        cumsum = np.cumsum(np.insert(data, 0, 0.0))
+        cumsum_sq = np.cumsum(np.insert(np.square(data), 0, 0.0))
+        means = (cumsum[window:] - cumsum[:-window]) / window
+        sq_means = (cumsum_sq[window:] - cumsum_sq[:-window]) / window
+        variances = np.maximum(sq_means - np.square(means), 0.0)
+        rolling_std = np.sqrt(variances)
+        pad_value = rolling_std[0] if rolling_std.size else 0.0
+        pad = np.full(window - 1, pad_value)
+        return np.concatenate([pad, rolling_std])
diff --git a/app/agent/probabilistic/signal_filter.py b/app/agent/probabilistic/signal_filter.py
new file mode 100644
index 0000000..b6c37cb
--- /dev/null
+++ b/app/agent/probabilistic/signal_filter.py
@@ -0,0 +1,156 @@
+from __future__ import annotations
+
+import math
+from dataclasses import dataclass, field
+from datetime import datetime
+from typing import List, Optional
+
+from app.dal.kalman import KalmanConfig, KalmanFilter1D
+from app.dal.schemas import Bars, SignalFrame
+
+
+@dataclass(slots=True)
+class FilterConfig:
+    """Configuration options for probabilistic signal filtering."""
+
+    kalman: Optional[KalmanConfig] = field(default_factory=KalmanConfig)
+    butterworth_cutoff: float = 0.1  # fraction of Nyquist (0, 0.5)
+    butterworth_order: int = 2
+    ema_span: int = 10
+
+
+class SignalFilteringAgent:
+    """Combine Kalman, Butterworth, and EMA filters to emit probabilistic signals."""
+
+    def __init__(self, config: Optional[FilterConfig] = None) -> None:
+        self.config = config or FilterConfig()
+        self._kalman: KalmanFilter1D | None = None
+        self._ema_prev: Optional[float] = None
+        self._ema_alpha: Optional[float] = None
+        self._butter_coeffs: Optional[tuple[float, float, float, float, float]] = None
+        self._butter_x1: Optional[float] = None
+        self._butter_x2: Optional[float] = None
+        self._butter_y1: Optional[float] = None
+        self._butter_y2: Optional[float] = None
+        self.reset()
+
+    def run(self, bars: Bars) -> List[SignalFrame]:
+        prices = [bar.close for bar in bars.data]
+        volumes = [bar.volume for bar in bars.data]
+        timestamps = [bar.timestamp for bar in bars.data]
+        if not prices:
+            return []
+
+        self.reset()
+        frames: List[SignalFrame] = []
+        for idx, price in enumerate(prices):
+            frames.append(
+                self.step(
+                    symbol=bars.symbol,
+                    vendor=bars.vendor,
+                    timestamp=timestamps[idx],
+                    price=price,
+                    volume=volumes[idx],
+                )
+            )
+        return frames
+
+    def reset(self) -> None:
+        """Reset filter state so the agent can process a fresh sequence."""
+        self._kalman = KalmanFilter1D(self.config.kalman)
+        if self.config.ema_span > 1:
+            self._ema_alpha = 2.0 / (self.config.ema_span + 1.0)
+        else:
+            self._ema_alpha = None
+        self._ema_prev = None
+        self._butter_coeffs = self._compute_butterworth_coeffs(
+            self.config.butterworth_cutoff, self.config.butterworth_order
+        )
+        self._butter_x1 = None
+        self._butter_x2 = None
+        self._butter_y1 = None
+        self._butter_y2 = None
+
+    def step(
+        self,
+        *,
+        symbol: str,
+        vendor: str,
+        timestamp: datetime,
+        price: float,
+        volume: float,
+    ) -> SignalFrame:
+        """Process a single observation and return the resulting SignalFrame."""
+        if self._kalman is None:
+            self.reset()
+
+        filtered, velocity, uncertainty = self._kalman.step(float(price))
+        butterworth_price = self._butterworth_step(float(price))
+        ema_price = self._ema_step(float(price))
+
+        return SignalFrame(
+            symbol=symbol,
+            vendor=vendor,
+            timestamp=timestamp,
+            price=float(price),
+            volume=float(volume),
+            filtered_price=filtered,
+            velocity=velocity,
+            uncertainty=uncertainty,
+            butterworth_price=butterworth_price,
+            ema_price=ema_price,
+        )
+
+    def _compute_butterworth_coeffs(
+        self, cutoff: float, order: int
+    ) -> tuple[float, float, float, float, float]:
+        cutoff = min(max(cutoff, 1e-5), 0.49)
+        ita = 1.0 / math.tan(math.pi * cutoff)
+        b0 = 1.0 / (1.0 + math.sqrt(2) * ita + ita * ita)
+        b1 = 2.0 * b0
+        b2 = b0
+        a1 = 2.0 * (ita * ita - 1.0) / (1.0 + math.sqrt(2) * ita + ita * ita)
+        a2 = (1.0 - math.sqrt(2) * ita + ita * ita) / (
+            1.0 + math.sqrt(2) * ita + ita * ita
+        )
+        return b0, b1, b2, a1, a2
+
+    def _butterworth_step(self, price: float) -> float:
+        if self._butter_coeffs is None:
+            return price
+        b0, b1, b2, a1, a2 = self._butter_coeffs
+        if self._butter_y1 is None:
+            y = b0 * price
+        elif self._butter_y2 is None or self._butter_x2 is None:
+            prev_x1 = self._butter_x1 if self._butter_x1 is not None else price
+            prev_y1 = self._butter_y1
+            y = b0 * price + b1 * prev_x1 - a1 * prev_y1
+        else:
+            prev_x1 = self._butter_x1 if self._butter_x1 is not None else price
+            prev_x2 = self._butter_x2
+            prev_y1 = self._butter_y1
+            prev_y2 = self._butter_y2
+            y = (
+                b0 * price
+                + b1 * prev_x1
+                + b2 * prev_x2
+                - a1 * prev_y1
+                - a2 * prev_y2
+            )
+
+        self._butter_x2 = self._butter_x1
+        self._butter_x1 = price
+        self._butter_y2 = self._butter_y1
+        self._butter_y1 = y
+        return y
+
+    def _ema_step(self, price: float) -> float:
+        if self._ema_alpha is None:
+            return price
+        if self._ema_prev is None:
+            self._ema_prev = price
+        else:
+            self._ema_prev = self._ema_alpha * price + (
+                1.0 - self._ema_alpha
+            ) * self._ema_prev
+        return self._ema_prev
diff --git a/app/api/routes/health.py b/app/api/routes/health.py
index c83f4c8..3fbcd6e 100644
--- a/app/api/routes/health.py
+++ b/app/api/routes/health.py
@@ -21,6 +21,7 @@ except Exception:
 from app.adapters.db.postgres import ping
 from app.api.routes.tasks import get_build_counters
 from app.domain.watchlist_service import get_counters as get_watchlist_counters
+from app.settings import get_database_settings, get_telegram_settings
 
 router = APIRouter(tags=["health"])
 
@@ -143,9 +144,12 @@ async def health_config() -> Dict[str, Any]:
     """
     env = os.getenv("ENV", "dev").lower()
 
-    telegram_token = os.getenv("TELEGRAM_BOT_TOKEN", "")
-    telegram_chat = os.getenv("TELEGRAM_DEFAULT_CHAT_ID", "")
-    database_url = os.getenv("DATABASE_URL", "")
+    telegram_settings = get_telegram_settings()
+    database_settings = get_database_settings()
+
+    telegram_token = telegram_settings.bot_token or ""
+    telegram_chat = telegram_settings.default_chat_id or ""
+    database_url = database_settings.primary_dsn or ""
     alpaca_key = os.getenv("ALPACA_API_KEY", "")
     alpaca_secret = os.getenv("ALPACA_API_SECRET", "")
     alpaca_feed = os.getenv("ALPACA_FEED", "iex")
diff --git a/app/api/routes/telegram.py b/app/api/routes/telegram.py
index e42e864..9e541cb 100644
--- a/app/api/routes/telegram.py
+++ b/app/api/routes/telegram.py
@@ -57,7 +57,22 @@ def _env():
         return F()
 
 
-SYMBOL_RE = re.compile(r"^[A-Za-z][A-Za-z0-9.\-]{0,20}$")
+SYMBOL_RE = re.compile(r"^[A-Z]{1,5}(?:\.[A-Z]{1,2})?$")
+
+from collections import deque
+_SEEN_UPDATE_IDS: deque[int] = deque(maxlen=2000)
+
+def _is_duplicate_update(update_id: Optional[int]) -> bool:
+    if update_id is None:
+        return False
+    try:
+        uid = int(update_id)
+    except Exception:
+        return False
+    if uid in _SEEN_UPDATE_IDS:
+        return True
+    _SEEN_UPDATE_IDS.append(uid)
+    return False
 
 
 def _mask(s: Optional[str]) -> str:
@@ -145,9 +160,9 @@ def _reply(tg: Any, chat_id: int | str, text: str) -> None:
         # fall through to generic path if needed
 
     candidates: list[tuple[str, dict]] = [
-        ("smart_send", {"parse_mode": "Markdown", "chunk_size": 3500}),
-        ("send_message", {"parse_mode": "Markdown"}),
-        ("send_text", {"parse_mode": "Markdown"}),
+        ("smart_send", {"parse_mode": "MarkdownV2", "chunk_size": 3500}),
+        ("send_message", {"parse_mode": "MarkdownV2"}),
+        ("send_text", {"parse_mode": "MarkdownV2"}),
         ("send", {}),
     ]
     for name, kwargs in candidates:
@@ -163,11 +178,20 @@ def _reply(tg: Any, chat_id: int | str, text: str) -> None:
                 method(chat_id, text)
                 return
             except Exception as exc:
-                logger.warning(
-                    "Telegram reply fallback failed via {}: {}", name, exc
-                )
+                logger.warning("Telegram reply fallback failed via {}: {}", name, exc)
         except Exception as exc:
+            # Retry once without parse_mode (helps on MarkdownV2 formatting errors)
+            if "parse_mode" in kwargs:
+                try:
+                    k2 = dict(kwargs)
+                    k2.pop("parse_mode", None)
+                    method(chat_id, text, **k2)
+                    return
+                except Exception as exc2:
+                    logger.warning("Telegram reply error via {} (no parse_mode retry failed): {}", name, exc2)
+                    continue
             logger.warning("Telegram reply error via {}: {}", name, exc)
+            continue
     logger.warning("No compatible Telegram send method found on {!r}", tg)
 
 
@@ -231,6 +255,9 @@ def _handle_watchlist(tg: Any, chat_id: int | str, args: list[str]) -> None:
         elif flag in {"0", "false", "f", "no", "n", "off"}:
             include_filters = False
 
+    # Session flag support
+    session_flag = (kv_flags.get("session") or parsed.get("session") or "").strip().lower() or None
+
     symbols_arg = parsed.get("symbols") or []
     resolved_source = None
 
@@ -256,7 +283,7 @@ def _handle_watchlist(tg: Any, chat_id: int | str, args: list[str]) -> None:
                 tmp.append(tok)
         symbols_arg = tmp
 
-    wl = build_watchlist(
+    build_kwargs = dict(
         symbols=symbols_arg or None,
         include_filters=(
             True
@@ -267,6 +294,9 @@ def _handle_watchlist(tg: Any, chat_id: int | str, args: list[str]) -> None:
         include_ohlcv=True,
         limit=limit,
     )
+    if session_flag:
+        build_kwargs["session"] = session_flag
+    wl = build_watchlist(**build_kwargs)
     if not isinstance(wl, dict):
         raise ValueError("Watchlist response malformed")
 
@@ -339,8 +369,10 @@ COMMANDS = {
 }
 
 
+import anyio
+
 @router.post("/webhook")
-def webhook(
+async def webhook(
     request: Request,
     payload: Dict[str, Any] = Body(...),
     x_secret_primary: Optional[str] = Header(
@@ -408,8 +440,20 @@ def webhook(
         )
         raise HTTPException(status_code=401, detail="Unauthorized")
 
-    # Extract chat/message
-    msg = payload.get("message") or payload.get("edited_message") or {}
+    # Callback query support
+    cb = payload.get("callback_query") or {}
+    if cb:
+        msg = (cb.get("message") or {})
+        # Prefer callback data as text, fall back to message text
+        cb_data = (cb.get("data") or "").strip()
+        if cb_data:
+            text = cb_data
+        else:
+            text = (msg.get("text") or "").strip()
+    else:
+        msg = payload.get("message") or payload.get("edited_message") or {}
+        text = (msg.get("text") or "").strip()
+
     chat_id = (
         msg.get("chat") or {}
     ).get("id") or os.getenv("TELEGRAM_DEFAULT_CHAT_ID")
@@ -422,12 +466,20 @@ def webhook(
         logger.info("[tg] unauthorized user id={} (ignored)", user_id)
         return {"ok": True, "ignored": True}
 
-    text = (msg.get("text") or "").strip()
+    # Idempotency guard
+    update_id = payload.get("update_id") or (payload.get("callback_query") or {}).get("id")
+    if _is_duplicate_update(update_id):
+        return {"ok": True, "duplicate": True}
+
     cmd, args = _parse_command(text)
 
     handler = COMMANDS.get(cmd)
     if handler:
-        handler(tg, chat_id, args)
+        # Offload heavy work for watchlist
+        if handler is _handle_watchlist:
+            await anyio.to_thread.run_sync(_handle_watchlist, tg, chat_id, args)
+        else:
+            handler(tg, chat_id, args)
         return {"ok": True, "cmd": cmd}
 
     # Free-text fallback: attempt to parse tickers and run /watchlist
@@ -438,7 +490,8 @@ def webhook(
             if SYMBOL_RE.match(tok):
                 try_syms.append(tok)
         if try_syms:
-            _handle_watchlist(tg, chat_id, try_syms)
+            # Offload heavy work for watchlist
+            await anyio.to_thread.run_sync(_handle_watchlist, tg, chat_id, try_syms)
             return {"ok": True, "cmd": "/watchlist", "implicit": True}
 
     _handle_help(tg, chat_id, [])
diff --git a/app/backtest/run_breakout.py b/app/backtest/run_breakout.py
index 724f298..484a2f6 100644
--- a/app/backtest/run_breakout.py
+++ b/app/backtest/run_breakout.py
@@ -7,9 +7,9 @@ import os
 import sys
 import traceback
 from dataclasses import asdict
-from datetime import UTC, datetime
+from datetime import UTC, datetime, timedelta
 from pathlib import Path
-from typing import Any, Callable, Dict, Tuple
+from typing import Any, Callable, Dict, List, Tuple
 
 import numpy as np
 import pandas as pd
@@ -19,8 +19,13 @@ from pandas import Timestamp
 from app.backtest import metrics as bt_metrics
 from app.backtest.engine import Costs, backtest_long_only
 from app.backtest.model import BetaWinrate
+from app.dal.manager import MarketDataDAL
+from app.dal.results import ProbabilisticBatch
+from app.dal.schemas import SignalFrame
+from app.agent.probabilistic.regime import RegimeSnapshot
 from app.providers.yahoo_provider import get_history_daily
 from app.strats.breakout import BreakoutParams, generate_signals
+from app.logging_utils import setup_logging
 
 # ------------------------------------------------------------------------------
 # Logging
@@ -32,12 +37,63 @@ def _setup_cli_logging(level: str = "INFO") -> None:
     Idempotent CLI logging initializer. Keeps uvicorn/fastapi logs quiet when used as a script,
     and provides consistent formatting for CI logs.
     """
-    logger.remove()
-    logger.add(
-        sys.stderr,
-        level=level.upper(),
-        format="{time} {level} {name}: {message}",
-    )
+    setup_logging(force=True, level=level)
+
+
+def _normalize_index(index: List[datetime]) -> pd.DatetimeIndex:
+    idx = pd.to_datetime(index)
+    tz = getattr(idx, "tz", None)
+    if tz is not None:
+        idx = idx.tz_convert("UTC").tz_localize(None)
+    return idx
+
+
+def _signal_frames_to_dataframe(signals: List[SignalFrame]) -> pd.DataFrame:
+    if not signals:
+        return pd.DataFrame()
+    records = [
+        {
+            "timestamp": frame.timestamp,
+            "prob_price": frame.price,
+            "prob_volume": frame.volume,
+            "prob_filtered_price": frame.filtered_price,
+            "prob_velocity": frame.velocity,
+            "prob_uncertainty": frame.uncertainty,
+            "prob_butterworth_price": frame.butterworth_price,
+            "prob_ema_price": frame.ema_price,
+        }
+        for frame in signals
+    ]
+    df = pd.DataFrame(records)
+    idx = _normalize_index(df.pop("timestamp").tolist())
+    df.index = idx
+    df = df.groupby(level=0).last()
+    return df.sort_index()
+
+
+def _regime_snapshots_to_dataframe(regimes: List[RegimeSnapshot]) -> pd.DataFrame:
+    if not regimes:
+        return pd.DataFrame()
+    records = []
+    for snapshot in regimes:
+        if snapshot.timestamp is None:
+            continue
+        records.append(
+            {
+                "timestamp": snapshot.timestamp,
+                "regime_label": snapshot.regime,
+                "regime_volatility": snapshot.volatility,
+                "regime_uncertainty": snapshot.uncertainty,
+                "regime_momentum": snapshot.momentum,
+            }
+        )
+    if not records:
+        return pd.DataFrame()
+    df = pd.DataFrame(records)
+    idx = _normalize_index(df.pop("timestamp").tolist())
+    df.index = idx
+    df = df.groupby(level=0).last()
+    return df.sort_index()
 
 
 def _roundish(x, ndigits=4):
@@ -95,6 +151,10 @@ def run(
     debug: bool = False,
     debug_signals: bool = False,
     debug_entries: bool = False,
+    regime_aware_sizing: bool = False,
+    use_probabilistic: bool = False,
+    dal_vendor: str = "alpaca",
+    dal_interval: str = "1Day",
     export_csv: str | None = None,
 ) -> None:
     """
@@ -114,10 +174,70 @@ def run(
         )
         return
 
+    start_ts = datetime.combine(start_dt, datetime.min.time(), tzinfo=UTC)
+    end_ts = (
+        datetime.combine(end_dt, datetime.min.time(), tzinfo=UTC) + timedelta(days=1)
+        if end
+        else None
+    )
+
     # Strategy params
     p = BreakoutParams(**params_kwargs)
     sig = generate_signals(df, asdict(p))
 
+    prob_batch: ProbabilisticBatch | None = None
+    if use_probabilistic:
+        try:
+            dal = MarketDataDAL(enable_postgres_metadata=False)
+            prob_batch = dal.fetch_bars(
+                symbol=symbol,
+                start=start_ts,
+                end=end_ts,
+                interval=dal_interval,
+                vendor=dal_vendor,
+            )
+            logger.info(
+                "Probabilistic DAL fetch ok vendor={} interval={} bars={} signals={} regimes={}",
+                dal_vendor,
+                dal_interval,
+                len(prob_batch.bars.data),
+                len(prob_batch.signals),
+                len(prob_batch.regimes),
+            )
+            if prob_batch.cache_paths:
+                logger.debug("Probabilistic cache artifacts: {}", prob_batch.cache_paths)
+
+            prob_signal_df = _signal_frames_to_dataframe(prob_batch.signals)
+            prob_regime_df = _regime_snapshots_to_dataframe(prob_batch.regimes)
+
+            if not prob_signal_df.empty:
+                sig = sig.join(prob_signal_df, how="left")
+            if not prob_regime_df.empty:
+                sig = sig.join(prob_regime_df, how="left")
+            joined_cols = [
+                col
+                for col in ("prob_filtered_price", "regime_label")
+                if col in sig.columns
+            ]
+            if joined_cols:
+                nonnull = sig[joined_cols].count()
+                logger.debug(
+                    "Probabilistic features joined: {}",
+                    {col: int(nonnull.get(col, 0)) for col in joined_cols},
+                )
+        except Exception as err:
+            logger.warning(
+                "Probabilistic DAL fetch failed (vendor={}, interval={}): {}",
+                dal_vendor,
+                dal_interval,
+                err,
+            )
+            prob_batch = None
+    elif regime_aware_sizing:
+        logger.warning(
+            "Regime-aware sizing requested but --use-probabilistic disabled; ignoring regime sizing toggle."
+        )
+
     # Engine OHLC input (ensure lowercase columns)
     if all(c in sig.columns for c in ["open", "high", "low", "close"]):
         df_engine = sig[["open", "high", "low", "close"]].copy()
@@ -191,6 +311,15 @@ def run(
                 "trail_stop",
                 "trend_ok",
                 "trigger",
+                "prob_filtered_price",
+                "prob_velocity",
+                "prob_uncertainty",
+                "prob_butterworth_price",
+                "prob_ema_price",
+                "regime_label",
+                "regime_volatility",
+                "regime_uncertainty",
+                "regime_momentum",
                 "long_entry",
                 "long_exit",
             ]
@@ -217,6 +346,56 @@ def run(
         else 0.01
     )
 
+    base_risk_frac = (
+        risk_frac_override if risk_frac_override is not None else default_risk
+    )
+    risk_multiplier = 1.0
+    applied_regime = None
+    applied_uncertainty = None
+    if regime_aware_sizing and prob_batch:
+        regime_series = sig.get("regime_label")
+        if regime_series is not None:
+            regime_values = regime_series.dropna()
+            if not regime_values.empty:
+                applied_regime = str(regime_values.iloc[-1])
+                regime_scalers = {
+                    "trend_up": 1.0,
+                    "trend_down": 0.6,
+                    "high_volatility": 0.5,
+                    "uncertain": 0.4,
+                    "sideways": 0.7,
+                    "calm": 0.85,
+                }
+                risk_multiplier = regime_scalers.get(applied_regime, 1.0)
+                uncertainty_series = sig.get("regime_uncertainty")
+                if uncertainty_series is not None:
+                    uncertainty_values = uncertainty_series.dropna()
+                    if not uncertainty_values.empty:
+                        applied_uncertainty = float(uncertainty_values.iloc[-1])
+                        if applied_uncertainty > 0.05:
+                            risk_multiplier *= 0.7
+                risk_multiplier = max(min(risk_multiplier, 1.0), 0.1)
+                logger.info(
+                    "Regime-aware sizing applied: regime={} uncertainty={} scale={:.3f}",
+                    applied_regime,
+                    (
+                        f"{applied_uncertainty:.4f}"
+                        if applied_uncertainty is not None
+                        else "n/a"
+                    ),
+                    risk_multiplier,
+                )
+            else:
+                logger.warning(
+                    "Regime-aware sizing enabled but regime series contained only NaNs."
+                )
+        else:
+            logger.warning(
+                "Regime-aware sizing enabled but regime_label column not found."
+            )
+
+    risk_frac_value = base_risk_frac * risk_multiplier
+
     atr_series = sig.get("atr")
     if atr_series is None:
         raise ValueError("Signal frame must contain 'atr' column")
@@ -228,11 +407,7 @@ def run(
         atr=atr_series,
         entry_price=p.entry_price,
         atr_mult=p.atr_mult,
-        risk_frac=(
-            risk_frac_override
-            if risk_frac_override is not None
-            else default_risk
-        ),
+        risk_frac=risk_frac_value,
         costs=Costs(
             slippage_bps=slippage_bps if slippage_bps is not None else 1.0,
             fee_per_share=fee_per_share if fee_per_share is not None else 0.0,
@@ -308,6 +483,15 @@ def run(
                     "trail_stop",
                     "trend_ok",
                     "trigger",
+                    "prob_filtered_price",
+                    "prob_velocity",
+                    "prob_uncertainty",
+                    "prob_butterworth_price",
+                    "prob_ema_price",
+                    "regime_label",
+                    "regime_volatility",
+                    "regime_uncertainty",
+                    "regime_momentum",
                     "long_entry",
                     "long_exit",
                 ]
@@ -400,6 +584,32 @@ if __name__ == "__main__":
         default=None,
         help="Directory to write <symbol>_equity.csv and <symbol>_trades.csv exports",
     )
+    ap.add_argument(
+        "--use-probabilistic",
+        dest="use_probabilistic",
+        action="store_true",
+        default=False,
+        help="Fetch bars via MarketDataDAL and join probabilistic signals/regimes into the breakout frame.",
+    )
+    ap.add_argument(
+        "--dal-vendor",
+        dest="dal_vendor",
+        default="alpaca",
+        help="MarketDataDAL vendor key when --use-probabilistic is enabled (default: alpaca).",
+    )
+    ap.add_argument(
+        "--dal-interval",
+        dest="dal_interval",
+        default="1Day",
+        help="MarketDataDAL bar interval when --use-probabilistic is enabled (default: 1Day).",
+    )
+    ap.add_argument(
+        "--regime-aware-sizing",
+        dest="regime_aware_sizing",
+        action="store_true",
+        default=False,
+        help="Scale breakout risk fraction using the latest probabilistic regime snapshot (requires --use-probabilistic).",
+    )
 
     # --- Strategy Parameters ---
     ap.add_argument(
@@ -600,6 +810,10 @@ if __name__ == "__main__":
             debug=args.debug,
             debug_signals=args.debug_signals or args.debug,
             debug_entries=args.debug_entries or args.debug,
+            regime_aware_sizing=args.regime_aware_sizing,
+            use_probabilistic=args.use_probabilistic,
+            dal_vendor=args.dal_vendor,
+            dal_interval=args.dal_interval,
             export_csv=args.export_csv,
         )
         if args.print_metrics_json:
diff --git a/app/dal/__init__.py b/app/dal/__init__.py
new file mode 100644
index 0000000..de5d86c
--- /dev/null
+++ b/app/dal/__init__.py
@@ -0,0 +1,24 @@
+"""Probabilistic market data abstraction layer."""
+
+from .kalman import KalmanConfig, KalmanFilter1D
+from .results import ProbabilisticBatch, ProbabilisticStreamFrame
+from .schemas import Bar, Bars, SignalFrame
+
+__all__ = [
+    "MarketDataDAL",
+    "KalmanConfig",
+    "KalmanFilter1D",
+    "ProbabilisticBatch",
+    "ProbabilisticStreamFrame",
+    "Bar",
+    "Bars",
+    "SignalFrame",
+]
+
+
+def __getattr__(name: str):
+    if name == "MarketDataDAL":
+        from .manager import MarketDataDAL as _MarketDataDAL
+
+        return _MarketDataDAL
+    raise AttributeError(f"module {__name__} has no attribute {name!r}")
diff --git a/app/dal/cache.py b/app/dal/cache.py
new file mode 100644
index 0000000..481b51c
--- /dev/null
+++ b/app/dal/cache.py
@@ -0,0 +1,106 @@
+from __future__ import annotations
+
+from pathlib import Path
+from typing import Iterable, Optional
+
+import pandas as pd
+from loguru import logger
+
+from app.agent.probabilistic.regime import RegimeSnapshot
+from app.dal.schemas import Bar, Bars, SignalFrame
+
+
+def store_bars_to_parquet(bars: Bars, directory: Path) -> Path:
+    directory.mkdir(parents=True, exist_ok=True)
+    file_path = directory / f"{bars.symbol}_{bars.vendor}.parquet"
+    df = bars.to_dataframe()
+    df.to_parquet(file_path, index=True)
+    logger.debug("stored bars to {} rows={} symbol={} vendor={}", file_path, len(df), bars.symbol, bars.vendor)
+    return file_path
+
+
+def load_bars_from_parquet(path: Path, symbol: str, vendor: str, timezone: str = "UTC") -> Bars:
+    df = pd.read_parquet(path)
+    index = pd.to_datetime(df.index)
+    if index.tz is None:  # type: ignore[attr-defined]
+        index = index.tz_localize("UTC")
+    df.index = index.tz_convert(timezone)
+    out = Bars(symbol=symbol, vendor=vendor, timezone=timezone)
+    for ts, row in df.sort_index().iterrows():
+        out.append(
+            Bar(
+                symbol=symbol,
+                vendor=vendor,
+                timestamp=ts.to_pydatetime(),
+                open=float(row.get("open", 0.0)),
+                high=float(row.get("high", 0.0)),
+                low=float(row.get("low", 0.0)),
+                close=float(row.get("close", 0.0)),
+                volume=float(row.get("volume", 0.0)),
+                timezone=timezone,
+                source="cache",
+            )
+        )
+    return out
+
+
+def store_signals_to_parquet(signals: Iterable[SignalFrame], directory: Path) -> Optional[Path]:
+    signals_list = list(signals)
+    if not signals_list:
+        return None
+    directory.mkdir(parents=True, exist_ok=True)
+    key = f"{signals_list[0].symbol}_{signals_list[0].vendor}_signals.parquet"
+    file_path = directory / key
+    df = pd.DataFrame(
+        [
+            {
+                "timestamp": frame.timestamp,
+                "price": frame.price,
+                "volume": frame.volume,
+                "filtered_price": frame.filtered_price,
+                "velocity": frame.velocity,
+                "uncertainty": frame.uncertainty,
+                "butterworth_price": frame.butterworth_price,
+                "ema_price": frame.ema_price,
+            }
+            for frame in signals_list
+        ]
+    ).set_index("timestamp")
+    df.to_parquet(file_path, index=True)
+    logger.debug(
+        "stored signal frames to {} rows={} symbol={} vendor={}",
+        file_path,
+        len(df),
+        signals_list[0].symbol,
+        signals_list[0].vendor,
+    )
+    return file_path
+
+
+def store_regimes_to_parquet(regimes: Iterable[RegimeSnapshot], directory: Path) -> Optional[Path]:
+    regime_list = list(regimes)
+    if not regime_list:
+        return None
+    directory.mkdir(parents=True, exist_ok=True)
+    key = f"{regime_list[0].symbol}_regimes.parquet"
+    file_path = directory / key
+    df = pd.DataFrame(
+        [
+            {
+                "timestamp": snapshot.timestamp,
+                "regime": snapshot.regime,
+                "volatility": snapshot.volatility,
+                "uncertainty": snapshot.uncertainty,
+                "momentum": snapshot.momentum,
+            }
+            for snapshot in regime_list
+        ]
+    ).set_index("timestamp")
+    df.to_parquet(file_path, index=True)
+    logger.debug(
+        "stored regime snapshots to {} rows={} symbol={}",
+        file_path,
+        len(df),
+        regime_list[0].symbol,
+    )
+    return file_path
diff --git a/app/dal/kalman.py b/app/dal/kalman.py
new file mode 100644
index 0000000..83f00ab
--- /dev/null
+++ b/app/dal/kalman.py
@@ -0,0 +1,69 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Tuple
+
+
+@dataclass
+class KalmanConfig:
+    """Configuration for a simple constant-velocity Kalman filter."""
+
+    process_variance: float = 1e-3
+    measurement_variance: float = 1e-2
+    dt: float = 1.0  # time delta between measurements, default 1 unit
+
+
+class KalmanFilter1D:
+    """Constant-velocity Kalman filter tracking price and velocity."""
+
+    def __init__(self, config: KalmanConfig | None = None) -> None:
+        cfg = config or KalmanConfig()
+        self.q = cfg.process_variance
+        self.r = cfg.measurement_variance
+        self.dt = cfg.dt
+        self.reset()
+
+    def reset(self) -> None:
+        self.x = 0.0  # position / price
+        self.v = 0.0  # velocity
+        self.p11 = 1.0
+        self.p12 = 0.0
+        self.p21 = 0.0
+        self.p22 = 1.0
+        self._initialized = False
+
+    def step(self, price: float) -> Tuple[float, float, float]:
+        """Consume a new price observation and return (filtered_price, velocity, uncertainty)."""
+        if not self._initialized:
+            self.x = price
+            self.v = 0.0
+            self._initialized = True
+            return price, 0.0, self.p11
+
+        # Prediction step
+        x_pred = self.x + self.v * self.dt
+        v_pred = self.v
+        p11_pred = self.p11 + (self.p12 + self.p21 + self.p22 * self.dt) * self.dt + self.q
+        p12_pred = self.p12 + self.p22 * self.dt
+        p21_pred = self.p21 + self.p22 * self.dt
+        p22_pred = self.p22 + self.q
+
+        # Innovation
+        y = price - x_pred
+        s = p11_pred + self.r
+        k1 = p11_pred / s
+        k2 = p21_pred / s
+
+        # Update
+        self.x = x_pred + k1 * y
+        self.v = v_pred + k2 * y
+        self.p11 = (1 - k1) * p11_pred
+        self.p12 = (1 - k1) * p12_pred
+        self.p21 = p21_pred - k2 * p11_pred
+        self.p22 = p22_pred - k2 * p12_pred
+
+        uncertainty = max(self.p11, 0.0)
+        return self.x, self.v, uncertainty
+
+
+__all__ = ["KalmanFilter1D", "KalmanConfig"]
diff --git a/app/dal/manager.py b/app/dal/manager.py
new file mode 100644
index 0000000..dc71565
--- /dev/null
+++ b/app/dal/manager.py
@@ -0,0 +1,221 @@
+from __future__ import annotations
+
+import asyncio
+import contextlib
+from datetime import datetime
+from pathlib import Path
+from typing import AsyncIterator, Dict, Iterable, List, Optional, Tuple
+
+from loguru import logger
+
+try:  # optional OTEL instrumentation
+    from opentelemetry import trace
+
+    _tracer = trace.get_tracer(__name__)
+except Exception:  # pragma: no cover - instrumentation optional
+    _tracer = None
+
+from sqlalchemy import text
+
+from app.adapters.db.postgres import get_engine
+from app.agent.probabilistic.regime import RegimeAnalysisAgent, RegimeSnapshot
+from app.agent.probabilistic.signal_filter import FilterConfig, SignalFilteringAgent
+from app.dal.cache import (
+    store_bars_to_parquet,
+    store_regimes_to_parquet,
+    store_signals_to_parquet,
+)
+from app.dal.kalman import KalmanConfig
+from app.dal.results import ProbabilisticBatch, ProbabilisticStreamFrame
+from app.dal.schemas import Bars, SignalFrame
+from app.dal.streaming import StreamingManager
+from app.dal.vendors.alpaca import AlpacaVendor
+from app.dal.vendors.alphavantage import AlphaVantageVendor
+from app.dal.vendors.base import FetchRequest, VendorClient
+from app.dal.vendors.finnhub import FinnhubVendor
+
+
+class MarketDataDAL:
+    """Unified interface for historical and streaming market data."""
+
+    def __init__(
+        self,
+        *,
+        cache_dir: Optional[Path] = Path("artifacts/marketdata/cache"),
+        vendor_clients: Optional[Dict[str, VendorClient]] = None,
+        kalman_config: Optional[KalmanConfig] = None,
+        filter_config: Optional[FilterConfig] = None,
+        regime_params: Optional[Dict[str, object]] = None,
+        enable_postgres_metadata: bool = True,
+    ) -> None:
+        self.cache_dir = cache_dir
+        self.vendor_clients = vendor_clients or self._default_vendors()
+        base_filter_config = filter_config or FilterConfig()
+        if kalman_config is not None:
+            base_filter_config = FilterConfig(
+                kalman=kalman_config,
+                butterworth_cutoff=base_filter_config.butterworth_cutoff,
+                butterworth_order=base_filter_config.butterworth_order,
+                ema_span=base_filter_config.ema_span,
+            )
+        self.filter_config = base_filter_config
+        self.kalman_config = self.filter_config.kalman or KalmanConfig()
+        self.regime_params: Dict[str, object] = dict(regime_params or {})
+        self.enable_postgres_metadata = enable_postgres_metadata
+        self._engine = get_engine() if enable_postgres_metadata else None
+        if self._engine is not None:
+            self._ensure_metadata_table()
+
+    def _default_vendors(self) -> Dict[str, VendorClient]:
+        return {
+            "alpaca": AlpacaVendor(),
+            "alphavantage": AlphaVantageVendor(),
+            "finnhub": FinnhubVendor(),
+        }
+
+    def fetch_bars(
+        self,
+        symbol: str,
+        *,
+        start: Optional[datetime] = None,
+        end: Optional[datetime] = None,
+        interval: str = "1Min",
+        vendor: str = "alpaca",
+        limit: Optional[int] = None,
+    ) -> ProbabilisticBatch:
+        client = self._get_vendor(vendor)
+        request = FetchRequest(
+            symbol=symbol,
+            start=start,
+            end=end,
+            interval=interval,
+            limit=limit,
+        )
+
+        def _call_fetch():
+            return client.fetch_bars(request)
+
+        span_ctx = (
+            _tracer.start_as_current_span("dal.fetch_bars", attributes={"vendor": vendor})
+            if _tracer
+            else contextlib.nullcontext()
+        )
+        with span_ctx:
+            bars = _call_fetch()
+
+        signals, regimes = self._run_probabilistic_pipeline(bars)
+        cache_paths: Dict[str, Path] = {}
+        if self.cache_dir:
+            bars_path = store_bars_to_parquet(bars, self.cache_dir)
+            cache_paths["bars"] = bars_path
+            signals_path = store_signals_to_parquet(signals, self.cache_dir)
+            if signals_path:
+                cache_paths["signals"] = signals_path
+            regimes_path = store_regimes_to_parquet(regimes, self.cache_dir)
+            if regimes_path:
+                cache_paths["regimes"] = regimes_path
+        bars_path = cache_paths.get("bars") if cache_paths else None
+        if self._engine is not None:
+            self._persist_metadata(bars, bars_path)
+        return ProbabilisticBatch(
+            bars=bars,
+            signals=signals,
+            regimes=regimes,
+            cache_paths=cache_paths,
+        )
+
+    async def stream_bars(
+        self,
+        symbols: Iterable[str],
+        *,
+        interval: str = "1Min",
+        vendor: str = "alpaca",
+    ) -> AsyncIterator[ProbabilisticStreamFrame]:
+        client = self._get_vendor(vendor)
+        if not client.supports_streaming():
+            raise RuntimeError(f"Vendor {vendor} does not support streaming transport")
+
+        def backfill(request: FetchRequest) -> Iterable[dict]:
+            bars = client.fetch_bars(request)
+            for bar in bars.data:
+                yield {
+                    "symbol": bar.symbol,
+                    "timestamp": bar.timestamp,
+                    "close": bar.close,
+                    "volume": bar.volume,
+                }
+
+        manager = StreamingManager(
+            client,
+            interval,
+            filter_config=self.filter_config,
+            regime_params=self.regime_params,
+            fetch_backfill=backfill,
+        )
+        async for frame in manager.stream(symbols):
+            yield frame
+
+    def _get_vendor(self, vendor: str) -> VendorClient:
+        try:
+            return self.vendor_clients[vendor]
+        except KeyError as exc:  # pragma: no cover - misconfiguration guard
+            raise ValueError(f"Unknown vendor: {vendor}") from exc
+
+    def _run_probabilistic_pipeline(
+        self, bars: Bars
+    ) -> Tuple[List[SignalFrame], List[RegimeSnapshot]]:
+        filter_agent = SignalFilteringAgent(self.filter_config)
+        signals = filter_agent.run(bars)
+        regime_agent = RegimeAnalysisAgent(**self.regime_params)
+        regimes = regime_agent.classify(signals)
+        return signals, regimes
+
+    def _ensure_metadata_table(self) -> None:
+        if self._engine is None:
+            return
+        ddl = text(
+            """
+            CREATE TABLE IF NOT EXISTS market_data_snapshots (
+                id BIGSERIAL PRIMARY KEY,
+                vendor TEXT NOT NULL,
+                symbol TEXT NOT NULL,
+                start_ts TIMESTAMPTZ,
+                end_ts TIMESTAMPTZ,
+                bar_count INTEGER,
+                created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
+                storage_path TEXT
+            )
+            """
+        )
+        with self._engine.begin() as conn:  # pragma: no cover - exercised in integration
+            conn.execute(ddl)
+
+    def _persist_metadata(self, bars: Bars, cache_path: Optional[Path]) -> None:
+        if self._engine is None:
+            return
+        if not bars.data:
+            return
+        start_ts = bars.data[0].timestamp
+        end_ts = bars.data[-1].timestamp
+        storage_path = str(cache_path.resolve()) if cache_path else None
+        insert_sql = text(
+            """
+            INSERT INTO market_data_snapshots (vendor, symbol, start_ts, end_ts, bar_count, storage_path)
+            VALUES (:vendor, :symbol, :start_ts, :end_ts, :bar_count, :storage_path)
+            """
+        )
+        with self._engine.begin() as conn:  # pragma: no cover - exercised in integration
+            conn.execute(
+                insert_sql,
+                {
+                    "vendor": bars.vendor,
+                    "symbol": bars.symbol,
+                    "start_ts": start_ts,
+                    "end_ts": end_ts,
+                    "bar_count": len(bars.data),
+                    "storage_path": storage_path,
+                },
+            )
+
+
+__all__ = ["MarketDataDAL"]
diff --git a/app/dal/results.py b/app/dal/results.py
new file mode 100644
index 0000000..2d3111f
--- /dev/null
+++ b/app/dal/results.py
@@ -0,0 +1,29 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Dict, List, Optional
+
+from app.agent.probabilistic.regime import RegimeSnapshot
+from app.dal.schemas import Bars, SignalFrame
+
+
+@dataclass(slots=True)
+class ProbabilisticBatch:
+    """Container for synchronized bars, signal frames, and regime snapshots."""
+
+    bars: Bars
+    signals: List[SignalFrame]
+    regimes: List[RegimeSnapshot]
+    cache_paths: Dict[str, Path] = field(default_factory=dict)
+
+
+@dataclass(slots=True)
+class ProbabilisticStreamFrame:
+    """Streaming probabilistic payload containing both signal and regime views."""
+
+    signal: SignalFrame
+    regime: RegimeSnapshot
+
+
+__all__ = ["ProbabilisticBatch", "ProbabilisticStreamFrame"]
diff --git a/app/dal/schemas.py b/app/dal/schemas.py
new file mode 100644
index 0000000..8e93847
--- /dev/null
+++ b/app/dal/schemas.py
@@ -0,0 +1,153 @@
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from datetime import datetime, timezone
+from typing import Iterable, List, Optional, Sequence
+
+import pandas as pd
+
+
+@dataclass(frozen=True, slots=True)
+class Bar:
+    """Normalized OHLCV bar."""
+
+    symbol: str
+    vendor: str
+    timestamp: datetime
+    open: float
+    high: float
+    low: float
+    close: float
+    volume: float
+    timezone: str = "UTC"
+    source: str = "historical"
+
+    def as_dict(self) -> dict:
+        ts = self.timestamp
+        if ts.tzinfo is None:
+            ts = ts.replace(tzinfo=timezone.utc)
+        return {
+            "symbol": self.symbol,
+            "vendor": self.vendor,
+            "timestamp": ts.isoformat(),
+            "open": self.open,
+            "high": self.high,
+            "low": self.low,
+            "close": self.close,
+            "volume": self.volume,
+            "timezone": self.timezone,
+            "source": self.source,
+        }
+
+
+@dataclass(slots=True)
+class Bars:
+    """Collection of bars for a single symbol."""
+
+    symbol: str
+    vendor: str
+    timezone: str
+    data: List[Bar] = field(default_factory=list)
+
+    def append(self, bar: Bar) -> None:
+        if bar.symbol != self.symbol:
+            raise ValueError("bar symbol mismatch")
+        self.data.append(bar)
+
+    def extend(self, bars: Iterable[Bar]) -> None:
+        for bar in bars:
+            self.append(bar)
+
+    def to_dicts(self) -> List[dict]:
+        return [bar.as_dict() for bar in self.data]
+
+    def to_dataframe(self) -> pd.DataFrame:
+        if not self.data:
+            return pd.DataFrame(
+                columns=[
+                    "timestamp",
+                    "open",
+                    "high",
+                    "low",
+                    "close",
+                    "volume",
+                ]
+            )
+        raw = [
+            {
+                "timestamp": bar.timestamp,
+                "open": bar.open,
+                "high": bar.high,
+                "low": bar.low,
+                "close": bar.close,
+                "volume": bar.volume,
+            }
+            for bar in self.data
+        ]
+        df = pd.DataFrame(raw).set_index("timestamp").sort_index()
+        if self.timezone:
+            df.index = df.index.tz_convert(self.timezone)
+        return df
+
+
+@dataclass(frozen=True, slots=True)
+class SignalFrame:
+    """Kalman-derived probabilistic snapshot."""
+
+    symbol: str
+    vendor: str
+    timestamp: datetime
+    price: float
+    volume: float
+    filtered_price: float
+    velocity: float
+    uncertainty: float
+    butterworth_price: Optional[float] = None
+    ema_price: Optional[float] = None
+
+    def as_dict(self) -> dict:
+        ts = self.timestamp
+        if ts.tzinfo is None:
+            ts = ts.replace(tzinfo=timezone.utc)
+        return {
+            "symbol": self.symbol,
+            "vendor": self.vendor,
+            "timestamp": ts.isoformat(),
+            "price": self.price,
+            "volume": self.volume,
+            "filtered_price": self.filtered_price,
+            "velocity": self.velocity,
+            "uncertainty": self.uncertainty,
+            "butterworth_price": self.butterworth_price,
+            "ema_price": self.ema_price,
+        }
+
+
+def merge_bars(symbol: str, vendor: str, timezone_name: str, series: Sequence[dict]) -> Bars:
+    """Utility to construct a Bars object from raw dict sequences."""
+    out = Bars(symbol=symbol, vendor=vendor, timezone=timezone_name)
+    for payload in series:
+        ts = payload.get("timestamp") or payload.get("time")
+        if isinstance(ts, str):
+            ts = datetime.fromisoformat(ts.replace("Z", "+00:00"))
+        elif isinstance(ts, (int, float)):
+            ts = datetime.fromtimestamp(float(ts), tz=timezone.utc)
+        elif not isinstance(ts, datetime):
+            continue
+        bar = Bar(
+            symbol=symbol,
+            vendor=vendor,
+            timestamp=ts.astimezone(timezone.utc),
+            open=float(payload["open"]),
+            high=float(payload["high"]),
+            low=float(payload["low"]),
+            close=float(payload["close"]),
+            volume=float(payload.get("volume") or payload.get("vol") or 0.0),
+            timezone=timezone_name,
+            source=str(payload.get("source") or "historical"),
+        )
+        out.append(bar)
+    return out
+
+
+__all__ = ["Bar", "Bars", "SignalFrame", "merge_bars"]
diff --git a/app/dal/streaming.py b/app/dal/streaming.py
new file mode 100644
index 0000000..a593de2
--- /dev/null
+++ b/app/dal/streaming.py
@@ -0,0 +1,200 @@
+from __future__ import annotations
+
+import asyncio
+import contextlib
+from collections import deque
+from datetime import datetime, timedelta, timezone
+from typing import AsyncIterator, Callable, Deque, Dict, Iterable, List, Optional
+
+from loguru import logger
+
+from app.agent.probabilistic.regime import RegimeAnalysisAgent, RegimeSnapshot
+from app.agent.probabilistic.signal_filter import FilterConfig, SignalFilteringAgent
+from app.dal.results import ProbabilisticStreamFrame
+from app.dal.schemas import SignalFrame
+from app.dal.vendors.base import FetchRequest, VendorClient
+
+
+_INTERVAL_SECONDS = {
+    "1Min": 60,
+    "5Min": 300,
+    "15Min": 900,
+    "30Min": 1_800,
+    "60Min": 3_600,
+    "1Hour": 3_600,
+    "1Day": 86_400,
+}
+
+
+def interval_to_seconds(interval: str) -> int:
+    return _INTERVAL_SECONDS.get(interval, 60)
+
+
+class _PipelineState:
+    def __init__(
+        self,
+        symbol: str,
+        vendor: str,
+        filter_config: FilterConfig,
+        regime_params: Dict[str, object],
+    ) -> None:
+        self.symbol = symbol
+        self.vendor = vendor
+        self.filter_agent = SignalFilteringAgent(filter_config)
+        self.regime_agent = RegimeAnalysisAgent(**regime_params)
+        buffer_len = max(self.regime_agent.window * 3, 64)
+        self.buffer: Deque[SignalFrame] = deque(maxlen=buffer_len)
+
+    def process(self, timestamp: datetime, price: float, volume: float) -> ProbabilisticStreamFrame:
+        signal = self.filter_agent.step(
+            symbol=self.symbol,
+            vendor=self.vendor,
+            timestamp=timestamp,
+            price=price,
+            volume=volume,
+        )
+        self.buffer.append(signal)
+        regime_snapshots = self.regime_agent.classify(list(self.buffer))
+        regime = regime_snapshots[-1] if regime_snapshots else RegimeSnapshot(
+            symbol=self.symbol,
+            timestamp=signal.timestamp,
+            regime="unknown",
+            volatility=0.0,
+            uncertainty=signal.uncertainty,
+            momentum=0.0,
+        )
+        return ProbabilisticStreamFrame(signal=signal, regime=regime)
+
+
+class StreamingManager:
+    def __init__(
+        self,
+        vendor: VendorClient,
+        interval: str,
+        filter_config: Optional[FilterConfig] = None,
+        regime_params: Optional[Dict[str, object]] = None,
+        *,
+        max_queue: int = 1024,
+        gap_threshold: Optional[float] = None,
+        fetch_backfill: Optional[Callable[[FetchRequest], Iterable[dict]]] = None,
+    ) -> None:
+        self.vendor = vendor
+        self.interval = interval
+        self.filter_config = filter_config or FilterConfig()
+        self.regime_params: Dict[str, object] = dict(regime_params or {})
+        self.max_queue = max_queue
+        self.gap_seconds = gap_threshold or interval_to_seconds(interval) * 3
+        self.fetch_backfill = fetch_backfill
+
+    async def stream(self, symbols: Iterable[str]) -> AsyncIterator[ProbabilisticStreamFrame]:
+        queue: asyncio.Queue = asyncio.Queue(maxsize=self.max_queue)
+        producer_task = asyncio.create_task(self._producer(queue, symbols))
+        pipeline_map: Dict[str, _PipelineState] = {
+            sym.upper(): _PipelineState(sym.upper(), self.vendor.name, self.filter_config, self.regime_params)
+            for sym in symbols
+        }
+        last_seen: Dict[str, datetime] = {sym.upper(): None for sym in symbols}  # type: ignore
+
+        try:
+            while True:
+                event = await queue.get()
+                if event.get("__end__"):
+                    break
+                symbol = (event.get("symbol") or event.get("S") or "").upper()
+                if not symbol:
+                    continue
+                ts = event.get("timestamp")
+                if isinstance(ts, (int, float)):
+                    ts = datetime.fromtimestamp(ts, tz=timezone.utc)
+                if isinstance(ts, str):
+                    ts = datetime.fromisoformat(ts)
+                if not isinstance(ts, datetime):
+                    ts = datetime.now(timezone.utc)
+                ts = ts.astimezone(timezone.utc)
+
+                price = event.get("close") or event.get("price") or event.get("p")
+                if price is None:
+                    continue
+                volume = float(event.get("volume") or event.get("v") or 0.0)
+
+                last_ts = last_seen.get(symbol)
+                frames_before = []
+                if last_ts and (ts - last_ts).total_seconds() > self.gap_seconds:
+                    frames_before = await self._backfill(symbol, last_ts, ts, pipeline_map[symbol])
+
+                for frame in frames_before:
+                    last_seen[symbol] = frame.signal.timestamp
+                    yield frame
+
+                pipeline = pipeline_map.setdefault(
+                    symbol, _PipelineState(symbol, self.vendor.name, self.filter_config, self.regime_params)
+                )
+                result = pipeline.process(ts, float(price), volume)
+                last_seen[symbol] = result.signal.timestamp
+                yield result
+        finally:
+            producer_task.cancel()
+            with contextlib.suppress(Exception):
+                await producer_task
+
+    async def _producer(self, queue: asyncio.Queue, symbols: Iterable[str]) -> None:
+        async for payload in self.vendor.stream_bars(symbols, self.interval):
+            await self._put_with_backpressure(queue, payload)
+        await self._put_with_backpressure(queue, {"__end__": True})
+
+    async def _put_with_backpressure(self, queue: asyncio.Queue, payload: dict) -> None:
+        while True:
+            try:
+                queue.put_nowait(payload)
+                return
+            except asyncio.QueueFull:
+                try:
+                    queue.get_nowait()
+                except asyncio.QueueEmpty:  # pragma: no cover - race
+                    await asyncio.sleep(0)
+
+    async def _backfill(
+        self,
+        symbol: str,
+        last_ts: datetime,
+        current_ts: datetime,
+        pipeline: _PipelineState,
+    ) -> List[ProbabilisticStreamFrame]:
+        if not self.fetch_backfill:
+            logger.debug(
+                "stream gap detected but no backfill handler configured symbol={} gap={}s",
+                symbol,
+                (current_ts - last_ts).total_seconds(),
+            )
+            return []
+        request = FetchRequest(
+            symbol=symbol,
+            start=last_ts - timedelta(seconds=self.gap_seconds),
+            end=current_ts,
+            interval=self.interval,
+            limit=None,
+        )
+        frames: List[SignalFrame] = []
+        loop = asyncio.get_running_loop()
+        raw_records = await loop.run_in_executor(
+            None, lambda: list(self.fetch_backfill(request) or [])
+        )
+        raw_records.sort(key=lambda item: item.get("timestamp"))
+        for record in raw_records:
+            ts = record.get("timestamp")
+            if isinstance(ts, (int, float)):
+                ts = datetime.fromtimestamp(ts, tz=timezone.utc)
+            if isinstance(ts, str):
+                ts = datetime.fromisoformat(ts)
+            if not isinstance(ts, datetime):
+                continue
+            ts = ts.astimezone(timezone.utc)
+            price = record.get("close") or record.get("price")
+            if price is None:
+                continue
+            volume = float(record.get("volume") or 0.0)
+            frames.append(pipeline.process(ts, float(price), volume))
+        return frames
+
+
+__all__ = ["StreamingManager", "interval_to_seconds"]
diff --git a/app/dal/vendors/__init__.py b/app/dal/vendors/__init__.py
new file mode 100644
index 0000000..222b9ac
--- /dev/null
+++ b/app/dal/vendors/__init__.py
@@ -0,0 +1,12 @@
+from .alpaca import AlpacaVendor
+from .alphavantage import AlphaVantageVendor
+from .finnhub import FinnhubVendor
+from .base import VendorClient, FetchRequest
+
+__all__ = [
+    "VendorClient",
+    "FetchRequest",
+    "AlpacaVendor",
+    "AlphaVantageVendor",
+    "FinnhubVendor",
+]
diff --git a/app/dal/vendors/alpaca.py b/app/dal/vendors/alpaca.py
new file mode 100644
index 0000000..c5e0ac2
--- /dev/null
+++ b/app/dal/vendors/alpaca.py
@@ -0,0 +1,134 @@
+from __future__ import annotations
+
+import asyncio
+import json
+from datetime import datetime, timezone
+from typing import AsyncIterator, Iterable, Optional
+
+import websockets
+from loguru import logger
+
+from app.dal.schemas import Bar, Bars
+from app.dal.vendors.base import FetchRequest, VendorClient
+from app.utils.env import ALPACA_API_KEY, ALPACA_API_SECRET, ALPACA_DATA_BASE_URL, ALPACA_FEED
+from app.utils.http import alpaca_headers, http_get
+
+
+class AlpacaVendor(VendorClient):
+    """HTTP + WebSocket client for Alpaca market data."""
+
+    def __init__(self, feed: Optional[str] = None) -> None:
+        super().__init__("alpaca")
+        self.feed = (feed or ALPACA_FEED or "iex").lower()
+
+    def fetch_bars(self, request: FetchRequest) -> Bars:
+        params: dict[str, str | int] = {
+            "symbols": request.symbol.upper(),
+            "timeframe": request.interval,
+            "feed": self.feed,
+        }
+        if request.limit:
+            params["limit"] = int(request.limit)
+        if request.start:
+            params["start"] = request.start.astimezone(timezone.utc).isoformat()
+        if request.end:
+            params["end"] = request.end.astimezone(timezone.utc).isoformat()
+
+        url = f"{ALPACA_DATA_BASE_URL}/stocks/bars"
+        status, payload = http_get(url, params=params, headers=alpaca_headers())
+        if status != 200:
+            logger.warning(
+                "alpaca fetch_bars failed symbol={} status={} payload={}",
+                request.symbol,
+                status,
+                payload,
+            )
+            return Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+
+        raw_bars = ((payload or {}).get("bars") or {}).get(request.symbol.upper(), [])
+        bars = Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+        for entry in raw_bars:
+            ts = entry.get("t")
+            if isinstance(ts, str):
+                ts = datetime.fromisoformat(ts.replace("Z", "+00:00"))
+            elif isinstance(ts, (int, float)):
+                ts = datetime.fromtimestamp(float(ts), tz=timezone.utc)
+            elif not isinstance(ts, datetime):
+                continue
+            bars.append(
+                Bar(
+                    symbol=request.symbol.upper(),
+                    vendor=self.name,
+                    timestamp=ts.astimezone(timezone.utc),
+                    open=float(entry.get("o", 0.0)),
+                    high=float(entry.get("h", 0.0)),
+                    low=float(entry.get("l", 0.0)),
+                    close=float(entry.get("c", 0.0)),
+                    volume=float(entry.get("v", 0.0)),
+                    timezone="UTC",
+                    source="historical",
+                )
+            )
+        return bars
+
+    def supports_streaming(self) -> bool:
+        return bool(ALPACA_API_KEY and ALPACA_API_SECRET)
+
+    async def stream_bars(
+        self, symbols: Iterable[str], interval: str
+    ) -> AsyncIterator[dict]:
+        if not self.supports_streaming():
+            raise RuntimeError("Alpaca streaming requires API credentials")
+        auth_payload = {
+            "action": "auth",
+            "key": ALPACA_API_KEY,
+            "secret": ALPACA_API_SECRET,
+        }
+        subscribe_payload = {
+            "action": "subscribe",
+            "bars": [sym.upper() for sym in symbols],
+        }
+        stream_url = f"wss://stream.data.alpaca.markets/v2/{self.feed}"
+
+        async for message in _alpaca_stream(  # pragma: no cover - network IO
+            stream_url, auth_payload, subscribe_payload
+        ):
+            if message.get("T") != "b":
+                continue
+            yield {
+                "symbol": message.get("S"),
+                "timestamp": datetime.fromtimestamp(message.get("t", 0) / 1_000_000_000, tz=timezone.utc),
+                "open": message.get("o"),
+                "high": message.get("h"),
+                "low": message.get("l"),
+                "close": message.get("c"),
+                "volume": message.get("v"),
+                "source": "stream",
+            }
+
+
+async def _alpaca_stream(
+    url: str,
+    auth_payload: dict,
+    subscribe_payload: dict,
+    reconnect_delay: float = 3.0,
+):
+    """Reconnect-aware streaming helper for Alpaca bars."""
+    while True:  # pragma: no cover - network IO
+        try:
+            async with websockets.connect(url, ping_interval=20, ping_timeout=20) as ws:
+                await ws.send(json.dumps(auth_payload))
+                await ws.send(json.dumps(subscribe_payload))
+                async for raw in ws:
+                    try:
+                        payload = json.loads(raw)
+                        if isinstance(payload, list):
+                            for item in payload:
+                                yield item
+                        else:
+                            yield payload
+                    except json.JSONDecodeError:
+                        logger.debug("alpaca stream non-json payload={}", raw)
+        except Exception as exc:
+            logger.warning("alpaca stream reconnecting after error: {}", exc)
+            await asyncio.sleep(reconnect_delay)
diff --git a/app/dal/vendors/alphavantage.py b/app/dal/vendors/alphavantage.py
new file mode 100644
index 0000000..00449f8
--- /dev/null
+++ b/app/dal/vendors/alphavantage.py
@@ -0,0 +1,75 @@
+from __future__ import annotations
+
+from datetime import datetime, timezone
+from typing import Optional
+
+from loguru import logger
+
+from app.dal.schemas import Bar, Bars
+from app.dal.vendors.base import FetchRequest, VendorClient
+from app.settings import get_market_data_settings
+from app.utils.http import http_get
+
+
+class AlphaVantageVendor(VendorClient):
+    BASE_URL = "https://www.alphavantage.co/query"
+    SUPPORTED_INTERVALS = {"1Min", "5Min", "15Min", "30Min", "60Min"}
+
+    def __init__(self, api_key: Optional[str] = None) -> None:
+        super().__init__("alphavantage")
+        self.api_key = api_key or get_market_data_settings().alphavantage_key
+        if not self.api_key:
+            logger.warning("AlphaVantage API key not configured; fetches will fail")
+
+    def fetch_bars(self, request: FetchRequest) -> Bars:
+        if not self.api_key:
+            raise RuntimeError("AlphaVantage API key missing")
+
+        interval = request.interval
+        if interval not in self.SUPPORTED_INTERVALS:
+            raise ValueError(f"AlphaVantage only supports intervals: {self.SUPPORTED_INTERVALS}")
+
+        params = {
+            "function": "TIME_SERIES_INTRADAY",
+            "symbol": request.symbol.upper(),
+            "interval": interval.lower(),
+            "apikey": self.api_key,
+            "outputsize": "full" if request.limit is None else "compact",
+        }
+        status, payload = http_get(self.BASE_URL, params=params)
+        if status != 200 or not isinstance(payload, dict):
+            logger.warning("alphavantage fetch failed symbol={} status={} payload={}",
+                           request.symbol, status, payload)
+            return Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+
+        series_key = next((k for k in payload.keys() if k.startswith("Time Series")), None)
+        if not series_key:
+            logger.warning("alphavantage response missing time series: {}", payload.keys())
+            return Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+
+        raw_series = payload.get(series_key, {})
+        items = list(raw_series.items())
+        if request.limit:
+            items = items[: request.limit]
+
+        bars = Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+        for ts_str, entry in items:
+            try:
+                ts = datetime.fromisoformat(ts_str).replace(tzinfo=timezone.utc)
+                bars.append(
+                    Bar(
+                        symbol=request.symbol.upper(),
+                        vendor=self.name,
+                        timestamp=ts,
+                        open=float(entry.get("1. open", 0.0)),
+                        high=float(entry.get("2. high", 0.0)),
+                        low=float(entry.get("3. low", 0.0)),
+                        close=float(entry.get("4. close", 0.0)),
+                        volume=float(entry.get("5. volume", 0.0)),
+                        timezone="UTC",
+                        source="historical",
+                    )
+                )
+            except Exception as exc:  # pragma: no cover - defensive guard
+                logger.debug("alphavantage parse error ts={} err={}", ts_str, exc)
+        return bars
diff --git a/app/dal/vendors/base.py b/app/dal/vendors/base.py
new file mode 100644
index 0000000..4de32d6
--- /dev/null
+++ b/app/dal/vendors/base.py
@@ -0,0 +1,40 @@
+from __future__ import annotations
+
+import abc
+from dataclasses import dataclass
+from datetime import datetime
+from typing import AsyncIterator, Iterable, Optional
+
+from app.dal.schemas import Bars
+
+
+@dataclass(slots=True)
+class FetchRequest:
+    symbol: str
+    start: Optional[datetime]
+    end: Optional[datetime]
+    interval: str
+    limit: Optional[int] = None
+
+
+class VendorClient(abc.ABC):
+    """Base class for market data vendor implementations."""
+
+    name: str
+
+    def __init__(self, name: str) -> None:
+        self.name = name
+
+    @abc.abstractmethod
+    def fetch_bars(self, request: FetchRequest) -> Bars:
+        """Fetch historical bars synchronously."""
+
+    def supports_streaming(self) -> bool:
+        return False
+
+    async def stream_bars(
+        self, symbols: Iterable[str], interval: str
+    ) -> AsyncIterator[dict]:
+        raise NotImplementedError(
+            f"Vendor {self.name} does not implement streaming bars."
+        )
diff --git a/app/dal/vendors/finnhub.py b/app/dal/vendors/finnhub.py
new file mode 100644
index 0000000..09e25d5
--- /dev/null
+++ b/app/dal/vendors/finnhub.py
@@ -0,0 +1,131 @@
+from __future__ import annotations
+
+import asyncio
+import json
+from datetime import datetime, timezone
+from typing import AsyncIterator, Iterable, Optional
+
+import websockets
+from loguru import logger
+
+from app.dal.schemas import Bar, Bars
+from app.dal.vendors.base import FetchRequest, VendorClient
+from app.settings import get_market_data_settings
+from app.utils.http import http_get
+
+
+_RESOLUTION_MAP = {
+    "1Min": "1",
+    "5Min": "5",
+    "15Min": "15",
+    "30Min": "30",
+    "60Min": "60",
+    "1Hour": "60",
+    "1Day": "D",
+}
+
+
+class FinnhubVendor(VendorClient):
+    BASE_URL = "https://finnhub.io/api/v1"
+
+    def __init__(self, api_key: Optional[str] = None) -> None:
+        super().__init__("finnhub")
+        self.api_key = api_key or get_market_data_settings().finnhub_key
+        if not self.api_key:
+            logger.warning("Finnhub API key not configured; fetches will fail")
+
+    def fetch_bars(self, request: FetchRequest) -> Bars:
+        if not self.api_key:
+            raise RuntimeError("Finnhub API key missing")
+
+        resolution = _RESOLUTION_MAP.get(request.interval)
+        if not resolution:
+            raise ValueError(f"Unsupported Finnhub interval: {request.interval}")
+
+        if not request.start or not request.end:
+            raise ValueError("Finnhub fetch requires explicit start and end timestamps")
+
+        params = {
+            "symbol": request.symbol.upper(),
+            "resolution": resolution,
+            "from": int(request.start.timestamp()),
+            "to": int(request.end.timestamp()),
+            "token": self.api_key,
+        }
+        status, payload = http_get(f"{self.BASE_URL}/stock/candle", params=params)
+        if status != 200 or not isinstance(payload, dict) or payload.get("s") != "ok":
+            logger.warning("Finnhub candle fetch failed symbol={} status={} payload={} ",
+                           request.symbol, status, payload)
+            return Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+
+        bars = Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+        timestamps = payload.get("t", [])
+        opens = payload.get("o", [])
+        highs = payload.get("h", [])
+        lows = payload.get("l", [])
+        closes = payload.get("c", [])
+        volumes = payload.get("v", [])
+        for idx, ts in enumerate(timestamps):
+            dt = datetime.fromtimestamp(ts, tz=timezone.utc)
+            bars.append(
+                Bar(
+                    symbol=request.symbol.upper(),
+                    vendor=self.name,
+                    timestamp=dt,
+                    open=float(opens[idx]),
+                    high=float(highs[idx]),
+                    low=float(lows[idx]),
+                    close=float(closes[idx]),
+                    volume=float(volumes[idx]),
+                    timezone="UTC",
+                    source="historical",
+                )
+            )
+        return bars
+
+    def supports_streaming(self) -> bool:
+        return bool(self.api_key)
+
+    async def stream_bars(
+        self, symbols: Iterable[str], interval: str
+    ) -> AsyncIterator[dict]:
+        if not self.api_key:
+            raise RuntimeError("Finnhub API key missing")
+        # Finnhub streams trade ticks; downstream components aggregate into bars.
+        query = f"wss://ws.finnhub.io?token={self.api_key}"
+        subscribe_messages = [
+            json.dumps({"type": "subscribe", "symbol": sym.upper()})
+            for sym in symbols
+        ]
+
+        async for message in _finnhub_stream(query, subscribe_messages):  # pragma: no cover
+            if message.get("type") != "trade":
+                continue
+            for trade in message.get("data", []):
+                price = trade.get("p")
+                volume = trade.get("v", 0.0)
+                ts = datetime.fromtimestamp(trade.get("t", 0) / 1000, tz=timezone.utc)
+                yield {
+                    "symbol": trade.get("s"),
+                    "timestamp": ts,
+                    "price": price,
+                    "volume": volume,
+                    "source": "trade",
+                    "interval": interval,
+                }
+
+
+async def _finnhub_stream(url: str, subscribe_messages: list[str], reconnect_delay: float = 5.0):
+    while True:  # pragma: no cover - network path
+        try:
+            async with websockets.connect(url, ping_interval=20, ping_timeout=20) as ws:
+                for msg in subscribe_messages:
+                    await ws.send(msg)
+                async for raw in ws:
+                    try:
+                        yield json.loads(raw)
+                    except json.JSONDecodeError:
+                        logger.debug("finnhub stream non-json payload={}", raw)
+        except Exception as exc:
+            logger.warning("Finnhub stream reconnecting after error: {}", exc)
+            await asyncio.sleep(reconnect_delay)
diff --git a/app/execution/alpaca_client.py b/app/execution/alpaca_client.py
index 65e56df..9d18608 100644
--- a/app/execution/alpaca_client.py
+++ b/app/execution/alpaca_client.py
@@ -7,6 +7,9 @@ from typing import Any, Dict, Optional
 import requests
 from loguru import logger
 
+from app.utils import env as ENV
+from app.utils.http import compute_backoff_delay
+
 
 class ExecutionError(RuntimeError):
     """Raised when an order placement or broker interaction fails."""
@@ -34,17 +37,22 @@ class AlpacaClient:
         base_url: str,
         *,
         data_url: str | None = None,
-        timeout: float = 10.0,
-        retries: int = 2,
-        backoff: float = 1.5,
+        timeout: float | None = None,
+        retries: int | None = None,
+        backoff: float | None = None,
     ) -> None:
         self.key = key
         self.secret = secret
         self.base_url = base_url.rstrip("/")
         self.data_url = (data_url or "https://data.alpaca.markets").rstrip("/")
-        self.timeout = timeout
-        self.retries = max(0, retries)
-        self.backoff = max(0.0, backoff)
+        env_timeout = float(getattr(ENV, "HTTP_TIMEOUT", 10))
+        env_retries = getattr(ENV, "HTTP_RETRIES", 2)
+        env_backoff = float(getattr(ENV, "HTTP_BACKOFF", 1.5))
+        self.timeout = float(timeout) if timeout is not None else env_timeout
+        self.retries = max(0, int(retries)) if retries is not None else env_retries
+        self.backoff = (
+            max(0.0, float(backoff)) if backoff is not None else env_backoff
+        )
 
     # -------------------------------------------------------------------------
     # Internal HTTP helpers
@@ -76,11 +84,11 @@ class AlpacaClient:
                     **kwargs,
                 )
                 # Retry on 429/5xx (except 501/505 etc — handled generically)
-                if (
-                    resp.status_code in (429, 500, 502, 503, 504)
-                    and attempt < self.retries
-                ):
-                    delay = self.backoff * (attempt + 1)
+                retryable = resp.status_code in (408, 429, 500, 502, 503, 504)
+                if retryable and attempt < self.retries:
+                    delay = compute_backoff_delay(
+                        attempt, self.backoff, resp.headers.get("Retry-After")
+                    )
                     logger.warning(
                         "HTTP {} {} -> {}; retrying in {:.1f}s (attempt {}/{})",
                         method,
@@ -96,7 +104,7 @@ class AlpacaClient:
                 return resp
             except requests.RequestException as e:
                 if attempt < self.retries:
-                    delay = self.backoff * (attempt + 1)
+                    delay = compute_backoff_delay(attempt, self.backoff, None)
                     logger.warning(
                         "HTTP {} {} exception: {}; retrying in {:.1f}s (attempt {}/{})",
                         method,
diff --git a/app/logging_utils.py b/app/logging_utils.py
new file mode 100644
index 0000000..8ee7287
--- /dev/null
+++ b/app/logging_utils.py
@@ -0,0 +1,180 @@
+"""Loguru configuration helpers for consistent structured logging."""
+
+from __future__ import annotations
+
+import logging
+import os
+import sys
+from typing import Optional, Union
+from os import PathLike
+from pathlib import Path
+from loguru import logger
+
+from app.config import settings as app_settings
+
+# Global record factory to provide defaults for missing OTEL fields.
+_old_factory = logging.getLogRecordFactory()
+def _otel_safe_record_factory(*args, **kwargs):
+    record = _old_factory(*args, **kwargs)
+    if not hasattr(record, "otelTraceID"):    record.otelTraceID = "-"
+    if not hasattr(record, "otelSpanID"):     record.otelSpanID = "-"
+    if not hasattr(record, "otelTraceFlags"): record.otelTraceFlags = "-"
+    return record
+logging.setLogRecordFactory(_otel_safe_record_factory)
+
+PathLikeArg = Union[str, PathLike]  # simple alias
+
+_LOG_FORMAT = (
+    "{time:YYYY-MM-DD HH:mm:ss.SSS} | {level:<8} | "
+    "req={extra[request_id]} | env={extra[environment]} | "
+    "ver={extra[service_version]} | sha={extra[git_sha]} | {message}"
+)
+
+OTEL_MISSING = {"otelTraceID": "-", "otelSpanID": "-", "otelTraceFlags": "-"}
+
+def _std_logging_sink(message) -> None:
+    record = message.record
+    exc = record["exception"]
+    exc_info = None
+    if exc:
+        tb = getattr(exc.traceback, "as_traceback", None)
+        std_tb = tb() if callable(tb) else exc.traceback  # fallback
+        # If the fallback isn't a real traceback (older Loguru), just pass None to avoid type errors.
+        exc_info = (exc.type, exc.value, std_tb) if std_tb else (exc.type, exc.value, None)
+
+    log_record = logging.LogRecord(
+        name=record["name"],
+        level=record["level"].no,
+        pathname=record["file"].path,
+        lineno=record["line"],
+        msg=record["message"],
+        args=(),
+        exc_info=exc_info,
+        func=record["function"],
+    )
+    for k, v in record["extra"].items():
+        setattr(log_record, k, v)
+
+    # Ensure OTEL attributes are always present on the stdlib record, even if the
+    # Loguru message didn't carry them. (The global LogRecordFactory does not run
+    # when we construct LogRecord manually.)
+    for k, v in OTEL_MISSING.items():
+        if not hasattr(log_record, k) or getattr(log_record, k) in (None, ""):
+            setattr(log_record, k, v)
+
+    logging.getLogger().handle(log_record)
+
+
+def setup_logging(*, force: bool = False, level: str | None = None) -> None:
+    """Configure Loguru sinks, bridge to stdlib, and attach contextual metadata."""
+    if not force and getattr(setup_logging, "_configured", False):
+        return
+
+    logger.remove()
+
+    log_level = (level or os.getenv("OTEL_LOG_LEVEL") or os.getenv("LOG_LEVEL") or "INFO").upper()
+    environment = os.getenv("ENV", "local")
+    git_sha = (
+        os.getenv("GIT_SHA")
+        or os.getenv("COMMIT_SHA")
+        or os.getenv("SOURCE_VERSION")
+        or "unknown"
+    )
+
+    logger.configure(
+        extra={
+            "service_version": app_settings.VERSION,
+            "environment": environment,
+            "git_sha": git_sha,
+            "request_id": "-",
+        }
+    )
+
+    logger.add(
+        sys.stdout,
+        level=log_level,
+        format=_LOG_FORMAT,
+        enqueue=False,
+        backtrace=False,
+        diagnose=False,
+    )
+    logger.add(
+        _std_logging_sink,
+        level=log_level,
+        enqueue=False,
+        backtrace=False,
+        diagnose=False,
+    )
+
+    std_level = getattr(logging, log_level, logging.INFO)
+    root_logger = logging.getLogger()
+    root_logger.setLevel(std_level)
+    if not root_logger.handlers:
+        root_logger.addHandler(logging.StreamHandler(sys.stdout))
+    setup_logging._configured = True  # type: ignore[attr-defined]
+
+
+def setup_test_logging(
+    arg: Optional[Union[str, PathLikeArg]] = None,
+    *,
+    level: Optional[str] = None,
+    file: Optional[PathLikeArg] = None,
+    filename: str = "pytest.log",       # default file name when a directory is provided
+    parallel_safe: bool = False,        # if True -> pytest-<pid>.log
+) -> None:
+    """
+    Lightweight logging setup for tests.
+
+    Accepts either:
+      - a positional path (arg) to write logs to (file or directory), or
+      - level="DEBUG"/"INFO", and/or file="/path/to/test.log".
+    If a directory is provided, logs will go to <dir>/<filename> (or pytest-<pid>.log if parallel_safe).
+    """
+    # Interpret the positional arg: prefer path semantics if path-like or looks like a path
+    inferred_level: Optional[str] = None
+    inferred_path: Optional[Path] = None
+    if arg is not None:
+        if hasattr(arg, "__fspath__"):
+            inferred_path = Path(arg)  # type: ignore[arg-type]
+        elif isinstance(arg, str) and ("/" in arg or arg.endswith(".log")):
+            inferred_path = Path(arg)
+        elif isinstance(arg, str):
+            inferred_level = arg
+
+    effective_level = (level or inferred_level or os.getenv("PYTEST_LOGLEVEL") or "INFO").upper()
+
+    # Base setup (stdout sink + stdlib bridge)
+    setup_logging(force=True, level=effective_level)
+
+    # Resolve target path preference: explicit `file` wins, then inferred_path
+    target = Path(file) if file is not None else (inferred_path if inferred_path is not None else None)
+    if target is None:
+        return  # no file sink requested; stdout-only is fine
+
+    # If target is a directory, place the file inside it
+    if target.exists() and target.is_dir():
+        fname = f"pytest-{os.getpid()}.log" if parallel_safe else filename
+        target = target / fname
+    else:
+        # If target has no suffix but points to a non-existent path that looks like a dir,
+        # we still respect it as a file path; callers can give explicit dirs to force dir behavior.
+        if str(target).endswith(os.sep):
+            target = Path(str(target).rstrip(os.sep))
+            target.mkdir(parents=True, exist_ok=True)
+            fname = f"pytest-{os.getpid()}.log" if parallel_safe else filename
+            target = target / fname
+
+    # Ensure parent directory exists
+    target.parent.mkdir(parents=True, exist_ok=True)
+
+    # Add file sink
+    logger.add(
+        str(target),
+        level=effective_level,
+        format=_LOG_FORMAT,
+        enqueue=False,
+        backtrace=False,
+        diagnose=False,
+    )
+
+__all__ = ["setup_logging", "setup_test_logging"]
diff --git a/app/main.py b/app/main.py
index d6cecd4..7d0201f 100644
--- a/app/main.py
+++ b/app/main.py
@@ -5,7 +5,7 @@ import os
 import time
 from contextlib import asynccontextmanager
 from uuid import uuid4
-
+import app  # noqa: F401  # ensure package __init__ (Sentry) runs
 from fastapi import FastAPI, Request
 from loguru import logger
 
@@ -13,12 +13,20 @@ from app.api.routes.health import router as health_router
 from app.api.routes.tasks import public_router, tasks_router
 from app.api.routes.telegram import router as telegram_router
 from app.config import settings
+from app.logging_utils import setup_logging
+from app.observability import configure_observability
 
 __all__ = ["app"]
 
 
+setup_logging()
+
+
 @asynccontextmanager
 async def lifespan(app: FastAPI):
+    setup_logging()
+    configure_observability()
+
     required = {
         "ALPACA_API_KEY": settings.alpaca_key,
         "ALPACA_API_SECRET": settings.alpaca_secret,
@@ -53,27 +61,27 @@ app.include_router(public_router)
 async def request_logging_middleware(request: Request, call_next):
     start = time.perf_counter()
     request_id = request.headers.get("X-Request-ID") or uuid4().hex
-    try:
-        response = await call_next(request)
-    except Exception:
+    request.state.request_id = request_id
+    with logger.contextualize(request_id=request_id):
+        try:
+            response = await call_next(request)
+        except Exception:
+            duration_ms = (time.perf_counter() - start) * 1000.0
+            logger.exception(
+                "request method={} path={} status=500 duration_ms={:.2f}",
+                request.method,
+                request.url.path,
+                duration_ms,
+            )
+            raise
+
         duration_ms = (time.perf_counter() - start) * 1000.0
-        logger.exception(
-            "request method={} path={} status=500 duration_ms={:.2f} request_id={}",
+        response.headers["X-Request-ID"] = request_id
+        logger.info(
+            "request method={} path={} status={} duration_ms={:.2f}",
             request.method,
             request.url.path,
+            response.status_code,
             duration_ms,
-            request_id,
         )
-        raise
-
-    duration_ms = (time.perf_counter() - start) * 1000.0
-    response.headers["X-Request-ID"] = request_id
-    logger.info(
-        "request method={} path={} status={} duration_ms={:.2f} request_id={}",
-        request.method,
-        request.url.path,
-        response.status_code,
-        duration_ms,
-        request_id,
-    )
-    return response
+        return response
diff --git a/app/monitoring/dashboard.py b/app/monitoring/dashboard.py
index d43eceb..d11a3e8 100644
--- a/app/monitoring/dashboard.py
+++ b/app/monitoring/dashboard.py
@@ -1,86 +1,126 @@
 """
-AI Trader — Streamlit Monitoring Dashboard
+AI Trader — Modern Monitoring Dashboard
 
 Run locally:
-  streamlit run app/monitoring/dashboard.py
+    streamlit run app/monitoring/dashboard.py
 
-Notes:
-- Reads optional DATABASE_URL and TELEGRAM_BOT_TOKEN from environment.
-- If DATABASE_URL is not provided or DB fetch fails, falls back to demo data.
-- Provides sidebar controls (date range, symbols, auto-refresh).
-- Caches queries via st.cache_data with TTL to avoid hammering the DB.
+Key features:
+* Pulls equity/trade data from Postgres (via DATABASE_URL), falls back to demo data.
+* Responsive analyst-style layout with KPI cards, interactive charts, and telemetry panels.
+* Sidebar filters for lookback window, auto-refresh cadence, and symbol overrides.
 """
 
 from __future__ import annotations
 
 import os
-from dataclasses import dataclass
 from datetime import datetime, timedelta, timezone
-from typing import List, Optional
+from typing import Dict, List, Optional, Tuple
 
+import altair as alt
 import numpy as np
 import pandas as pd
 import streamlit as st
 
-# --- Page config & CSS tweaks ---
-st.set_page_config(page_title="AI Trader Dashboard", layout="wide", page_icon="📈")
+# -----------------------------------------------------------------------------
+# Page styling
+# -----------------------------------------------------------------------------
+st.set_page_config(
+    page_title="AI Trader Dashboard",
+    page_icon="📈",
+    layout="wide",
+    initial_sidebar_state="expanded",
+)
+
 st.markdown(
     """
     <style>
-    .small { font-size: 0.85rem; opacity: 0.85; }
-    .ok { color: var(--text-color); }
-    .warn { color: #c77d00; }
-    .bad { color: #b00020; }
+    :root { --accent-color: #3b82f6; }
+    body, [data-testid="stAppViewContainer"] {
+        background: linear-gradient(135deg, #0f172a 0%, #1e293b 45%, #111827 100%);
+        color: #e2e8f0;
+    }
+    [data-testid="stSidebar"] {
+        background-color: #101623 !important;
+        border-right: 1px solid rgba(148, 163, 184, 0.1);
+    }
+    h1, h2, h3, h4, h5, h6 {
+        color: #f8fafc !important;
+        letter-spacing: 0.03em;
+    }
+    .card {
+        background: rgba(15, 23, 42, 0.72);
+        border: 1px solid rgba(148, 163, 184, 0.18);
+        border-radius: 18px;
+        padding: 1.1rem 1.4rem;
+        box-shadow: 0 8px 24px rgba(15, 23, 42, 0.35);
+        backdrop-filter: blur(18px);
+    }
+    .metric-label {
+        text-transform: uppercase;
+        font-size: 0.75rem;
+        letter-spacing: 0.15em;
+        color: rgba(148, 163, 184, 0.9);
+    }
+    .metric-value {
+        font-size: 1.9rem;
+        font-weight: 700;
+        color: #f8fafc;
+    }
+    .metric-delta {
+        font-size: 0.95rem;
+        font-weight: 600;
+    }
+    .metric-delta.positive { color: #34d399; }
+    .metric-delta.negative { color: #f87171; }
+    .metric-subtext {
+        font-size: 0.8rem;
+        color: rgba(148, 163, 184, 0.75);
+    }
+    .stDataFrame div, .stTable, .stCaption, .stTextInput>div>div>input {
+        color: #f8fafc !important;
+    }
+    .stTabs [role="tab"] {
+        color: rgba(241, 245, 249, 0.75) !important;
+        border: none !important;
+        padding: 0.65rem 1rem !important;
+        background: transparent !important;
+        border-radius: 12px 12px 0 0 !important;
+    }
+    .stTabs [role="tab"][aria-selected="true"] {
+        color: #f8fafc !important;
+        background: rgba(59, 130, 246, 0.15) !important;
+        border-bottom: 2px solid var(--accent-color) !important;
+    }
+    .badge {
+        display: inline-flex;
+        padding: 0.25rem 0.55rem;
+        border-radius: 9999px;
+        font-size: 0.7rem;
+        font-weight: 600;
+        background: rgba(59, 130, 246, 0.12);
+        color: #bfdbfe;
+        text-transform: uppercase;
+        letter-spacing: 0.08em;
+    }
     </style>
     """,
     unsafe_allow_html=True,
 )
 
-# --- Simple data-source toggle & config ---
+# -----------------------------------------------------------------------------
+# Configuration & Caching
+# -----------------------------------------------------------------------------
 DB_URL = os.getenv("DATABASE_URL", "")
-TZ_LOCAL = os.getenv("DASHBOARD_TZ", "America/Los_Angeles")
-DEFAULT_SYMBOLS = os.getenv("DASHBOARD_SYMBOLS", "AAPL,MSFT,NVDA,SPY,QQQ").split(",")
-
-# --- Sidebar controls ---
-st.sidebar.header("Controls")
-lookback_days = st.sidebar.slider(
-    "Lookback (days)", min_value=5, max_value=365, value=30, step=5
-)
-auto_refresh_sec = st.sidebar.number_input(
-    "Auto-refresh (sec)",
-    min_value=0,
-    max_value=600,
-    value=60,
-    help="0 disables auto-refresh",
-)
-symbols_input = st.sidebar.text_input(
-    "Symbols (comma-separated)", value=",".join(DEFAULT_SYMBOLS)
-).strip()
-symbols: List[str] = [s.strip().upper() for s in symbols_input.split(",") if s.strip()]
-
-# Auto refresh (client-side)
-if auto_refresh_sec and auto_refresh_sec > 0:
-    st_autorefresh = (
-        st.experimental_rerun
-    )  # placeholder to avoid import; we call none here
-    # Streamlit doesn't have a built-in timer; we provide a manual refresh button instead.
-refresh = st.sidebar.button("🔄 Manual refresh")
-
-
-# --- Helpers -----------------------------------------------------------------
-@dataclass
-class EquitySnapshot:
-    timestamp: datetime
-    equity: float
+TZ_LOCAL = os.getenv("DASHBOARD_TZ", "America/New_York")
+DEFAULT_SYMBOLS = os.getenv("DASHBOARD_SYMBOLS", "AAPL,MSFT,NVDA,SPY,QQQ")
 
 
 def _now_utc() -> datetime:
     return datetime.now(timezone.utc)
 
 
-@st.cache_data(show_spinner=False, ttl=60)
-def _demo_equity(n_points: int = 120) -> pd.DataFrame:
-    """Generate a noisy upward-sloping equity curve for demo."""
+@st.cache_data(show_spinner=False, ttl=45)
+def _demo_equity(n_points: int = 390) -> pd.DataFrame:
     base = 100_000.0
     timestamps = pd.date_range(
         _now_utc() - timedelta(minutes=n_points - 1),
@@ -88,69 +128,37 @@ def _demo_equity(n_points: int = 120) -> pd.DataFrame:
         freq="min",
         tz="UTC",
     )
-    drift = np.linspace(0, 1500, n_points)
-    noise = np.random.randn(n_points) * 200
+    drift = np.linspace(0, 2200, n_points)
+    noise = np.random.randn(n_points) * 120
     equity = base + drift + noise
     return pd.DataFrame({"timestamp": timestamps, "equity": equity}).set_index(
         "timestamp"
     )
 
 
-def _compute_metrics(equity: pd.Series) -> pd.DataFrame:
-    """Compute simple performance metrics from equity series (UTC-indexed)."""
-    if equity.empty:
-        return pd.DataFrame([{"Metric": "Total Return", "Value": "n/a"}])
-
-    eq = equity.dropna().astype(float)
-    total_ret = (eq.iloc[-1] / eq.iloc[0]) - 1.0 if len(eq) > 1 else 0.0
-    rets = eq.pct_change().dropna()
-    ann_factor = 252  # daily-like; if minute-level, this is heuristic. For minute data use 252*390.
-    sharpe = (
-        (rets.mean() / (rets.std() + 1e-12)) * np.sqrt(ann_factor)
-        if len(rets) > 1
-        else 0.0
-    )
-
-    # Max drawdown
-    roll_max = eq.cummax()
-    dd = (eq / roll_max) - 1.0
-    max_dd = dd.min() if not dd.empty else 0.0
-
-    # Format
-    rows = [
-        {"Metric": "Total Return", "Value": f"{total_ret*100:.2f}%"},
-        {"Metric": "Sharpe Ratio", "Value": f"{sharpe:.2f}"},
-        {"Metric": "Max Drawdown", "Value": f"{max_dd*100:.2f}%"},
-        {"Metric": "Samples", "Value": f"{len(eq):,}"},
-    ]
-    return pd.DataFrame(rows)
-
-
 @st.cache_data(show_spinner=False, ttl=30)
 def _fetch_equity_from_db(since: datetime) -> Optional[pd.DataFrame]:
-    """Fetch equity curve from Postgres if DATABASE_URL is configured. Expect a table `equity_snapshots(ts_utc TIMESTAMPTZ, equity NUMERIC)`."""
     if not DB_URL:
         return None
     try:
-        import sqlalchemy as sa  # lazy import so Streamlit still works without it
+        import sqlalchemy as sa  # lazy import
 
-        eng = sa.create_engine(DB_URL, pool_pre_ping=True, pool_size=3, max_overflow=2)
+        engine = sa.create_engine(DB_URL, pool_pre_ping=True)
         query = """
             SELECT ts_utc AS timestamp, equity
             FROM equity_snapshots
             WHERE ts_utc >= :since
             ORDER BY ts_utc ASC
         """
-        with eng.connect() as conn:
+        with engine.connect() as conn:
             df = pd.read_sql(query, conn, params={"since": since})
         if df.empty:
             return None
-        # Ensure tz-aware
         if df["timestamp"].dtype.tz is None:
             df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True)
         return df.set_index("timestamp").sort_index()
-    except Exception as e:
-        st.warning(f"DB equity fetch failed: {e}")
+    except Exception as exc:  # pragma: no cover
+        st.warning(f"Equity query failed: {exc}")
         return None
 
 
@@ -158,93 +166,349 @@ def _fetch_equity_from_db(since: datetime) -> Optional[pd.DataFrame]:
 def _fetch_trades_from_db(
     since: datetime, symbols: List[str] | None
 ) -> Optional[pd.DataFrame]:
-    """Fetch recent trades. Expect a table `trades(symbol TEXT, side TEXT, qty NUMERIC, price NUMERIC, pnl NUMERIC, ts_utc TIMESTAMPTZ)`."""
     if not DB_URL:
         return None
     try:
         import sqlalchemy as sa
 
-        eng = sa.create_engine(DB_URL, pool_pre_ping=True, pool_size=3, max_overflow=2)
-        sym_filter = ""
-        params = {"since": since}
+        engine = sa.create_engine(DB_URL, pool_pre_ping=True)
+        params: Dict[str, object] = {"since": since}
+        sym_clause = ""
         if symbols:
-            sym_filter = " AND symbol = ANY(:symbols) "
+            sym_clause = " AND symbol = ANY(:symbols)"
             params["symbols"] = symbols
         query = f"""
-            SELECT symbol AS "Symbol", side AS "Side", qty AS "Qty", price AS "Price",
-                   pnl AS "PnL", ts_utc AS "Timestamp"
+            SELECT symbol AS "Symbol",
+                   side AS "Side",
+                   qty AS "Qty",
+                   price AS "Entry Price",
+                   pnl AS "PnL",
+                   ts_utc AS "Timestamp"
             FROM trades
             WHERE ts_utc >= :since
-            {sym_filter}
+            {sym_clause}
             ORDER BY ts_utc DESC
-            LIMIT 200
+            LIMIT 150
         """
-        with eng.connect() as conn:
+        with engine.connect() as conn:
             df = pd.read_sql(query, conn, params=params)
         if df.empty:
             return None
         if df["Timestamp"].dtype.tz is None:
             df["Timestamp"] = pd.to_datetime(df["Timestamp"], utc=True)
         return df
-    except Exception as e:
-        st.warning(f"DB trades fetch failed: {e}")
+    except Exception as exc:  # pragma: no cover
+        st.warning(f"Trade query failed: {exc}")
         return None
 
 
-# --- Header ------------------------------------------------------------------
-st.title("AI Trading Agent — Monitoring Dashboard")
-st.caption("Live strategy diagnostics and telemetry view")
+# -----------------------------------------------------------------------------
+# Helpers
+# -----------------------------------------------------------------------------
+def _format_currency(value: float) -> str:
+    if pd.isna(value):
+        return "—"
+    return f"${value:,.0f}"
 
-# --- Account Equity Overview --------------------------------------------------
-st.header("📊 Account Equity Overview")
 
+def _format_delta(value: float) -> Tuple[str, str]:
+    if pd.isna(value) or value == 0:
+        return "0.0%", "neutral"
+    sign = "+" if value > 0 else ""
+    return f"{sign}{value:.2f}%", "positive" if value > 0 else "negative"
+
+
+def _compute_summary(equity: pd.Series) -> Dict[str, float]:
+    eq = equity.dropna().astype(float)
+    if eq.empty:
+        return {"latest": 0.0, "daily": 0.0, "total": 0.0, "drawdown": 0.0, "vol": 0.0}
+
+    latest = float(eq.iloc[-1])
+    daily = (
+        ((eq.iloc[-1] / eq.iloc[-min(len(eq), 390)]) - 1.0) * 100
+        if len(eq) > 1
+        else 0.0
+    )
+    total = ((eq.iloc[-1] / eq.iloc[0]) - 1.0) * 100 if len(eq) > 1 else 0.0
+    roll_max = eq.cummax()
+    drawdown = ((eq / roll_max) - 1.0).min() * 100 if not roll_max.empty else 0.0
+    rets = eq.pct_change().dropna()
+    vol = rets.std() * np.sqrt(252) * 100 if not rets.empty else 0.0
+    return {
+        "latest": latest,
+        "daily": daily,
+        "total": total,
+        "drawdown": drawdown,
+        "vol": vol,
+    }
+
+
+def _render_stat_card(title: str, value: str, delta: str | None, delta_class: str, footnote: str) -> None:
+    st.markdown(
+        f"""
+        <div class="card">
+            <div class="metric-label">{title}</div>
+            <div class="metric-value">{value}</div>
+            {f'<div class="metric-delta {delta_class}">{delta}</div>' if delta else ''}
+            <div class="metric-subtext">{footnote}</div>
+        </div>
+        """,
+        unsafe_allow_html=True,
+    )
+
+
+def _render_equity_chart(df: pd.DataFrame, tz_display: str) -> None:
+    if df.empty:
+        st.warning("No equity data available.")
+        return
+    plot_df = df.copy()
+    plot_df = plot_df.rename(columns={"equity": "Equity"}).reset_index()
+    if tz_display != "UTC":
+        plot_df["timestamp"] = plot_df["timestamp"].dt.tz_convert(tz_display)
+    base = (
+        alt.Chart(plot_df)
+        .mark_line(color="#60a5fa", strokeWidth=2.2)
+        .encode(
+            x=alt.X("timestamp:T", title=f"Time ({tz_display})"),
+            y=alt.Y("Equity:Q", title="Account Equity"),
+            tooltip=[
+                alt.Tooltip("timestamp:T", title="Timestamp"),
+                alt.Tooltip("Equity:Q", title="Equity", format=",.0f"),
+            ],
+        )
+    )
+    area = base.mark_area(opacity=0.18, color="#60a5fa")
+    st.altair_chart((base + area).interactive(), width="stretch")
+
+
+def _render_trades_table(trades: pd.DataFrame) -> None:
+    if trades.empty:
+        st.info("No trades captured within the selected window.")
+        return
+    df = trades.copy()
+    df["PnL"] = df["PnL"].astype(float)
+    styled = df.style.format(
+        {"Entry Price": "${:,.2f}", "PnL": "${:,.2f}"},
+        na_rep="—",
+    )
+
+    def _pnl_style(value: float) -> Optional[str]:
+        if pd.isna(value) or value == 0:
+            return None
+        return "color: #34d399;" if value > 0 else "color: #f87171;"
+
+    styled = styled.map(_pnl_style, subset=pd.IndexSlice[:, ["PnL"]])
+    st.dataframe(styled, width="stretch", height=360)
+
+
+def _sample_positions(equity: pd.Series) -> pd.DataFrame:
+    base_symbols = ["AAPL", "MSFT", "NVDA", "SPY", "QQQ"]
+    weights = np.array([0.22, 0.18, 0.16, 0.24, 0.2])
+    latest = float(equity.iloc[-1]) if not equity.empty else 100_000
+    exposures = latest * weights
+    return pd.DataFrame(
+        {
+            "Symbol": base_symbols,
+            "Net Exposure": exposures,
+            "Weight": weights * 100,
+        }
+    )
+
+
+# -----------------------------------------------------------------------------
+# Sidebar controls
+# -----------------------------------------------------------------------------
+st.sidebar.header("Control Panel")
+lookback_days = st.sidebar.slider(
+    "Lookback window (days)",
+    min_value=5,
+    max_value=365,
+    value=60,
+    step=5,
+)
+auto_refresh_sec = st.sidebar.number_input(
+    "Auto-refresh (sec)",
+    min_value=0,
+    max_value=600,
+    value=0,
+    help="0 disables auto refresh. For production, use 60–120s.",
+)
+symbols_input = st.sidebar.text_input(
+    "Focus symbols",
+    value=DEFAULT_SYMBOLS,
+    help="Comma separated universe filter (optional).",
+)
+symbols: List[str] = [s.strip().upper() for s in symbols_input.split(",") if s.strip()]
+
+refresh_manual = st.sidebar.button("Refresh data", type="primary")
+if auto_refresh_sec and auto_refresh_sec > 0:
+    st.sidebar.caption(
+        f":small_blue_diamond: Auto-refresh requested every {auto_refresh_sec}s (manual refresh recommended during development)."
+    )
+
+
+# -----------------------------------------------------------------------------
+# Data preparation
+# -----------------------------------------------------------------------------
 since = _now_utc() - timedelta(days=lookback_days)
 equity_df = _fetch_equity_from_db(since) or _demo_equity(
     n_points=min(lookback_days * 390, 2000)
 )
+if refresh_manual:
+    st.experimental_rerun()  # safe to rerun manually
 
-# Allow user to view in local timezone
-tz_option = st.selectbox("Display time in", ["UTC", TZ_LOCAL], index=1)
-equity_plot = equity_df.copy()
-if tz_option != "UTC":
-    equity_plot.index = equity_plot.index.tz_convert(tz_option)
-
-st.line_chart(equity_plot.rename(columns={"equity": "Equity"}))
+summary = _compute_summary(equity_df["equity"])
+last_updated_utc = equity_df.index[-1] if not equity_df.empty else _now_utc()
+last_updated_local = last_updated_utc.astimezone(timezone.utc).astimezone(
+    timezone(timedelta(hours=0))
+)
 
-# Metrics block
-st.subheader("Performance Metrics")
-st.table(_compute_metrics(equity_df["equity"]))
+# -----------------------------------------------------------------------------
+# Header & KPI row
+# -----------------------------------------------------------------------------
+st.markdown(
+    f"""
+    <div class="card" style="margin-bottom: 1.2rem;">
+        <div style="display:flex; justify-content: space-between; align-items:center;">
+            <div>
+                <span class="badge">Live Monitoring</span>
+                <h1 style="margin:0.25rem 0 0;">AI Trader — Portfolio Pulse</h1>
+                <div class="metric-subtext" style="margin-top:0.2rem;">
+                    Insights for trading ops teams — track equity, trades, and positions in real time.
+                </div>
+            </div>
+            <div style="text-align:right;">
+                <div class="metric-subtext">Last update (UTC)</div>
+                <div class="metric-value" style="font-size:1.1rem;">{last_updated_utc.strftime('%Y-%m-%d %H:%M')}</div>
+            </div>
+        </div>
+    </div>
+    """,
+    unsafe_allow_html=True,
+)
 
-# --- Recent Trades ------------------------------------------------------------
-st.header("💼 Recent Trades")
-trades_df = _fetch_trades_from_db(since, symbols)  # may be None
+card_cols = st.columns(4)
+with card_cols[0]:
+    _render_stat_card(
+        "Net Liquidity",
+        _format_currency(summary["latest"]),
+        None,
+        "",
+        "Approximate account equity",
+    )
+with card_cols[1]:
+    daily_delta, delta_class = _format_delta(summary["daily"])
+    _render_stat_card(
+        "Daily Move",
+        daily_delta,
+        None,
+        delta_class,
+        "Change vs. session open",
+    )
+with card_cols[2]:
+    total_delta, total_class = _format_delta(summary["total"])
+    _render_stat_card(
+        "Total Return",
+        total_delta,
+        None,
+        total_class,
+        f"Since {lookback_days}-day lookback start",
+    )
+with card_cols[3]:
+    _render_stat_card(
+        "Max Drawdown",
+        f"{summary['drawdown']:.2f}%",
+        None,
+        "negative" if summary["drawdown"] < 0 else "positive",
+        "Peak-to-trough within window",
+    )
 
-if trades_df is None:
-    # Provide a placeholder demo table if DB isn't wired yet
-    demo_trades = pd.DataFrame(
-        {
-            "Symbol": ["AAPL", "NVDA", "MSFT"],
-            "Side": ["BUY", "SELL", "BUY"],
-            "Qty": [10, 5, 12],
-            "Price": [182.5, 442.1, 319.7],
-            "PnL": [150.0, -75.0, 210.5],
-            "Timestamp": [datetime.now(timezone.utc)] * 3,
-        }
+# -----------------------------------------------------------------------------
+# Equity chart & metrics
+# -----------------------------------------------------------------------------
+equity_container = st.container()
+with equity_container:
+    chart_cols = st.columns([2.5, 1])
+    with chart_cols[0]:
+        tz_selection = st.selectbox("Display timezone", ["UTC", TZ_LOCAL], index=0)
+        _render_equity_chart(equity_df, tz_selection)
+    with chart_cols[1]:
+        st.markdown("#### Performance Snapshot")
+        metrics_table = pd.DataFrame(
+            [
+                {"Metric": "Total Return", "Value": f"{summary['total']:.2f}%"},
+                {"Metric": "Daily Move", "Value": f"{summary['daily']:.2f}%"},
+                {"Metric": "Max Drawdown", "Value": f"{summary['drawdown']:.2f}%"},
+                {"Metric": "Annualized Vol", "Value": f"{summary['vol']:.2f}%"},
+            ]
+        )
+        st.table(metrics_table)
+
+# -----------------------------------------------------------------------------
+# Trades & Positions
+# -----------------------------------------------------------------------------
+lower_tabs = st.tabs(["Recent Trades", "Positions", "Operational Notes"])
+
+with lower_tabs[0]:
+    st.markdown("##### Blotter (latest 150 fills)")
+    trades_df = _fetch_trades_from_db(since, symbols) or pd.DataFrame()
+    if trades_df.empty:
+        demo_trades = pd.DataFrame(
+            {
+                "Symbol": ["AAPL", "NVDA", "MSFT", "SPY"],
+                "Side": ["BUY", "SELL", "BUY", "SELL"],
+                "Qty": [10, 5, 12, 8],
+                "Entry Price": [182.5, 442.1, 319.7, 505.2],
+                "PnL": [150.0, -75.0, 210.5, -42.0],
+                "Timestamp": [datetime.now(timezone.utc)] * 4,
+            }
+        )
+        st.info("Demo trades shown — configure DATABASE_URL to surface live fills.")
+        _render_trades_table(demo_trades)
+    else:
+        _render_trades_table(trades_df)
+
+with lower_tabs[1]:
+    st.markdown("##### Net Exposure by Symbol")
+    positions_df = _sample_positions(equity_df["equity"])
+    exposure_chart = (
+        alt.Chart(positions_df)
+        .mark_bar(cornerRadiusTopLeft=6, cornerRadiusTopRight=6, color="#38bdf8")
+        .encode(
+            x=alt.X("Net Exposure:Q", title="Net Exposure ($)"),
+            y=alt.Y("Symbol:N", sort="-x"),
+            tooltip=[
+                alt.Tooltip("Symbol:N"),
+                alt.Tooltip("Net Exposure:Q", format="$,.0f"),
+                alt.Tooltip("Weight:Q", format=".2f"),
+            ],
+        )
+    )
+    st.altair_chart(exposure_chart, width="stretch")
+    st.caption(
+        "Exposure split derived from latest equity. Replace `_sample_positions` with live holdings to reflect actual positions."
     )
-    st.info("Using demo trades — wire Postgres `trades` table to see live fills.")
-    st.dataframe(demo_trades)
-else:
-    st.dataframe(trades_df)
 
-# --- Alerts & Logs ------------------------------------------------------------
-st.header("🚨 Alerts & Logs")
-st.caption("Wire Telegram alerts & backend runtime logs here (follow-up).")
-st.text_area("Runtime Logs", "No alerts. System stable.", height=150)
+with lower_tabs[2]:
+    st.markdown("##### Operational Alerts & Notes")
+    st.write(
+        """
+        - ✅ **Latency**: no anomalies detected.
+        - ⚙️ **Telegram**: webhook heartbeat received in the past 5 minutes.
+        - 📡 **Market data**: Alpaca streaming stable. Yahoo fallback engaged when snapshots unavailable.
+        """
+    )
+    st.text_area(
+        "Runbook notes",
+        "No incidents logged. Use this space for hand-off notes between operators.",
+        height=180,
+    )
 
-# --- Footer -------------------------------------------------------------------
-st.divider()
+# -----------------------------------------------------------------------------
+# Footer
+# -----------------------------------------------------------------------------
+st.markdown("---")
 st.caption(
-    "AI Trader v0.1.0 — Monitoring Layer • "
-    "Set DATABASE_URL to enable live queries • "
-    "Symbols filtered: " + (", ".join(symbols) if symbols else "All")
+    f"AI Trader • Monitoring Dashboard • Symbols: {', '.join(symbols) if symbols else 'All'} • "
+    f"Last refresh (UTC): {last_updated_utc.strftime('%Y-%m-%d %H:%M:%S')}"
 )
diff --git a/app/observability/__init__.py b/app/observability/__init__.py
new file mode 100644
index 0000000..aa93eb6
--- /dev/null
+++ b/app/observability/__init__.py
@@ -0,0 +1,241 @@
+"""Centralized OpenTelemetry configuration helpers for AI Trader.
+
+The functions in this module rely exclusively on environment variables for configuration and
+gracefully degrade (no-op) when optional OpenTelemetry settings are not present.
+"""
+
+from __future__ import annotations
+
+import logging
+from typing import Any, Dict
+import os
+import atexit
+
+from app.settings import get_otel_settings
+
+logger = logging.getLogger(__name__)
+
+_tracing_configured = False
+_metrics_configured = False
+_logging_configured = False
+
+# Module-level handles to providers for shutdown
+_tracer_provider = None
+_meter_provider = None
+_logger_provider = None
+
+
+def configure_observability() -> bool:
+    """Configure tracing, metrics, and logging exporters if the environment requests it."""
+    tracing = configure_tracing()
+    metrics = configure_metrics()
+    logs = configure_logging()
+    return tracing or metrics or logs
+
+
+def configure_tracing() -> bool:
+    """Configure the OpenTelemetry tracer provider if OTLP settings are present."""
+    global _tracing_configured
+
+    if _tracing_configured:
+        return True
+    if not _should_configure_tracing():
+        return False
+
+    try:
+        from opentelemetry import trace
+        from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
+        from opentelemetry.sdk.resources import Resource
+        from opentelemetry.sdk.trace import TracerProvider
+        from opentelemetry.sdk.trace.export import BatchSpanProcessor
+    except ImportError:  # pragma: no cover - optional dependency
+        logger.warning("OpenTelemetry tracing dependencies not installed; tracing disabled.")
+        return False
+
+    try:
+        resource = _build_resource(Resource)
+        tracer_provider = TracerProvider(resource=resource)
+        tracer_provider.add_span_processor(
+            BatchSpanProcessor(
+                OTLPSpanExporter(**_exporter_kwargs("trace"))
+            )
+        )
+        trace.set_tracer_provider(tracer_provider)
+        global _tracer_provider
+        _tracer_provider = tracer_provider
+    except Exception as exc:  # pragma: no cover - defensive guard
+        logger.exception("Failed to configure tracing: %s", exc)
+        return False
+
+    _tracing_configured = True
+    logger.info("Tracing configured via OpenTelemetry.")
+    return True
+
+
+def configure_metrics() -> bool:
+    """Configure the OpenTelemetry meter provider if the environment enables metrics."""
+    global _metrics_configured
+
+    if _metrics_configured:
+        return True
+    if not _should_configure_metrics():
+        return False
+
+    try:
+        from opentelemetry import metrics
+        from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter
+        from opentelemetry.sdk.metrics import MeterProvider
+        from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
+        from opentelemetry.sdk.resources import Resource
+    except ImportError:  # pragma: no cover - optional dependency
+        logger.warning("OpenTelemetry metrics dependencies not installed; metrics disabled.")
+        return False
+
+    try:
+        resource = _build_resource(Resource)
+        metric_exporter = OTLPMetricExporter(**_exporter_kwargs("metric"))
+        reader = PeriodicExportingMetricReader(metric_exporter)
+        _mp = MeterProvider(resource=resource, metric_readers=[reader])
+        metrics.set_meter_provider(_mp)
+        global _meter_provider
+        _meter_provider = _mp
+    except Exception as exc:  # pragma: no cover - defensive guard
+        logger.exception("Failed to configure metrics: %s", exc)
+        return False
+
+    _metrics_configured = True
+    logger.info("Metrics configured via OpenTelemetry.")
+    return True
+
+
+def configure_logging() -> bool:
+    """Configure the OpenTelemetry logger provider if the environment enables log export."""
+    global _logging_configured
+
+    if _logging_configured:
+        return True
+    if not _should_configure_logs():
+        return False
+
+    try:
+        import logging as std_logging
+
+        from opentelemetry._logs import set_logger_provider
+        from opentelemetry.exporter.otlp.proto.http._log_exporter import OTLPLogExporter
+        from opentelemetry.instrumentation.logging import LoggingInstrumentor
+        from opentelemetry.sdk._logs import LoggerProvider
+        from opentelemetry.sdk._logs.export import BatchLogRecordProcessor
+        from opentelemetry.sdk.resources import Resource
+    except ImportError:  # pragma: no cover - optional dependency
+        logger.warning("OpenTelemetry logging dependencies not installed; log export disabled.")
+        return False
+
+    try:
+        resource = _build_resource(Resource)
+        logger_provider = LoggerProvider(resource=resource)
+        logger_provider.add_log_record_processor(
+            BatchLogRecordProcessor(OTLPLogExporter(**_exporter_kwargs("log")))
+        )
+        set_logger_provider(logger_provider)
+        global _logger_provider
+        _logger_provider = logger_provider
+        LoggingInstrumentor().instrument(set_logging_format=True)
+        # Capture stdlib warnings and honor LOG_LEVEL if provided
+        std_logging.captureWarnings(True)
+        _level_name = os.getenv("LOG_LEVEL", "INFO").upper()
+        try:
+            _level = getattr(std_logging, _level_name, std_logging.INFO)
+        except Exception:
+            _level = std_logging.INFO
+        std_logging.getLogger().setLevel(_level)
+        std_logging.getLogger().debug("Python logging bridged to OpenTelemetry.")
+    except Exception as exc:  # pragma: no cover - defensive guard
+        logger.exception("Failed to configure log export: %s", exc)
+        return False
+
+    _logging_configured = True
+    logger.info("Log export configured via OpenTelemetry.")
+    return True
+
+
+def _should_configure_tracing() -> bool:
+    return get_otel_settings().traces_enabled
+
+
+def _should_configure_metrics() -> bool:
+    return get_otel_settings().metrics_enabled
+
+
+def _should_configure_logs() -> bool:
+    return get_otel_settings().logs_enabled
+
+
+def _exporter_kwargs(kind: str) -> Dict[str, Any]:
+    settings = get_otel_settings()
+    base = settings.exporter_otlp_endpoint
+    if kind == "trace":
+        endpoint = settings.exporter_otlp_traces_endpoint or base
+    elif kind == "metric":
+        endpoint = settings.exporter_otlp_metrics_endpoint or base
+    else:
+        endpoint = settings.exporter_otlp_logs_endpoint or base
+
+    kwargs: Dict[str, Any] = {}
+    if endpoint:
+        kwargs["endpoint"] = endpoint
+    headers = dict(settings.parsed_headers)
+    if headers:
+        kwargs["headers"] = headers
+    return kwargs
+
+
+def _build_resource(resource_cls) -> "Resource":
+    """Create an OpenTelemetry resource honoring OTEL_* attributes if present."""
+    attributes: Dict[str, str] = {}
+
+    settings = get_otel_settings()
+
+    service_name = settings.service_name
+    if service_name:
+        attributes["service.name"] = service_name
+
+    for key, value in settings.resource_attributes_map:
+        attributes[key] = value
+
+    default_resource = resource_cls.create({})
+    if not attributes:
+        return default_resource
+    return default_resource.merge(resource_cls.create(attributes))
+
+
+
+# Shutdown helper and atexit hook
+def shutdown_observability() -> None:
+    """Flush and shutdown OTEL providers to ensure logs/traces/metrics are exported on process exit."""
+    # Shut down in reverse order of configuration
+    try:
+        if _logger_provider is not None:
+            _logger_provider.shutdown()
+    except Exception:
+        logger.exception("Error during logger provider shutdown")
+    try:
+        if _meter_provider is not None:
+            _meter_provider.shutdown()
+    except Exception:
+        logger.exception("Error during meter provider shutdown")
+    try:
+        if _tracer_provider is not None:
+            _tracer_provider.shutdown()
+    except Exception:
+        logger.exception("Error during tracer provider shutdown")
+
+# Ensure we always flush on app termination
+atexit.register(shutdown_observability)
+
+__all__ = [
+    "configure_observability",
+    "configure_tracing",
+    "configure_metrics",
+    "configure_logging",
+    "shutdown_observability",
+]
diff --git a/app/providers/yahoo_provider.py b/app/providers/yahoo_provider.py
index 8681a4f..4c88074 100644
--- a/app/providers/yahoo_provider.py
+++ b/app/providers/yahoo_provider.py
@@ -1,7 +1,6 @@
 from __future__ import annotations
 
 import math
-import random
 import threading
 import time
 from datetime import date, datetime, timedelta, timezone
@@ -11,14 +10,12 @@ import requests
 from loguru import logger
 
 from app.utils import env as ENV
-from app.utils.http import http_get
+from app.utils.http import compute_backoff_delay, http_get
 
 # yfinance is optional at runtime; we guard imports and degrade gracefully.
 
 _CHUNK_SIZE = 50  # keep multi-symbol batches reasonable
 _YAHOO_CHART_URL = "https://query1.finance.yahoo.com/v8/finance/chart/{symbol}"
-_YAHOO_BACKOFF = [0.5, 1.0, 2.0]
-
 _breaker_lock = threading.Lock()
 _breaker_failures = 0
 _breaker_open_until = 0.0
@@ -64,10 +61,17 @@ def _yahoo_request(url: str, params: Optional[Dict[str, Any]] = None) -> Tuple[i
         )
         return 503, {"error": "yahoo_circuit_open"}
 
-    timeout = getattr(ENV, "HTTP_TIMEOUT_SECS", 10)
+    timeout = float(
+        getattr(ENV, "HTTP_TIMEOUT", getattr(ENV, "HTTP_TIMEOUT_SECS", 10))
+    )
+    retries = max(0, int(getattr(ENV, "HTTP_RETRIES", 2)))
+    backoff = float(
+        getattr(ENV, "HTTP_BACKOFF", getattr(ENV, "HTTP_RETRY_BACKOFF_SEC", 1.5))
+    )
     headers = {"User-Agent": getattr(ENV, "HTTP_USER_AGENT", "ai-trader/1.0")}
 
-    for attempt in range(len(_YAHOO_BACKOFF) + 1):
+    last_status = 0
+    for attempt in range(retries + 1):
         try:
             resp = requests.get(
                 url,
@@ -75,10 +79,11 @@ def _yahoo_request(url: str, params: Optional[Dict[str, Any]] = None) -> Tuple[i
                 headers=headers,
                 timeout=timeout,
             )
+            last_status = resp.status_code
         except requests.RequestException as exc:
             logger.warning("yahoo request error attempt={} url={} error={}", attempt + 1, url, exc)
-            if attempt < len(_YAHOO_BACKOFF):
-                sleep = _YAHOO_BACKOFF[attempt] * random.uniform(0.75, 1.25)
+            if attempt < retries:
+                sleep = compute_backoff_delay(attempt, backoff, None)
                 time.sleep(sleep)
                 continue
             return 599, {}
@@ -93,8 +98,10 @@ def _yahoo_request(url: str, params: Optional[Dict[str, Any]] = None) -> Tuple[i
                 attempt + 1,
                 url,
             )
-            if attempt < len(_YAHOO_BACKOFF):
-                sleep = _YAHOO_BACKOFF[attempt] * random.uniform(0.75, 1.25)
+            if attempt < retries:
+                sleep = compute_backoff_delay(
+                    attempt, backoff, resp.headers.get("Retry-After")
+                )
                 time.sleep(sleep)
                 continue
             _breaker_record_throttle()
@@ -115,7 +122,8 @@ def _yahoo_request(url: str, params: Optional[Dict[str, Any]] = None) -> Tuple[i
             return resp.status_code, {}
 
     _breaker_record_throttle()
-    return 429, {"error": "yahoo_throttled"}
+    status = last_status or 429
+    return status, {"error": "yahoo_throttled"}
 
 if TYPE_CHECKING:
     # for type hints without importing pandas at runtime
@@ -223,9 +231,9 @@ def _fetch_chart_history(
     status, data = http_get(
         _YAHOO_CHART_URL.format(symbol=symbol.upper()),
         params=params,
-        timeout=getattr(ENV, "HTTP_TIMEOUT_SECS", 10),
+        timeout=getattr(ENV, "HTTP_TIMEOUT", getattr(ENV, "HTTP_TIMEOUT_SECS", 10)),
         retries=getattr(ENV, "HTTP_RETRIES", 2),
-        backoff=getattr(ENV, "HTTP_BACKOFF", [0.5, 1.0, 2.0]),
+        backoff=getattr(ENV, "HTTP_BACKOFF", getattr(ENV, "HTTP_RETRY_BACKOFF_SEC", 1.5)),
         headers={"User-Agent": getattr(ENV, "HTTP_USER_AGENT", "ai-trader/1.0")},
     )
 
diff --git a/app/services/market_data.py b/app/services/market_data.py
new file mode 100644
index 0000000..3714bac
--- /dev/null
+++ b/app/services/market_data.py
@@ -0,0 +1,438 @@
+from __future__ import annotations
+
+import os
+from datetime import datetime, timezone
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
+
+import numpy as np
+import pandas as pd
+import requests
+
+from app.adapters.market.alpaca_client import AlpacaAuthError, AlpacaMarketClient
+from app.utils import env as ENV
+
+try:  # optional dependency for redundancy
+    import yfinance as yf  # type: ignore
+except ImportError:  # pragma: no cover
+    yf = None
+
+ALPHAVANTAGE_URL = "https://www.alphavantage.co/query"
+FINNHUB_URL = "https://finnhub.io/api/v1"
+TWELVEDATA_URL = "https://api.twelvedata.com"
+
+
+Snapshot = Dict[str, Dict[str, Optional[float]]]
+
+
+def _now_iso() -> str:
+    return datetime.now(timezone.utc).isoformat()
+
+
+def _clean_symbol(symbol: str) -> str:
+    return (symbol or "").strip().upper()
+
+
+def _parse_timestamp(value: object) -> Optional[datetime]:
+    if value is None:
+        return None
+    if isinstance(value, (int, float)):
+        try:
+            return datetime.fromtimestamp(float(value), tz=timezone.utc)
+        except Exception:
+            return None
+    if isinstance(value, str):
+        candidate = value.strip()
+        if not candidate:
+            return None
+        candidate = candidate.replace(" ", "T")
+        try:
+            dt = datetime.fromisoformat(candidate)
+            if dt.tzinfo is None:
+                dt = dt.replace(tzinfo=timezone.utc)
+            return dt
+        except Exception:
+            try:
+                dt = datetime.strptime(candidate, "%Y-%m-%dT%H:%M:%S")
+                return dt.replace(tzinfo=timezone.utc)
+            except Exception:
+                return None
+    return None
+
+
+def _alpha_quote(symbols: Sequence[str]) -> Tuple[Dict[str, Snapshot], Optional[str]]:
+    api_key = getattr(ENV, "ALPHAVANTAGE_API_KEY", "")
+    if not api_key:
+        return {}, None
+    out: Dict[str, Snapshot] = {}
+    for sym in symbols:
+        params = {
+            "function": "GLOBAL_QUOTE",
+            "symbol": sym,
+            "apikey": api_key,
+        }
+        try:
+            resp = requests.get(ALPHAVANTAGE_URL, params=params, timeout=ENV.HTTP_TIMEOUT)
+            payload = resp.json().get("Global Quote", {}) if resp.status_code == 200 else {}
+            price = float(payload.get("05. price", "nan")) if payload else float("nan")
+            if not payload or np.isnan(price):
+                continue
+            out[sym] = {
+                "latestTrade": {
+                    "price": price,
+                    "timestamp": payload.get("07. latest trading day") or _now_iso(),
+                },
+                "dailyBar": {
+                    "o": float(payload.get("02. open", "nan")),
+                    "c": price,
+                    "h": float(payload.get("03. high", "nan")),
+                    "l": float(payload.get("04. low", "nan")),
+                },
+            }
+        except Exception:
+            continue
+    note = "Alpha Vantage" if out else None
+    return out, note
+
+
+def _finnhub_quote(symbols: Sequence[str]) -> Tuple[Dict[str, Snapshot], Optional[str]]:
+    api_key = os.getenv("FINNHUB_API_KEY") or getattr(ENV, "FINNHUB_API_KEY", "")
+    if not api_key:
+        return {}, None
+    out: Dict[str, Snapshot] = {}
+    for sym in symbols:
+        try:
+            resp = requests.get(
+                f"{FINNHUB_URL}/quote",
+                params={"symbol": sym, "token": api_key},
+                timeout=ENV.HTTP_TIMEOUT,
+            )
+            data = resp.json() if resp.status_code == 200 else {}
+            price = data.get("c")
+            if price in (None, 0):
+                continue
+            out[sym] = {
+                "latestTrade": {
+                    "price": float(price),
+                    "timestamp": (
+                        datetime.fromtimestamp(data.get("t", 0), tz=timezone.utc).isoformat()
+                        if data.get("t")
+                        else _now_iso()
+                    ),
+                },
+                "dailyBar": {
+                    "o": float(data.get("o", 0.0)),
+                    "c": float(price),
+                    "h": float(data.get("h", 0.0)),
+                    "l": float(data.get("l", 0.0)),
+                },
+            }
+        except Exception:
+            continue
+    note = "Finnhub" if out else None
+    return out, note
+
+
+def _twelvedata_quote(symbols: Sequence[str]) -> Tuple[Dict[str, Snapshot], Optional[str]]:
+    api_key = os.getenv("TWELVEDATA_API_KEY") or getattr(ENV, "TWELVEDATA_API_KEY", "")
+    if not api_key:
+        return {}, None
+    out: Dict[str, Snapshot] = {}
+    for sym in symbols:
+        try:
+            resp = requests.get(
+                f"{TWELVEDATA_URL}/quote",
+                params={"symbol": sym, "apikey": api_key},
+                timeout=ENV.HTTP_TIMEOUT,
+            )
+            data = resp.json() if resp.status_code == 200 else {}
+            price = data.get("close")
+            if price in (None, ""):
+                continue
+            ts = data.get("datetime")
+            parsed_ts = _parse_timestamp(ts)
+            out[sym] = {
+                "latestTrade": {
+                    "price": float(price),
+                    "timestamp": (parsed_ts or datetime.now(timezone.utc)).isoformat(),
+                },
+                "dailyBar": {
+                    "o": float(data.get("open", 0.0)),
+                    "c": float(price),
+                    "h": float(data.get("high", 0.0)),
+                    "l": float(data.get("low", 0.0)),
+                },
+            }
+        except Exception:
+            continue
+    note = "Twelve Data" if out else None
+    return out, note
+
+
+def _yahoo_quote(symbols: Sequence[str]) -> Tuple[Dict[str, Snapshot], Optional[str]]:
+    if yf is None:
+        return {}, None
+    out: Dict[str, Snapshot] = {}
+    for sym in symbols:
+        try:
+            ticker = yf.Ticker(sym)
+            info = ticker.fast_info
+            price = info.get("lastPrice") or info.get("last_price")
+            if price in (None, 0):
+                continue
+            out[sym] = {
+                "latestTrade": {"price": float(price), "timestamp": _now_iso()},
+                "dailyBar": {
+                    "o": float(info.get("open", 0.0)),
+                    "c": float(price),
+                    "h": float(info.get("dayHigh", 0.0)),
+                    "l": float(info.get("dayLow", 0.0)),
+                },
+            }
+        except Exception:
+            continue
+    note = "Yahoo Finance" if out else None
+    return out, note
+
+
+def _alpaca_quote(symbols: Sequence[str]) -> Tuple[Dict[str, Snapshot], Optional[str]]:
+    try:
+        client = AlpacaMarketClient()
+    except Exception:
+        return {}, None
+    out: Dict[str, Snapshot] = {}
+    for sym in symbols:
+        try:
+            status, payload = client.snapshots([sym])
+            if status != 200:
+                continue
+            snap = (payload or {}).get(sym)
+            if not snap:
+                continue
+            trade = snap.get("latestTrade") or {}
+            bar = snap.get("dailyBar") or {}
+            price = trade.get("price") or bar.get("c")
+            if price is None:
+                continue
+            out[sym] = {
+                "latestTrade": {
+                    "price": float(price),
+                    "timestamp": trade.get("timestamp") or _now_iso(),
+                },
+                "dailyBar": {
+                    "o": float(bar.get("o", 0.0)),
+                    "c": float(price),
+                    "h": float(bar.get("h", 0.0)),
+                    "l": float(bar.get("l", 0.0)),
+                },
+            }
+        except AlpacaAuthError:
+            break
+        except Exception:
+            continue
+    note = "Alpaca" if out else None
+    return out, note
+
+
+def get_market_snapshots(symbols: Iterable[str]) -> Tuple[Dict[str, Snapshot], str]:
+    ordered = [_clean_symbol(sym) for sym in symbols if sym]
+    remaining = [sym for sym in ordered if sym]
+    snapshots: Dict[str, Snapshot] = {}
+    provenance: Dict[str, str] = {}
+    notes: List[str] = []
+
+    providers = [
+        ("Alpha Vantage", _alpha_quote),
+        ("Finnhub", _finnhub_quote),
+        ("Twelve Data", _twelvedata_quote),
+        ("Yahoo Finance", _yahoo_quote),
+        ("Alpaca", _alpaca_quote),
+    ]
+
+    for label, fetcher in providers:
+        missing = [sym for sym in remaining if sym not in snapshots]
+        if not missing:
+            break
+        data, provider_note = fetcher(missing)
+        if not data:
+            continue
+        for sym, payload in data.items():
+            if sym in snapshots:
+                continue
+            snapshots[sym] = payload
+            provenance[sym] = label
+        if provider_note:
+            notes.append(provider_note)
+
+    if not snapshots:
+        return {}, "No data"
+
+    ordered_labels = list(dict.fromkeys(provenance.values()))
+    summary = " / ".join(ordered_labels)
+    return snapshots, summary
+
+
+def _alpha_bars(symbol: str, interval: str) -> Optional[pd.DataFrame]:
+    api_key = getattr(ENV, "ALPHAVANTAGE_API_KEY", "")
+    if not api_key:
+        return None
+    params = {
+        "function": "TIME_SERIES_INTRADAY",
+        "symbol": symbol,
+        "interval": interval,
+        "apikey": api_key,
+        "outputsize": "compact",
+    }
+    try:
+        resp = requests.get(ALPHAVANTAGE_URL, params=params, timeout=ENV.HTTP_TIMEOUT)
+        if resp.status_code != 200:
+            return None
+        key = next((k for k in resp.json().keys() if k.startswith("Time Series")), None)
+        if not key:
+            return None
+        data = resp.json()[key]
+        rows = []
+        for ts, values in data.items():
+            parsed = _parse_timestamp(ts)
+            if not parsed:
+                continue
+            rows.append((parsed, float(values.get("4. close", 0.0))))
+        if not rows:
+            return None
+        rows.sort(key=lambda x: x[0])
+        return pd.DataFrame({"close": [v for _, v in rows]}, index=[t for t, _ in rows])
+    except Exception:
+        return None
+
+
+def _finnhub_bars(symbol: str, resolution: str, count: int) -> Optional[pd.DataFrame]:
+    api_key = os.getenv("FINNHUB_API_KEY") or getattr(ENV, "FINNHUB_API_KEY", "")
+    if not api_key:
+        return None
+    try:
+        resp = requests.get(
+            f"{FINNHUB_URL}/stock/candle",
+            params={
+                "symbol": symbol,
+                "resolution": resolution,
+                "count": count,
+                "token": api_key,
+            },
+            timeout=ENV.HTTP_TIMEOUT,
+        )
+        data = resp.json() if resp.status_code == 200 else {}
+        if data.get("s") != "ok":
+            return None
+        timestamps = [datetime.fromtimestamp(ts, tz=timezone.utc) for ts in data.get("t", [])]
+        closes = data.get("c", [])
+        if not timestamps or not closes:
+            return None
+        return pd.DataFrame({"close": closes}, index=timestamps)
+    except Exception:
+        return None
+
+
+def _twelvedata_bars(symbol: str, interval: str, outputsize: int) -> Optional[pd.DataFrame]:
+    api_key = os.getenv("TWELVEDATA_API_KEY") or getattr(ENV, "TWELVEDATA_API_KEY", "")
+    if not api_key:
+        return None
+    try:
+        resp = requests.get(
+            f"{TWELVEDATA_URL}/time_series",
+            params={
+                "symbol": symbol,
+                "interval": interval,
+                "outputsize": outputsize,
+                "apikey": api_key,
+            },
+            timeout=ENV.HTTP_TIMEOUT,
+        )
+        data = resp.json() if resp.status_code == 200 else {}
+        values = data.get("values", [])
+        if not values:
+            return None
+        rows = []
+        for item in values:
+            parsed = _parse_timestamp(item.get("datetime"))
+            if not parsed:
+                continue
+            rows.append((parsed, float(item.get("close", 0.0))))
+        rows.sort(key=lambda x: x[0])
+        return pd.DataFrame({"close": [v for _, v in rows]}, index=[t for t, _ in rows])
+    except Exception:
+        return None
+
+
+def _yahoo_bars(symbol: str, interval: str, period: str) -> Optional[pd.DataFrame]:
+    if yf is None:
+        return None
+    try:
+        ticker = yf.Ticker(symbol)
+        hist = ticker.history(interval=interval, period=period)
+        if hist.empty:
+            return None
+        return pd.DataFrame({"close": hist["Close"]}).sort_index()
+    except Exception:
+        return None
+
+
+def _alpaca_bars(symbol: str, timeframe: str, limit: int) -> Optional[pd.DataFrame]:
+    headers = _alpaca_headers()
+    if not headers:
+        return None
+    base_url = ENV.ALPACA_DATA_BASE_URL.rstrip("/")
+    try:
+        resp = requests.get(
+            f"{base_url}/stocks/{symbol}/bars",
+            params={"timeframe": timeframe, "limit": limit},
+            headers=headers,
+            timeout=ENV.HTTP_TIMEOUT,
+        )
+        if resp.status_code != 200:
+            return None
+        payload = resp.json().get("bars") or []
+        rows = []
+        for bar in payload:
+            ts = _parse_timestamp(bar.get("t")) or datetime.now(timezone.utc)
+            rows.append((ts, float(bar.get("c", 0.0))))
+        if not rows:
+            return None
+        rows.sort(key=lambda x: x[0])
+        return pd.DataFrame({"close": [v for _, v in rows]}, index=[t for t, _ in rows])
+    except Exception:
+        return None
+
+
+def _alpaca_headers() -> Optional[Dict[str, str]]:
+    key = ENV.ALPACA_API_KEY
+    secret = ENV.ALPACA_API_SECRET
+    if not key or not secret:
+        return None
+    return {"APCA-API-KEY-ID": key, "APCA-API-SECRET-KEY": secret}
+
+
+def get_intraday_bars(symbol: str, *, timeframe: str = "1H") -> pd.DataFrame:
+    symbol = _clean_symbol(symbol)
+    if not symbol:
+        return pd.DataFrame(columns=["close"])
+    timeframe = timeframe or "1H"
+    interval_mapping = {
+        "1H": ("60min", "60", "1h"),
+        "15m": ("15min", "15", "15m"),
+        "5m": ("5min", "5", "5m"),
+    }
+    alpha_interval, finnhub_interval, twelve_interval = interval_mapping.get(timeframe, ("60min", "60", "1h"))
+    limit = 200 if timeframe == "5m" else 120
+
+    providers = [
+        lambda: _alpha_bars(symbol, alpha_interval),
+        lambda: _finnhub_bars(symbol, finnhub_interval, limit),
+        lambda: _twelvedata_bars(symbol, twelve_interval, limit),
+        lambda: _yahoo_bars(symbol, interval=timeframe.lower(), period="5d" if timeframe != "1H" else "1mo"),
+        lambda: _alpaca_bars(symbol, timeframe.upper(), limit),
+    ]
+
+    for fetch in providers:
+        df = fetch()
+        if df is not None and not df.empty:
+            return df.sort_index()
+    return pd.DataFrame(columns=["close"])
diff --git a/app/services/watchlist_service.py b/app/services/watchlist_service.py
index 838a265..6edf7ef 100644
--- a/app/services/watchlist_service.py
+++ b/app/services/watchlist_service.py
@@ -4,8 +4,12 @@ from __future__ import annotations
 from typing import Iterable, List, Optional
 
 # Existing sources you already have
-from app.source.finviz_source import get_symbols as finviz_symbols
 from app.source.textlist_source import get_symbols as textlist_symbols
+from app.services.watchlist_sources import (
+    fetch_alpha_vantage_symbols,
+    fetch_finnhub_symbols,
+    fetch_twelvedata_symbols,
+)
 
 
 def _dedupe(seq: Iterable[str]) -> List[str]:
@@ -31,7 +35,7 @@ def build_watchlist(
     """
     Return a deduped list of symbols from the requested source.
 
-    source: 'auto' | 'finviz' | 'textlist'
+    source: 'auto' | 'alpha' | 'finnhub' | 'textlist' | 'manual'
     scanner: optional scanner name (e.g., 'top_gainers'); handled by source
     limit: cap result length
     sort: 'alpha' or None
@@ -42,9 +46,21 @@ def build_watchlist(
 
     symbols: List[str] = []
 
-    def _fetch_finviz() -> List[str]:
+    def _fetch_alpha() -> List[str]:
         try:
-            return list(finviz_symbols(scanner=scanner))
+            return list(fetch_alpha_vantage_symbols(scanner=scanner))
+        except Exception:
+            return []
+
+    def _fetch_finnhub() -> List[str]:
+        try:
+            return list(fetch_finnhub_symbols(scanner=scanner))
+        except Exception:
+            return []
+
+    def _fetch_twelvedata() -> List[str]:
+        try:
+            return list(fetch_twelvedata_symbols(scanner=scanner))
         except Exception:
             return []
 
@@ -54,15 +70,21 @@ def build_watchlist(
         except Exception:
             return []
 
-    if source == "finviz":
-        symbols = _fetch_finviz()
+    if source == "alpha":
+        symbols = _fetch_alpha()
+    elif source == "finnhub":
+        symbols = _fetch_finnhub()
     elif source == "textlist":
         symbols = _fetch_textlist()
     else:
-        # auto: prefer finviz, fallback to textlist
-        symbols = _fetch_finviz()
+        # auto: Alpha Vantage, fallback to Finnhub then textlist, then Twelve Data
+        symbols = _fetch_alpha()
+        if not symbols:
+            symbols = _fetch_finnhub()
         if not symbols:
             symbols = _fetch_textlist()
+        if not symbols:
+            symbols = _fetch_twelvedata()
 
     symbols = _dedupe(symbols)
 
@@ -72,4 +94,4 @@ def build_watchlist(
     if limit is not None and limit > 0:
         symbols = symbols[:limit]
 
-    return symbols
\ No newline at end of file
+    return symbols
diff --git a/app/services/watchlist_sources.py b/app/services/watchlist_sources.py
new file mode 100644
index 0000000..39a48fe
--- /dev/null
+++ b/app/services/watchlist_sources.py
@@ -0,0 +1,77 @@
+from __future__ import annotations
+
+import os
+import time
+from typing import Iterable, Iterator, List, Optional
+
+import requests
+
+from app.utils import env as ENV
+
+ALPHAVANTAGE_ENDPOINT = "https://www.alphavantage.co/query"
+FINNHUB_ENDPOINT = "https://finnhub.io/api/v1"
+TWELVEDATA_ENDPOINT = "https://api.twelvedata.com"
+
+
+def _normalize(symbols: Iterable[str]) -> List[str]:
+    uniq = []
+    seen = set()
+    for raw in symbols:
+        sym = (raw or "").strip().upper()
+        if not sym or sym in seen:
+            continue
+        seen.add(sym)
+        uniq.append(sym)
+    return uniq
+
+
+def fetch_alpha_vantage_symbols(*, scanner: Optional[str] = None, limit: int = 50) -> List[str]:
+    api_key = getattr(ENV, "ALPHAVANTAGE_API_KEY", "")
+    if not api_key:
+        return []
+    params = {
+        "function": "LISTING_STATUS",
+        "state": "active",
+        "apikey": api_key,
+    }
+    try:
+        resp = requests.get(ALPHAVANTAGE_ENDPOINT, params=params, timeout=ENV.HTTP_TIMEOUT)
+        if resp.status_code != 200:
+            return []
+        lines = resp.text.splitlines()[1:limit + 1]
+        symbols = [line.split(",")[0] for line in lines if line]
+        return _normalize(symbols[:limit])
+    except Exception:
+        return []
+
+
+def fetch_finnhub_symbols(*, scanner: Optional[str] = None, limit: int = 50) -> List[str]:
+    api_key = os.getenv("FINNHUB_API_KEY") or getattr(ENV, "FINNHUB_API_KEY", "")
+    if not api_key:
+        return []
+    params = {"exchange": "US", "token": api_key}
+    try:
+        resp = requests.get(f"{FINNHUB_ENDPOINT}/stock/symbol", params=params, timeout=ENV.HTTP_TIMEOUT)
+        if resp.status_code != 200:
+            return []
+        data = resp.json()
+        symbols = [item.get("symbol", "") for item in data if item.get("type") == "Common Stock"]
+        return _normalize(symbols[:limit])
+    except Exception:
+        return []
+
+
+def fetch_twelvedata_symbols(*, scanner: Optional[str] = None, limit: int = 50) -> List[str]:
+    api_key = os.getenv("TWELVEDATA_API_KEY") or getattr(ENV, "TWELVEDATA_API_KEY", "")
+    if not api_key:
+        return []
+    params = {"source": "docs", "apikey": api_key}
+    try:
+        resp = requests.get(f"{TWELVEDATA_ENDPOINT}/stocks", params=params, timeout=ENV.HTTP_TIMEOUT)
+        if resp.status_code != 200:
+            return []
+        data = resp.json().get("data", [])
+        symbols = [item.get("symbol", "") for item in data if item.get("currency") == "USD"]
+        return _normalize(symbols[:limit])
+    except Exception:
+        return []
diff --git a/app/settings.py b/app/settings.py
new file mode 100644
index 0000000..c7f4a02
--- /dev/null
+++ b/app/settings.py
@@ -0,0 +1,347 @@
+"""Centralized application settings powered by Pydantic.
+
+Environment matrix:
+
+| Section  | Environment Variable            | Default                     | Purpose                                  |
+|----------|---------------------------------|-----------------------------|------------------------------------------|
+| OTEL     | `OTEL_EXPORTER_OTLP_ENDPOINT`   | `None`                      | Root OTLP collector endpoint             |
+| OTEL     | `OTEL_EXPORTER_OTLP_HEADERS`    | `None`                      | Additional OTLP request headers          |
+| OTEL     | `OTEL_SERVICE_NAME`             | `ai-trader`                 | Logical service identifier               |
+| OTEL     | `OTEL_RESOURCE_ATTRIBUTES`      | `None`                      | Extra resource attributes (key=value)    |
+| OTEL     | `OTEL_LOGS_EXPORTER`            | `None`                      | Logs exporter implementation             |
+| OTEL     | `OTEL_TRACES_EXPORTER`          | `None`                      | Traces exporter implementation           |
+| OTEL     | `OTEL_METRICS_EXPORTER`         | `None`                      | Metrics exporter implementation          |
+| Sentry   | `SENTRY_DSN`                    | `None`                      | Sentry ingest DSN                        |
+| Sentry   | `SENTRY_TRACES_SAMPLE_RATE`     | `0.0`                       | Fraction of transactions to trace        |
+| Sentry   | `SENTRY_ENVIRONMENT`            | `None`                      | Deployment environment label             |
+| Telegram | `TELEGRAM_BOT_TOKEN`            | `None`                      | Telegram bot token                       |
+| Telegram | `TELEGRAM_ALLOWED_USER_IDS`     | `""`                        | Comma-separated allowlist of user IDs    |
+| Telegram | `TELEGRAM_DEFAULT_CHAT_ID`      | `None`                      | Default destination chat/channel         |
+| Telegram | `TELEGRAM_WEBHOOK_SECRET`       | `None`                      | Shared secret for webhook validation     |
+| Telegram | `TELEGRAM_TIMEOUT_SECS`         | `10`                        | HTTP timeout for Telegram API calls      |
+| Database | `DATABASE_URL`                  | `None`                      | Primary Postgres connection URI          |
+| Database | `TEST_DATABASE_URL`             | `None`                      | Fallback Postgres URI for tests/CI       |
+| Database | `PGHOST`                        | `localhost`                 | Postgres host when building DSN manually |
+| Database | `PGPORT`                        | `5432`                      | Postgres port                            |
+| Database | `PGDATABASE`                    | `ai_trader`                 | Postgres database                        |
+| Database | `PGUSER`                        | `postgres`                  | Postgres user                            |
+| Database | `PGPASSWORD`                    | `""`                        | Postgres password                        |
+| Database | `PGSSLMODE`                     | `prefer`                    | Postgres SSL mode                        |
+
+The settings objects below expose structured access to these values along with a few
+helper properties to streamline conditional logic (e.g., whether tracing or Sentry is
+enabled). They source environment variables at import-time and are intended to be
+treated as read-only.
+"""
+
+from __future__ import annotations
+
+import os
+from functools import cached_property
+from typing import List, Tuple
+from urllib.parse import quote_plus
+
+from pydantic import BaseModel, Field, computed_field, field_validator
+from pydantic_settings import BaseSettings, SettingsConfigDict
+
+
+class _SettingsBase(BaseSettings):
+    """Common configuration for BaseSettings subclasses."""
+
+    model_config = SettingsConfigDict(
+        env_prefix="",
+        populate_by_name=True,
+        extra="ignore",
+        case_sensitive=True,
+        frozen=True,
+    )
+
+
+class OTELSettings(_SettingsBase):
+    """OpenTelemetry exporter configuration."""
+
+    exporter_otlp_endpoint: str | None = Field(
+        default=None, alias="OTEL_EXPORTER_OTLP_ENDPOINT"
+    )
+    exporter_otlp_traces_endpoint: str | None = Field(
+        default=None, alias="OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"
+    )
+    exporter_otlp_metrics_endpoint: str | None = Field(
+        default=None, alias="OTEL_EXPORTER_OTLP_METRICS_ENDPOINT"
+    )
+    exporter_otlp_logs_endpoint: str | None = Field(
+        default=None, alias="OTEL_EXPORTER_OTLP_LOGS_ENDPOINT"
+    )
+    exporter_otlp_headers: str | None = Field(
+        default=None, alias="OTEL_EXPORTER_OTLP_HEADERS"
+    )
+    logs_exporter: str | None = Field(default=None, alias="OTEL_LOGS_EXPORTER")
+    traces_exporter: str | None = Field(default=None, alias="OTEL_TRACES_EXPORTER")
+    metrics_exporter: str | None = Field(default=None, alias="OTEL_METRICS_EXPORTER")
+    service_name: str = Field(default="ai-trader", alias="OTEL_SERVICE_NAME")
+    resource_attributes: str | None = Field(
+        default=None, alias="OTEL_RESOURCE_ATTRIBUTES"
+    )
+    python_log_level: str | None = Field(
+        default=None, alias="OTEL_PYTHON_LOG_LEVEL"
+    )
+
+    @computed_field
+    @property
+    def traces_enabled(self) -> bool:
+        return any(
+            value
+            for value in (
+                self.exporter_otlp_endpoint,
+                self.exporter_otlp_traces_endpoint,
+                self.traces_exporter,
+            )
+        )
+
+    @computed_field
+    @property
+    def metrics_enabled(self) -> bool:
+        return any(
+            value
+            for value in (
+                self.exporter_otlp_endpoint,
+                self.exporter_otlp_metrics_endpoint,
+                self.metrics_exporter,
+            )
+        )
+
+    @computed_field
+    @property
+    def logs_enabled(self) -> bool:
+        return any(
+            value
+            for value in (
+                self.exporter_otlp_endpoint,
+                self.exporter_otlp_logs_endpoint,
+                self.logs_exporter,
+            )
+        )
+
+    @computed_field
+    @property
+    def parsed_headers(self) -> Tuple[Tuple[str, str], ...]:
+        raw = (self.exporter_otlp_headers or "").strip()
+        if not raw:
+            return tuple()
+        pairs: List[Tuple[str, str]] = []
+        for part in raw.split(","):
+            key, _, value = part.partition("=")
+            key = key.strip()
+            value = value.strip()
+            if key and value:
+                pairs.append((key, value))
+        return tuple(pairs)
+
+    @computed_field
+    @property
+    def resource_attributes_map(self) -> Tuple[Tuple[str, str], ...]:
+        raw = (self.resource_attributes or "").strip()
+        if not raw:
+            return tuple()
+        pairs: List[Tuple[str, str]] = []
+        for part in raw.split(","):
+            key, _, value = part.partition("=")
+            key = key.strip()
+            value = value.strip()
+            if key and value:
+                pairs.append((key, value))
+        return tuple(pairs)
+
+
+class SentrySettings(_SettingsBase):
+    """Sentry SDK configuration."""
+
+    dsn: str | None = Field(default=None, alias="SENTRY_DSN")
+    traces_sample_rate: float = Field(default=0.0, alias="SENTRY_TRACES_SAMPLE_RATE")
+    environment: str | None = Field(default=None, alias="SENTRY_ENVIRONMENT")
+
+    @computed_field
+    @property
+    def enabled(self) -> bool:
+        return bool(self.dsn)
+
+
+class TelegramSettings(_SettingsBase):
+    """Telegram notifier configuration."""
+
+    bot_token: str | None = Field(default=None, alias="TELEGRAM_BOT_TOKEN")
+    allowed_user_ids_raw: str | None = Field(
+        default=None, alias="TELEGRAM_ALLOWED_USER_IDS"
+    )
+    default_chat_id: str | None = Field(
+        default=None, alias="TELEGRAM_DEFAULT_CHAT_ID"
+    )
+    webhook_secret: str | None = Field(
+        default=None, alias="TELEGRAM_WEBHOOK_SECRET"
+    )
+    timeout_secs: int = Field(default=10, alias="TELEGRAM_TIMEOUT_SECS")
+    fake_mode: bool = Field(default=False, alias="TELEGRAM_FAKE")
+
+    @field_validator("timeout_secs", mode="before")
+    @classmethod
+    def _coerce_timeout(cls, value: int | str | None) -> int:
+        if value in (None, ""):
+            return 10
+        try:
+            return int(value)
+        except (TypeError, ValueError):
+            return 10
+
+    @computed_field
+    @property
+    def allowed_user_ids(self) -> Tuple[int, ...]:
+        raw = (self.allowed_user_ids_raw or "").strip()
+        if not raw:
+            return tuple()
+        ids: List[int] = []
+        for token in raw.split(","):
+            token = token.strip()
+            if not token:
+                continue
+            try:
+                ids.append(int(token))
+            except ValueError:
+                continue
+        return tuple(ids)
+
+    @computed_field
+    @property
+    def enabled(self) -> bool:
+        return bool(self.bot_token)
+
+    @computed_field
+    @property
+    def effective_fake_mode(self) -> bool:
+        env = (os.getenv("ENV") or "dev").lower()
+        return False if env == "prod" else bool(self.fake_mode)
+
+
+class DatabaseSettings(_SettingsBase):
+    """Postgres configuration, supports DSN override or manual assembly."""
+
+    url: str | None = Field(default=None, alias="DATABASE_URL")
+    test_url: str | None = Field(default=None, alias="TEST_DATABASE_URL")
+    host: str = Field(default="localhost", alias="PGHOST")
+    port: int = Field(default=5432, alias="PGPORT")
+    name: str = Field(default="ai_trader", alias="PGDATABASE")
+    user: str = Field(default="postgres", alias="PGUSER")
+    password: str = Field(default="", alias="PGPASSWORD")
+    sslmode: str = Field(default="prefer", alias="PGSSLMODE")
+
+    @field_validator("port", mode="before")
+    @classmethod
+    def _coerce_port(cls, value: int | str | None) -> int:
+        if value in (None, ""):
+            return 5432
+        try:
+            return int(value)
+        except (TypeError, ValueError):
+            return 5432
+
+    @computed_field
+    @property
+    def primary_dsn(self) -> str | None:
+        return self.url or self.test_url
+
+    @cached_property
+    def assembled_dsn(self) -> str:
+        user = quote_plus(self.user or "")
+        password = quote_plus(self.password or "")
+        return (
+            f"postgresql+psycopg2://{user}:{password}@"
+            f"{self.host}:{self.port}/{self.name}?sslmode={self.sslmode}"
+        )
+
+    def effective_dsn(self) -> str | None:
+        return self.primary_dsn or self.assembled_dsn
+
+
+class MarketDataSettings(_SettingsBase):
+    """API credentials and feature flags for external market data vendors."""
+
+    alphavantage_key: str | None = Field(
+        default=None, alias="ALPHAVANTAGE_API_KEY"
+    )
+    finnhub_key: str | None = Field(default=None, alias="FINNHUB_API_KEY")
+
+    @computed_field
+    @property
+    def has_alphavantage(self) -> bool:
+        return bool(self.alphavantage_key)
+
+    @computed_field
+    @property
+    def has_finnhub(self) -> bool:
+        return bool(self.finnhub_key)
+
+
+class Settings(BaseModel):
+    """Aggregate accessor for domain-specific settings."""
+
+    otel: OTELSettings = Field(default_factory=OTELSettings)
+    sentry: SentrySettings = Field(default_factory=SentrySettings)
+    telegram: TelegramSettings = Field(default_factory=TelegramSettings)
+    database: DatabaseSettings = Field(default_factory=DatabaseSettings)
+    market_data: MarketDataSettings = Field(default_factory=MarketDataSettings)
+
+    model_config = {
+        "frozen": True,
+        "arbitrary_types_allowed": True,
+    }
+
+
+def get_settings() -> Settings:
+    """Instantiate settings from the current environment."""
+    return Settings()
+
+
+def reload_settings() -> Settings:
+    """Alias for get_settings to maintain a consistent API."""
+    return get_settings()
+
+
+def get_otel_settings() -> OTELSettings:
+    return get_settings().otel
+
+
+def get_sentry_settings() -> SentrySettings:
+    return get_settings().sentry
+
+
+def get_telegram_settings() -> TelegramSettings:
+    return get_settings().telegram
+
+
+def get_database_settings() -> DatabaseSettings:
+    return get_settings().database
+
+
+def get_market_data_settings() -> MarketDataSettings:
+    return get_settings().market_data
+
+# Hard guard: never allow Telegram fake mode in production
+if (os.getenv("ENV") or "dev").lower() == "prod":
+    if (os.getenv("TELEGRAM_FAKE") or "").lower() in {"1", "true"}:
+        # Force-disable any accidental fake flag in prod
+        os.environ["TELEGRAM_FAKE"] = "0"
+
+__all__ = [
+    "Settings",
+    "get_settings",
+    "reload_settings",
+    "get_otel_settings",
+    "get_sentry_settings",
+    "get_telegram_settings",
+    "get_database_settings",
+    "OTELSettings",
+    "SentrySettings",
+    "TelegramSettings",
+    "DatabaseSettings",
+    "MarketDataSettings",
+    "get_market_data_settings",
+]
diff --git a/app/source/__init__.py b/app/source/__init__.py
new file mode 100644
index 0000000..00e41c8
--- /dev/null
+++ b/app/source/__init__.py
@@ -0,0 +1,5 @@
+from __future__ import annotations
+
+from app.sources import dedupe_merge  # re-export helper used by legacy imports
+
+__all__ = ["dedupe_merge"]
diff --git a/app/source/textlist_source.py b/app/source/textlist_source.py
new file mode 100644
index 0000000..48f0129
--- /dev/null
+++ b/app/source/textlist_source.py
@@ -0,0 +1,3 @@
+from __future__ import annotations
+
+from app.sources.textlist_source import *  # noqa: F401,F403
diff --git a/app/sources/finviz_source.py b/app/sources/finviz_source.py
deleted file mode 100644
index 4bafd8c..0000000
--- a/app/sources/finviz_source.py
+++ /dev/null
@@ -1,123 +0,0 @@
-from __future__ import annotations
-
-import os
-from typing import List, Optional
-
-from loguru import logger
-
-try:
-    from finvizfinance.screener import Screener
-
-    _FINVIZ_OK = True
-except Exception:
-    _FINVIZ_OK = False
-
-
-def is_ready() -> bool:
-    """Return True if Finviz source is operational and importable."""
-    return _FINVIZ_OK
-
-
-def fetch_symbols(
-    preset: str = "Top Gainers",
-    filters: Optional[List[str]] = None,
-    max_symbols: int = 50,
-) -> List[str]:
-    """
-    Fetch ticker symbols from Finviz using its public Screener API.
-
-    Parameters
-    ----------
-    preset : str, optional
-        Finviz preset signal name (e.g., "Top Gainers", "Unusual Volume").
-    filters : list[str], optional
-        Finviz filter codes (e.g., ["sh_avgvol_o3000", "ta_perf_4w20o"]).
-    max_symbols : int, optional
-        Maximum number of tickers to return, by default 50.
-
-    Returns
-    -------
-    list[str]
-        A list of uppercase ticker symbols (AAPL, TSLA, etc.).
-    """
-    if not _FINVIZ_OK:
-        logger.warning("finvizfinance not installed; returning []")
-        return []
-
-    try:
-        s = Screener(filters=filters or [], tickers=[], order="price")
-        if preset:
-            s.set_filter(signal=preset)
-        df = s.get_screen_df()
-
-        # Support both DataFrame and list-of-dicts returns
-        if hasattr(df, "get"):
-            tickers = df.get("Ticker", [])
-        elif isinstance(df, list):
-            tickers = [
-                row.get("Ticker") for row in df if isinstance(row, dict)
-            ]
-        else:
-            logger.warning("Unexpected Finviz schema type: {}", type(df))
-            tickers = []
-
-        # Normalize and filter symbols
-        symbols = [str(t).upper().strip() for t in tickers if t]
-        symbols = [s for s in symbols if s.isalpha() and 1 <= len(s) <= 5]
-
-        logger.info(
-            "Finviz returned {} tickers for preset='{}'", len(symbols), preset
-        )
-        return symbols[:max_symbols]
-    except Exception as e:
-        logger.exception("Finviz fetch failed: {}", e)
-        return []
-
-
-# --- Compatibility wrapper for unified watchlist interface ---
-def get_symbols(*, max_symbols: int | None = None) -> List[str]:
-    """
-    Unified API: returns top tickers from Finviz screener.
-
-    max_symbols limits the final deduplicated list if provided.
-    """
-    preset = os.getenv("FINVIZ_PRESET", "most-active")
-    filters_raw = os.getenv(
-        "FINVIZ_FILTERS", "cap_large,sh_avgvol_o1000"
-    )
-    filter_list = (
-        [f.strip() for f in filters_raw.split(",") if f.strip()] or None
-    )
-
-    limit = (
-        max_symbols
-        if isinstance(max_symbols, int) and max_symbols > 0
-        else None
-    )
-    fetch_limit = limit if limit is not None else 100
-
-    try:
-        symbols = fetch_symbols(
-            preset=preset, filters=filter_list, max_symbols=fetch_limit
-        )
-    except Exception as exc:
-        logger.warning("[FinvizSource] Failed to fetch symbols: {}", exc)
-        return []
-
-    seen: set[str] = set()
-    result: List[str] = []
-    for sym in symbols or []:
-        ticker = (sym or "").strip().upper()
-        if not ticker or ticker in seen:
-            continue
-        seen.add(ticker)
-        result.append(ticker)
-        if limit is not None and len(result) >= limit:
-            break
-
-    if limit is not None and len(result) > limit:
-        return result[:limit]
-    return result
-
-
-__all__ = ["fetch_symbols", "get_symbols"]
diff --git a/app/utils/env.py b/app/utils/env.py
index fba269c..4f0c6e0 100644
--- a/app/utils/env.py
+++ b/app/utils/env.py
@@ -2,7 +2,7 @@ from __future__ import annotations
 
 import os
 from dataclasses import dataclass, field
-from typing import List, Set
+from typing import Iterable, List, Set
 
 # ------------------------------------------------------------------------------
 # Helpers
@@ -41,6 +41,38 @@ def get_float(name: str, default: float) -> float:
         return default
 
 
+def get_int_chain(names: Iterable[str], default: int) -> int:
+    """Return the first valid int from a list of env vars."""
+    for name in names:
+        raw = os.getenv(name)
+        if raw is None:
+            continue
+        candidate = str(raw).strip()
+        if not candidate:
+            continue
+        try:
+            return int(candidate)
+        except Exception:
+            continue
+    return default
+
+
+def get_float_chain(names: Iterable[str], default: float) -> float:
+    """Return the first valid float from a list of env vars."""
+    for name in names:
+        raw = os.getenv(name)
+        if raw is None:
+            continue
+        candidate = str(raw).strip()
+        if not candidate:
+            continue
+        try:
+            return float(candidate)
+        except Exception:
+            continue
+    return default
+
+
 def get_csv(name: str, default: str = "") -> List[str]:
     """Parse comma-delimited strings into a list of trimmed tokens."""
     raw = os.getenv(name)
@@ -176,15 +208,21 @@ class EnvSettings:
 
     #: Default HTTP request timeout (seconds).
     HTTP_TIMEOUT_SECS: int = field(
-        default_factory=lambda: get_int("HTTP_TIMEOUT_SECS", 10)
+        default_factory=lambda: get_int_chain(
+            ("HTTP_TIMEOUT", "HTTP_TIMEOUT_SECS"), 10
+        )
     )
     #: Default retry attempts for outbound HTTP.
     HTTP_RETRY_ATTEMPTS: int = field(
-        default_factory=lambda: get_int("HTTP_RETRY_ATTEMPTS", 2)
+        default_factory=lambda: get_int_chain(
+            ("HTTP_RETRIES", "HTTP_RETRY_ATTEMPTS"), 2
+        )
     )
     #: Default retry backoff for outbound HTTP.
     HTTP_RETRY_BACKOFF_SEC: float = field(
-        default_factory=lambda: get_float("HTTP_RETRY_BACKOFF_SEC", 1.5)
+        default_factory=lambda: get_float_chain(
+            ("HTTP_BACKOFF", "HTTP_RETRY_BACKOFF_SEC"), 1.5
+        )
     )
     #: HTTP user-agent header for outbound requests.
     HTTP_USER_AGENT: str = field(
diff --git a/app/utils/http.py b/app/utils/http.py
index 2b799df..a6190c8 100644
--- a/app/utils/http.py
+++ b/app/utils/http.py
@@ -49,7 +49,9 @@ def with_alpaca(headers: Optional[Dict[str, str]] = None) -> Dict[str, str]:
 # ------------------------------------------------------------------------------
 
 
-def _compute_sleep(attempt: int, backoff: float, retry_after: Optional[str]) -> float:
+def compute_backoff_delay(
+    attempt: int, backoff: float, retry_after: Optional[str]
+) -> float:
     # Respect Retry-After if present and valid
     if retry_after:
         try:
@@ -108,13 +110,9 @@ def request_json(
     - On non-JSON responses, returns empty dict.
     - On repeated network failure, returns (599, {}).
     """
-    timeout = timeout if timeout is not None else ENV.HTTP_TIMEOUT_SECS
-    retries = (
-        retries
-        if retries is not None
-        else getattr(ENV, "HTTP_RETRIES", ENV.HTTP_RETRY_ATTEMPTS)
-    )
-    backoff = backoff if backoff is not None else ENV.HTTP_RETRY_BACKOFF_SEC
+    timeout = timeout if timeout is not None else ENV.HTTP_TIMEOUT
+    retries = retries if retries is not None else ENV.HTTP_RETRIES
+    backoff = backoff if backoff is not None else ENV.HTTP_BACKOFF
 
     merged = _ensure_ua(headers)
 
@@ -155,7 +153,7 @@ def request_json(
             # Retryable?
             retryable = resp.status_code in {408, 429, 500, 502, 503, 504}
             if retryable and attempt < retries:
-                sleep_s = _compute_sleep(
+                sleep_s = compute_backoff_delay(
                     attempt, backoff, resp.headers.get("Retry-After")
                 )
                 _log_http_event(
@@ -212,7 +210,7 @@ def request_json(
                 note=f"error={e}",
             )
             if attempt < retries:
-                sleep_s = _compute_sleep(attempt, backoff, None)
+                sleep_s = compute_backoff_delay(attempt, backoff, None)
                 logger.warning(
                     "HTTP {} {} error — retry {}/{} in {:.2f}s: {}",
                     method.upper(),
diff --git a/app/wiring/__init__.py b/app/wiring/__init__.py
index 536fc6d..17c50c8 100644
--- a/app/wiring/__init__.py
+++ b/app/wiring/__init__.py
@@ -14,10 +14,4 @@ from app.api.routes.telegram import router as telegram_router  # noqa: E402
 router.include_router(telegram_router)
 
 
-def TelegramDep():
-    return build_client_from_env()
-
-
-def get_telegram():
-    # Keep a stable import path for existing code
-    return build_client_from_env()
+from app.api.routes.telegram import TelegramDep, get_telegram  # re-export
diff --git a/appsettings.txt b/appsettings.txt
deleted file mode 100644
index 57e364e..0000000
--- a/appsettings.txt
+++ /dev/null
@@ -1,262 +0,0 @@
-[
-  {
-    "name": "ACR_LOGIN_SERVER",
-    "slotSetting": false,
-    "value": "aitraderapp.azurecr.io"
-  },
-  {
-    "name": "ACR_PASSWORD",
-    "slotSetting": false,
-    "value": "/zgeH0ZVZsKTaBni8LvWlMHzXGjgqSt19qOUxYlfzA+ACRBV0ZdB"
-  },
-  {
-    "name": "ACR_USERNAME",
-    "slotSetting": false,
-    "value": "aitraderapp"
-  },
-  {
-    "name": "ADMIN_PASSPHRASE",
-    "slotSetting": false,
-    "value": "GunnersBStepping#04"
-  },
-  {
-    "name": "ALPACA_API_KEY",
-    "slotSetting": false,
-    "value": "PK64Y4IYEKAOU67MCSAGYESYFO"
-  },
-  {
-    "name": "ALPACA_API_SECRET",
-    "slotSetting": false,
-    "value": "A3G3nwAWAjTHWox5mRYN3RYv2dgL1KjdZpFMpsH8cJdS"
-  },
-  {
-    "name": "ALPACA_BASE_URL",
-    "slotSetting": false,
-    "value": "https://paper-api.alpaca.markets"
-  },
-  {
-    "name": "ALPACA_DATA_FEED",
-    "slotSetting": false,
-    "value": "iex"
-  },
-  {
-    "name": "APP_ENVIRONMENT",
-    "slotSetting": false,
-    "value": "prod"
-  },
-  {
-    "name": "AZURE_STORAGE_ACCOUNT_KEY",
-    "slotSetting": false,
-    "value": "cvzyxGIIc+L9n12FVDn/olxepmFnWQhhQcskLeffeQAz+uXemARtEmYjKdBcy0oGWwJYPHRXJSfX+AStXzND0Q=="
-  },
-  {
-    "name": "AZURE_STORAGE_ACCOUNT_NAME",
-    "slotSetting": false,
-    "value": "aitraderblobstore"
-  },
-  {
-    "name": "AZURE_STORAGE_CONNECTION_STRING",
-    "slotSetting": false,
-    "value": "DefaultEndpointsProtocol=https;AccountName=aitraderblobstore;AccountKey=cvzyxGIIc+L9n12FVDn/olxepmFnWQhhQcskLeffeQAz+uXemARtEmYjKdBcy0oGWwJYPHRXJSfX+AStXzND0Q==;EndpointSuffix=core.windows.net"
-  },
-  {
-    "name": "AZURE_STORAGE_CONTAINER_NAME",
-    "slotSetting": false,
-    "value": "traderdata"
-  },
-  {
-    "name": "CONCENTRATION_MANUAL_GATE",
-    "slotSetting": false,
-    "value": "0.5"
-  },
-  {
-    "name": "DAILY_DRAWDOWN_HALT",
-    "slotSetting": false,
-    "value": "0.05"
-  },
-  {
-    "name": "DATABASE_URL",
-    "slotSetting": false,
-    "value": "postgresql+psycopg2://db_admin:aitrader%2304@ai-trader-db.postgres.database.azure.com:5432/traderdata?sslmode=require"
-  },
-  {
-    "name": "DOCKER_REGISTRY_SERVER_PASSWORD",
-    "slotSetting": false,
-    "value": null
-  },
-  {
-    "name": "DOCKER_REGISTRY_SERVER_URL",
-    "slotSetting": false,
-    "value": "aitraderapp.azurecr.io"
-  },
-  {
-    "name": "DOCKER_REGISTRY_SERVER_USERNAME",
-    "slotSetting": false,
-    "value": "aitraderapp"
-  },
-  {
-    "name": "ENV",
-    "slotSetting": false,
-    "value": "prod"
-  },
-  {
-    "name": "GRAFANA_BASIC_AUTH",
-    "slotSetting": false,
-    "value": "ODMwNzIwOmdsY19leUp2SWpvaU1UQXlPRGt6TkNJc0ltNGlPaUpoYVMxMGNtRmtaWEl0WVhCd0lpd2lheUk2SW14VWVIZEphV2RyZWtoQk16RXdOV0kwTWpNNVpUbFNOaUlzSW0waU9uc2ljaUk2SW5CeWIyUXRkWE10ZDJWemRDMHdJbjE5"
-  },
-  {
-    "name": "MAX_RISK_PER_TRADE",
-    "slotSetting": false,
-    "value": "0.01"
-  },
-  {
-    "name": "MAX_WATCHLIST",
-    "slotSetting": false,
-    "value": "25"
-  },
-  {
-    "name": "OTEL_EXPORTER_OTLP_ENDPOINT",
-    "slotSetting": false,
-    "value": "http://otel-collector:4317"
-  },
-  {
-    "name": "OTEL_EXPORTER_OTLP_HEADERS",
-    "slotSetting": false,
-    "value": "Authorization=Basic%20ODMwNzIwOmdsY19leUp2SWpvaU1UQXlPRGt6TkNJc0ltNGlPaUpoYVMxMGNtRmtaWEl0WVhCd0lpd2lheUk2SW14VWVIZEphV2RyZWtoQk16RXdOV0kwTWpNNVpUbFNOaUlzSW0waU9uc2ljaUk2SW5CeWIyUXRkWE10ZDJWemRDMHdJbjE5"
-  },
-  {
-    "name": "OTEL_PYTHON_LOG_LEVEL",
-    "slotSetting": false,
-    "value": "info"
-  },
-  {
-    "name": "OTEL_RESOURCE_ATTRIBUTES",
-    "slotSetting": false,
-    "value": "service.name=ai-trader-app,service.namespace=trading-platform,deployment.environment=prod"
-  },
-  {
-    "name": "OTEL_SERVICE_NAME",
-    "slotSetting": false,
-    "value": "ai-trader-app"
-  },
-  {
-    "name": "PAPER_TRADING",
-    "slotSetting": false,
-    "value": "true"
-  },
-  {
-    "name": "PGDATABASE",
-    "slotSetting": false,
-    "value": "traderdata"
-  },
-  {
-    "name": "PGHOST",
-    "slotSetting": false,
-    "value": "ai-trader-db.postgres.database.azure.com"
-  },
-  {
-    "name": "PGPASSWORD",
-    "slotSetting": false,
-    "value": "aitrader#04"
-  },
-  {
-    "name": "PGPORT",
-    "slotSetting": false,
-    "value": "5432"
-  },
-  {
-    "name": "PGSSLMODE",
-    "slotSetting": false,
-    "value": "require"
-  },
-  {
-    "name": "PGUSER",
-    "slotSetting": false,
-    "value": "db_admin"
-  },
-  {
-    "name": "PORT",
-    "slotSetting": false,
-    "value": "8000"
-  },
-  {
-    "name": "PYTHONUNBUFFERED",
-    "slotSetting": false,
-    "value": "1"
-  },
-  {
-    "name": "SCM_DO_BUILD_DURING_DEPLOYMENT",
-    "slotSetting": false,
-    "value": "false"
-  },
-  {
-    "name": "SENTRY_DSN",
-    "slotSetting": false,
-    "value": "https://a4928e0f2eb4b321bdded24e333f68fe@o4510287200190464.ingest.us.sentry.io/4510288172613632"
-  },
-  {
-    "name": "TELEGRAM_ALLOWED_USER_IDS",
-    "slotSetting": false,
-    "value": "8414729426"
-  },
-  {
-    "name": "TELEGRAM_BOT_TOKEN",
-    "slotSetting": false,
-    "value": "8313975890:AAFhCadnhIlb7qTLHgSZYo516nM5pmRXrSA"
-  },
-  {
-    "name": "TELEGRAM_CHAT_ID",
-    "slotSetting": false,
-    "value": "8414729426"
-  },
-  {
-    "name": "TELEGRAM_ENABLE",
-    "slotSetting": false,
-    "value": "true"
-  },
-  {
-    "name": "TELEGRAM_WEBHOOK_SECRET",
-    "slotSetting": false,
-    "value": "lSLfPpmSLSiGm1qsVO2jRRsUYdz_S4PDUxyky0feAA6qVx6bxGFsGNpRY4fMTAGa"
-  },
-  {
-    "name": "TRADING_ENABLED",
-    "slotSetting": false,
-    "value": "true"
-  },
-  {
-    "name": "TZ",
-    "slotSetting": false,
-    "value": "America/Los_Angeles"
-  },
-  {
-    "name": "WATCHLIST_SOURCE",
-    "slotSetting": false,
-    "value": "textlist"
-  },
-  {
-    "name": "WATCHLIST_TEXT",
-    "slotSetting": false,
-    "value": "AAPL, MSFT, NVDA, TSLA, AMZN, META, GOOGL, NFLX, RGTI, SOUN, OKLO, SPY, QQQ, IWM"
-  },
-  {
-    "name": "WEBSITE_HEALTHCHECK_MAXPINGFAILURES",
-    "slotSetting": false,
-    "value": "10"
-  },
-  {
-    "name": "WEBSITES_PORT",
-    "slotSetting": false,
-    "value": "8000"
-  },
-  {
-    "name": "TELEGRAM_WEBHOOK_URL",
-    "slotSetting": false,
-    "value": "https://ai-trader-app.azurewebsites.net/telegram/webhook"
-  },
-  {
-    "name": "OTEL_LOGS_EXPORTER",
-    "slotSetting": false,
-    "value": "otlp"
-  }
-]
diff --git a/connect.py b/connect.py
deleted file mode 100644
index 08849df..0000000
--- a/connect.py
+++ /dev/null
@@ -1,29 +0,0 @@
-import os
-
-import alpaca_trade_api as tradeapi
-from dotenv import load_dotenv
-
-# Load environment variables from the .env file
-load_dotenv()
-
-# Get the API keys from the environment variables
-API_KEY = os.getenv("API_KEY")
-SECRET_KEY = os.getenv("SECRET_KEY")
-BASE_URL = os.getenv("BASE_URL")
-
-# --- Sanity Check (Optional but Recommended) ---
-if not all([API_KEY, SECRET_KEY, BASE_URL]):
-    raise ValueError("API keys or Base URL not set in .env file.")
-
-# Connect to the Alpaca API
-api = tradeapi.REST(API_KEY, SECRET_KEY, BASE_URL, api_version="v2")
-
-try:
-    # Get and print your account information
-    account = api.get_account()
-    print("Connection Successful!")
-    print(f"Account Status: {account.status}")
-    print(f"Buying Power: ${account.buying_power}")
-
-except Exception as e:
-    print(f"Connection failed: {e}")
diff --git a/debug_webhook_call.py b/debug_webhook_call.py
deleted file mode 100644
index e3d25d7..0000000
--- a/debug_webhook_call.py
+++ /dev/null
@@ -1,29 +0,0 @@
-# debug_webhook_call.py
-import os
-
-from dotenv import load_dotenv
-from starlette.testclient import TestClient
-
-# Load .env and force bypass ON to match test’s expectation
-load_dotenv(override=True)
-os.environ.setdefault("TELEGRAM_ALLOW_TEST_NO_SECRET", "1")
-
-import app.main as main_module  # noqa: E402
-
-client = TestClient(main_module.app)
-
-payload = {"message": {"chat": {"id": 1}, "text": "/ping"}}
-
-resp = client.post(
-    "/telegram/webhook",
-    json=payload,
-    headers={
-        # Try empty header to use bypass
-        # "X-Telegram-Bot-Api-Secret-Token": "",
-        # Or uncomment to test secret path:
-        "X-Telegram-Bot-Api-Secret-Token": os.getenv("TELEGRAM_WEBHOOK_SECRET", ""),
-    },
-)
-
-print("status:", resp.status_code)
-print("body  :", resp.text)
diff --git a/dev.sh b/dev.sh
deleted file mode 100755
index 5b34323..0000000
--- a/dev.sh
+++ /dev/null
@@ -1,16 +0,0 @@
-#!/bin/zsh
-
-# dev.sh
-# Loads development environment variables and runs the app with OTel instrumentation.
-
-echo "🚀 Starting AI Trader in DEV mode..."
-
-# Load the .env.dev file
-# This command finds all lines in .env.dev, removes comments (#),
-# and exports them as environment variables for this script's session.
-export $(grep -v '^#' .env.dev | grep -E '.+=.+' | xargs)
-
-# Run the app
-# opentelemetry-instrument wraps the uvicorn command,
-# reads the OTEL_ environment variables, and configures telemetry.
-opentelemetry-instrument uvicorn app.main:app --host 0.0.0.0 --reload
\ No newline at end of file
diff --git a/docs/architecture/PHASE3_BACKTESTING.md b/docs/architecture/PHASE3_BACKTESTING.md
index 223970b..a9d01bf 100644
--- a/docs/architecture/PHASE3_BACKTESTING.md
+++ b/docs/architecture/PHASE3_BACKTESTING.md
@@ -37,6 +37,8 @@ flowchart LR
   CFG --> STRAT
   PQ --> DL
   PG <--> MTRX
+  MKT -->|streams| DL
+  DL -->|Kalman (x, v, P)| STRAT
   CLI --> DL
   CLI --> STRAT
   API --> DL
@@ -85,11 +87,12 @@ classDiagram
   }
 
   class SignalFrame {
-    +index: datetime
-    +momentum: float
-    +rank: float
-    +long_entry: bool
-    +long_exit: bool
+    +timestamp: datetime
+    +filtered_price: float
+    +velocity: float
+    +uncertainty: float
+    +price: float
+    +volume: float
   }
 
   class Order {
@@ -158,6 +161,39 @@ classDiagram
   Position --> EquityCurve : mark_to_market
   EquityCurve --> Metrics : compute
 
+%% ------------------------------------------------------------
+%% 4) Probabilistic Data Abstraction Layer (new)
+%% ------------------------------------------------------------
+
+```mermaid
+flowchart LR
+  subgraph DAL[MarketDataDAL]
+    VC[VendorClient
+    (Alpaca/AlphaVantage/Finnhub)] --> NM[Normalizer
+    (Bars, SignalFrame)]
+    NM --> KF[KalmanFilter1D
+    (x, v, P)]
+    KF --> PC[Parquet Cache]
+    KF --> PG[Postgres Metadata]
+  end
+
+  AlpacaWS -.-> VC
+  AlpacaHTTP --> VC
+  FinnhubWS -.-> VC
+  FinnhubHTTP --> VC
+  AlphaVantageHTTP --> VC
+
+  VC -. gap repair .-> NM
+```
+
+### Implementation Notes
+
+- `app/dal/` houses the new probabilistic DAL. `MarketDataDAL.fetch_bars()` returns a `ProbabilisticBatch` (bars + signal frames + regime snapshots) and `.stream_bars()` yields `ProbabilisticStreamFrame` objects with synchronized signal/regime views.
+- Vendors are pluggable via `VendorClient` implementations (`alpaca`, `alphavantage`, `finnhub`) supporting HTTP and/or WebSocket transports. Streaming gaps trigger automatic HTTP backfill.
+- Cached parquet data lives under `artifacts/marketdata/cache/` by default; the DAL writes bars, probabilistic signals, and regimes alongside optional metadata rows (`market_data_snapshots`) in Postgres.
+- `SignalFrame` now carries filtered price, velocity, and covariance-derived uncertainty to feed probabilistic strategies.
+- Tests covering Kalman and DAL flows live in `tests/dal/`.
+
 %% ------------------------------------------------------------
 %% 4) Parameter sweep & parallelization
 %% ------------------------------------------------------------
@@ -275,4 +311,4 @@ mindmap
     Scale
       (Chunking by symbol/date)
       (Parallel by symbol/grid)
-      (Artifacts in blob/s3)
\ No newline at end of file
+      (Artifacts in blob/s3)
diff --git a/dump_json b/dump_json
deleted file mode 100644
index 27bfb82..0000000
--- a/dump_json
+++ /dev/null
@@ -1,137 +0,0 @@
-[
-  {
-    "name": "PORT",
-    "value": "8000",
-    "slotSetting": false
-  },
-  {
-    "name": "WEBSITES_PORT",
-    "value": "8000",
-    "slotSetting": false
-  },
-  {
-    "name": "PYTHONUNBUFFERED",
-    "value": "1",
-    "slotSetting": false
-  },
-  {
-    "name": "PGHOST",
-    "value": "ai-trader-db.postgres.database.azure.com",
-    "slotSetting": false
-  },
-  {
-    "name": "PGPORT",
-    "value": "5432",
-    "slotSetting": false
-  },
-  {
-    "name": "PGDATABASE",
-    "value": "traderdata",
-    "slotSetting": false
-  },
-  {
-    "name": "PGUSER",
-    "value": "db_admin",
-    "slotSetting": false
-  },
-  {
-    "name": "PGPASSWORD",
-    "value": "aitrader#04",
-    "slotSetting": false
-  },
-  {
-    "name": "PGSSLMODE",
-    "value": "require",
-    "slotSetting": false
-  },
-  {
-    "name": "AZURE_STORAGE_ACCOUNT_NAME",
-    "value": "aitraderblobstore",
-    "slotSetting": false
-  },
-  {
-    "name": "AZURE_STORAGE_ACCOUNT_KEY",
-    "value": "cvzyxGIIc+L9n12FVDn/olxepmFnWQhhQcskLeffeQAz+uXemARtEmYjKdBcy0oGWwJYPHRXJSfX+AStXzND0Q==",
-    "slotSetting": false
-  },
-  {
-    "name": "AZURE_STORAGE_CONTAINER_NAME",
-    "value": "traderdata",
-    "slotSetting": false
-  },
-  {
-    "name": "ALPACA_API_KEY",
-    "value": "PK64Y4IYEKAOU67MCSAGYESYFO",
-    "slotSetting": false
-  },
-  {
-    "name": "ALPACA_API_SECRET",
-    "value": "PK64Y4IYEKAOU67MCSAGYESYFO",
-    "slotSetting": false
-  },
-  {
-    "name": "ALPACA_BASE_URL",
-    "value": "https://paper-api.alpaca.markets",
-    "slotSetting": false
-  },
-  {
-    "name": "PAPER_TRADING",
-    "value": "true",
-    "slotSetting": false
-  },
-  {
-    "name": "TRADING_ENABLED",
-    "value": "true",
-    "slotSetting": false
-  },
-  {
-    "name": "MAX_RISK_PER_TRADE",
-    "value": "0.01",
-    "slotSetting": false
-  },
-  {
-    "name": "DAILY_DRAWDOWN_HALT",
-    "value": "0.05",
-    "slotSetting": false
-  },
-  {
-    "name": "CONCENTRATION_MANUAL_GATE",
-    "value": "0.5",
-    "slotSetting": false
-  },
-  {
-    "name": "TELEGRAM_ENABLE",
-    "value": "true",
-    "slotSetting": false
-  },
-  {
-    "name": "TELEGRAM_BOT_TOKEN",
-    "value": "8313975890:AAFhCadnhIlb7qTLHgSZYo516nM5pmRXrSA",
-    "slotSetting": false
-  },
-  {
-    "name": "TELEGRAM_CHAT_ID",
-    "value": "8414729426",
-    "slotSetting": false
-  },
-  {
-    "name": "TELEGRAM_ALLOWED_USER_IDS",
-    "value": "8414729426",
-    "slotSetting": false
-  },
-  {
-    "name": "TELEGRAM_WEBHOOK_SECRET",
-    "value": "lSLfPpmSLSiGm1qsVO2jRRsUYdz_S4PDUxyky0feAA6qVx6bxGFsGNpRY4fMTAGa",
-    "slotSetting": false
-  },
-  {
-    "name": "ADMIN_PASSPHRASE",
-    "value": "GunnersBStepping#04",
-    "slotSetting": false
-  },
-  {
-    "name": "TZ",
-    "value": "America/Los_Angeles",
-    "slotSetting": false
-  }
-]
\ No newline at end of file
diff --git a/ecosystem.config.cjs b/ecosystem.config.cjs
deleted file mode 100644
index d53227e..0000000
--- a/ecosystem.config.cjs
+++ /dev/null
@@ -1,57 +0,0 @@
-const path = require("path");
-const fs = require("fs");
-
-const LOG_DIR =
-  process.env.LOG_DIR ||
-  (process.env.HOME ? path.join(process.env.HOME, "ai_trader_logs") : path.join(process.cwd(), "logs"));
-
-fs.mkdirSync(LOG_DIR, { recursive: true });
-
-module.exports = {
-  apps: [
-    {
-      name: "ai_trader",
-      // Use the uvicorn binary directly (no -m)
-      script: path.join(process.cwd(), ".venv/bin/uvicorn"),
-      args: "app.main:app --host 0.0.0.0 --port 8000 --workers 1",
-      interpreter: "none",              // <— IMPORTANT
-      cwd: process.env.PWD || ".",
-      env: {
-        PYTHONUNBUFFERED: "1",
-        PYTHONPATH: ".",
-        ENV: process.env.ENV || "production",
-        TZ: "America/Los_Angeles",
-        PORT: process.env.PORT || "8000",
-        LOG_DIR: LOG_DIR,
-      },
-      out_file: path.join(LOG_DIR, "uvicorn.out.log"),
-      error_file: path.join(LOG_DIR, "uvicorn.err.log"),
-      merge_logs: true,
-      autorestart: true,
-      max_restarts: 10,
-    },
-    {
-      name: "ngrok",
-      script: "ngrok",
-      args: "http 8000 --region us --host-header=rewrite --log=stdout",
-      interpreter: "none",              // <— IMPORTANT
-      out_file: path.join(LOG_DIR, "ngrok.out.log"),
-      error_file: path.join(LOG_DIR, "ngrok.err.log"),
-      merge_logs: true,
-      autorestart: true,
-    },
-    {
-      name: "pm2-logrotate",
-      script: "pm2-logrotate.config.js",
-      interpreter: "none",              // <— IMPORTANT
-      autorestart: true,
-      env: {
-        PM2_LOGROTATE_ENABLE: true,
-        PM2_LOGROTATE_CRON: "0 0 * * *",
-        PM2_LOGROTATE_RETENTION: "7",
-        PM2_LOGROTATE_DATE_FORMAT: "YYYY-MM-DD",
-        PM2_LOGROTATE_COMPRESS: true,
-      },
-    },
-  ],
-};
\ No newline at end of file
diff --git a/containers.appservice.json b/infra/azure/containers.appservice.json
similarity index 87%
rename from containers.appservice.json
rename to infra/azure/containers.appservice.json
index 732addd..cf5dfde 100644
--- a/containers.appservice.json
+++ b/infra/azure/containers.appservice.json
@@ -10,8 +10,8 @@
       "containers": [
         {
           "name": "app",
-          "image": "__APP_IMAGE__",              // ACR image (sha tag)
-          "command": [],                         // optional; prefer image CMD
+          "image": "__APP_IMAGE__",              
+          "command": [],                        
           "args": [],
           "ports": [ { "port": 8000 } ],
           "resources": { "cpu": 1, "memoryInGb": 1.5 },
@@ -30,8 +30,8 @@
         },
         {
           "name": "otel-collector",
-          "image": "__OTEL_IMAGE__",             // ACR image for collector
-          "command": [],                         // use default entrypoint
+          "image": "__OTEL_IMAGE__",             
+          "command": [],                         
           "args": ["--config=/etc/otelcol-config.yaml"],
           "ports": [
             { "port": 4317, "protocol": "tcp" },
diff --git a/infra/azure/render_sidecar.py b/infra/azure/render_sidecar.py
new file mode 100644
index 0000000..4b24f38
--- /dev/null
+++ b/infra/azure/render_sidecar.py
@@ -0,0 +1,168 @@
+#!/usr/bin/env python3
+import json, os, re, sys, base64
+from pathlib import Path
+
+BASE_DIR = Path(__file__).resolve().parent
+SRC = Path(os.environ.get("CONTAINERS_SRC", BASE_DIR / "containers.appservice.json"))
+DST = Path(os.environ.get("CONTAINERS_DST", BASE_DIR / "containers.appservice.rendered.json"))
+
+# --- simple comment stripper for //... and /* ... */ ---
+def strip_json_comments(text: str) -> str:
+    # remove /* ... */ (multi-line)
+    text = re.sub(r"/\*.*?\*/", "", text, flags=re.S)
+    # remove // ... (to end of line), but not within quotes
+    def _strip_line(line):
+        in_str = False
+        esc = False
+        out = []
+        for i, ch in enumerate(line):
+            if ch == '"' and not esc:
+                in_str = not in_str
+            if ch == "\\" and not esc:
+                esc = True
+                out.append(ch)
+                continue
+            if not in_str and ch == "/" and i + 1 < len(line) and line[i+1] == "/":
+                break
+            out.append(ch)
+            esc = False
+        return "".join(out)
+    return "\n".join(_strip_line(ln) for ln in text.splitlines())
+
+def find_containers(doc):
+    # new-ish shape: siteConfig.containers.containers (list)
+    try:
+        lst = doc["siteConfig"]["containers"]["containers"]
+        if isinstance(lst, list):
+            def setter(new): doc["siteConfig"]["containers"]["containers"] = new
+            return lst, setter
+    except Exception:
+        pass
+    # shape: properties.siteConfig.containers.containers (list)
+    try:
+        lst = doc["properties"]["siteConfig"]["containers"]["containers"]
+        if isinstance(lst, list):
+            def setter(new): doc["properties"]["siteConfig"]["containers"]["containers"] = new
+            return lst, setter
+    except Exception:
+        pass
+    # flat list variant: siteConfig.containers (list)
+    try:
+        lst = doc["siteConfig"]["containers"]
+        if isinstance(lst, list):
+            def setter(new): doc["siteConfig"]["containers"] = new
+            return lst, setter
+    except Exception:
+        pass
+    # flat list variant: properties.siteConfig.containers (list)
+    try:
+        lst = doc["properties"]["siteConfig"]["containers"]
+        if isinstance(lst, list):
+            def setter(new): doc["properties"]["siteConfig"]["containers"] = new
+            return lst, setter
+    except Exception:
+        pass
+    return None, None
+
+def set_env(env_list, name, value):
+    for item in env_list:
+        if item.get("name") == name:
+            item["value"] = value
+            return
+    env_list.append({"name": name, "value": value})
+
+# ---- read & strip comments ----
+try:
+    raw = open(SRC, "r", encoding="utf-8").read()
+except FileNotFoundError:
+    print(f"ERROR: {SRC} not found", file=sys.stderr)
+    sys.exit(2)
+
+clean = strip_json_comments(raw)
+
+try:
+    data = json.loads(clean)
+except json.JSONDecodeError as e:
+    print(f"ERROR: {SRC} is not valid JSON after stripping comments: {e}", file=sys.stderr)
+    sys.exit(2)
+
+containers, set_back = find_containers(data)
+if containers is None:
+    print("ERROR: couldn't locate containers array in input JSON", file=sys.stderr)
+    sys.exit(2)
+
+# Resolve images (allow override)
+app_image = os.environ.get("APP_IMAGE")
+otel_image = os.environ.get("OTEL_IMAGE")
+acr = os.environ.get("ACR_LOGIN_SERVER")
+sha = os.environ.get("GITHUB_SHA")
+if not app_image:
+    if not (acr and sha):
+        print("ERROR: set APP_IMAGE or ACR_LOGIN_SERVER and GITHUB_SHA", file=sys.stderr)
+        sys.exit(2)
+    app_image = f"{acr}/ai-trader:{sha}"
+if not otel_image and acr and sha:
+    otel_image = f"{acr}/ai-trader-otel-collector:{sha}"
+
+# Optional: embed otel config from env (already base64), or pass-through placeholder
+otel_b64 = os.environ.get("OTEL_CONFIG_B64")
+
+# Replace placeholders across containers + envs
+for c in containers:
+    name = c.get("name")
+    if name == "app":
+        # image
+        c["image"] = app_image
+        # envs
+        envs = c.get("environmentVariables", [])
+        replacements = {
+            "__APP_VERSION__":           os.environ.get("APP_VERSION",""),
+            "__DATABASE_URL__":          os.environ.get("DATABASE_URL",""),
+            "__SENTRY_DSN__":            os.environ.get("SENTRY_DSN",""),
+            "__GRAFANA_OTLP_ENDPOINT__": os.environ.get("GRAFANA_OTLP_ENDPOINT",""),
+            "__GRAFANA_BASIC_AUTH__":    os.environ.get("GRAFANA_BASIC_AUTH",""),
+            "__TELEGRAM_TOKEN__":        os.environ.get("TELEGRAM_TOKEN",""),
+            "__TELEGRAM_WEBHOOK_URL__":  os.environ.get("TELEGRAM_WEBHOOK_URL",""),
+            "__TELEGRAM_WEBHOOK_SECRET__": os.environ.get("TELEGRAM_WEBHOOK_SECRET",""),
+        }
+        # fill env vars by name match on "value" placeholder
+        for ev in envs:
+            val = ev.get("value")
+            if isinstance(val, str) and val in replacements:
+                ev["value"] = replacements[val]
+        c["environmentVariables"] = envs
+
+    elif name == "otel-collector" and otel_image:
+        c["image"] = otel_image
+
+# Replace volumes content if placeholder present
+# path: siteConfig.containers.volumes or properties.siteConfig.containers.volumes
+def find_volumes(doc):
+    for path in (
+        ("siteConfig","containers","volumes"),
+        ("properties","siteConfig","containers","volumes"),
+    ):
+        cur = doc
+        ok = True
+        for p in path:
+            cur = cur.get(p)
+            if cur is None:
+                ok = False
+                break
+        if ok and isinstance(cur, list):
+            return cur
+    return None
+
+vols = find_volumes(data)
+if vols:
+    for v in vols:
+        content = v.get("content")
+        if isinstance(content, dict) and content.get("base64") in ("__OTEL_CONFIG_B64__", None, ""):
+            if otel_b64:
+                content["base64"] = otel_b64
+
+# Write output
+set_back(containers)
+with open(DST, "w", encoding="utf-8") as f:
+    json.dump(data, f, ensure_ascii=False, indent=2)
+print(f"Wrote {DST}")
diff --git a/Dockerfile b/infra/docker/Dockerfile
similarity index 100%
rename from Dockerfile
rename to infra/docker/Dockerfile
diff --git a/infra/docker/Dockerfile.ui b/infra/docker/Dockerfile.ui
new file mode 100644
index 0000000..50b165c
--- /dev/null
+++ b/infra/docker/Dockerfile.ui
@@ -0,0 +1,24 @@
+FROM python:3.11-slim
+
+ENV PYTHONUNBUFFERED=1 \
+    PYTHONDONTWRITEBYTECODE=1 \
+    PORT=8000
+
+WORKDIR /app
+
+COPY requirements.txt /tmp/requirements.txt
+
+RUN apt-get update \
+    && apt-get install -y --no-install-recommends build-essential curl \
+    && pip install --no-cache-dir -r /tmp/requirements.txt \
+    && apt-get purge -y build-essential \
+    && apt-get autoremove -y \
+    && rm -rf /var/lib/apt/lists/* /tmp/requirements.txt
+
+COPY . /app
+
+ENV PYTHONPATH=/app
+
+EXPOSE 8000
+
+CMD ["sh", "-c", "python -m streamlit run ui/streamlit_app.py --server.address 0.0.0.0 --server.port ${PORT:-8000} --server.headless true --server.baseUrlPath /ui --browser.gatherUsageStats false"]
diff --git a/docker-compose.yml b/infra/docker/docker-compose.yml
similarity index 63%
rename from docker-compose.yml
rename to infra/docker/docker-compose.yml
index 41a227b..9cef491 100644
--- a/docker-compose.yml
+++ b/infra/docker/docker-compose.yml
@@ -4,8 +4,9 @@ version: "3.8"
 
 services:
   app:
-    # This assumes your Dockerfile is in the current directory
-    build: .
+    build:
+      context: ../..
+      dockerfile: infra/docker/Dockerfile
     ports:
       # Exposes your FastAPI app
       - "8000:8000"
@@ -17,10 +18,16 @@ services:
       
       # --- OpenTelemetry ---
       # Tells the app to send telemetry to the collector service
-      - OTEL_EXPORTER_OTLP_ENDPOINT=http:///localhost:4317
+      - OTEL_EXPORTER_OTLP_ENDPOINT=https://ai-trader-otel.wonderfulfield-e5e6ab0a.westus2.azurecontainerapps.io
+      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://ai-trader-otel.wonderfulfield-e5e6ab0a.westus2.azurecontainerapps.io/v1/traces
+      - OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=https://ai-trader-otel.wonderfulfield-e5e6ab0a.westus2.azurecontainerapps.io/v1/metrics
+      - OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=https://ai-trader-otel.wonderfulfield-e5e6ab0a.westus2.azurecontainerapps.io/v1/logs
       # This service name will show up in Grafana
       - OTEL_SERVICE_NAME=ai-trader-app
       - OTEL_LOGS_EXPORTER=otlp
+      - OTEL_TRACES_EXPORTER=otlp
+      - OTEL_METRICS_EXPORTER=otlp
+      - OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
       # --- Other Secrets ---
       # Add other env vars your app needs
       # - DATABASE_URL=${DATABASE_URL}
@@ -41,7 +48,7 @@ services:
     image: otel/opentelemetry-collector-contrib:latest
     volumes:
       # Mounts the config file into the container
-      - ./otel-collector-config.yml:/etc/otelcol-contrib/config.yml:ro
+      - ../otel/collector-config.yml:/etc/otelcol-contrib/config.yaml:ro
     ports:
       # Exposes OTLP ports to your local machine (for ./dev.sh)
       - "4317:4317" # OTLP gRPC
@@ -49,6 +56,10 @@ services:
     environment:
       # Injects the environment variable into the collector's config
       - APP_ENVIRONMENT=${APP_ENVIRONMENT}
+      - GRAFANA_OTLP_ENDPOINT=${GRAFANA_OTLP_ENDPOINT}
+      - GRAFANA_BASIC_AUTH=${GRAFANA_BASIC_AUTH}
+      - SERVICE_NAME=ai-trader-app
+      - SERVICE_INSTANCE_ID=otel-local
     healthcheck:
       test: ["CMD-SHELL", "stat /etc/otelcol-contrib/config.yml || exit 1"]
       interval: 30s
@@ -61,4 +72,4 @@ services:
 
 networks:
   ai-trader-net:
-    driver: bridge
\ No newline at end of file
+    driver: bridge
diff --git a/infra/otel/collector-config.yml b/infra/otel/collector-config.yml
new file mode 100644
index 0000000..169aabd
--- /dev/null
+++ b/infra/otel/collector-config.yml
@@ -0,0 +1,67 @@
+receivers:
+  otlp:
+    protocols:
+      grpc:
+        endpoint: "0.0.0.0:4317"
+      http:
+        endpoint: "0.0.0.0:4318"
+
+processors:
+  memory_limiter:
+    check_interval: "2s"
+    limit_percentage: 80
+    spike_limit_percentage: 25
+  resource:
+    attributes:
+      - key: "deployment.environment"
+        value: "${APP_ENVIRONMENT}"
+        action: "upsert"
+      - key: "service.name"
+        value: "${SERVICE_NAME}"
+        action: "upsert"
+      - key: "service.instance.id"
+        value: "${SERVICE_INSTANCE_ID}"
+        action: "upsert"
+  batch:
+    timeout: "5s"
+    send_batch_size: 2048
+
+exporters:
+  otlphttp:
+    endpoint: "${GRAFANA_OTLP_ENDPOINT}"
+    headers:
+      Authorization: "Basic ${GRAFANA_BASIC_AUTH}"
+    timeout: "10s"
+    compression: "gzip"
+    sending_queue:
+      enabled: true
+      num_consumers: 4
+      queue_size: 5000
+    retry_on_failure:
+      enabled: true
+      initial_interval: 2s
+      max_interval: 60s
+      max_elapsed_time: 10m
+
+extensions:
+  health_check:
+    endpoint: "0.0.0.0:13133"
+
+service:
+  telemetry:
+    logs:
+      level: "info"
+  extensions: ["health_check"]
+  pipelines:
+    traces:
+      receivers: ["otlp"]
+      processors: ["memory_limiter", "resource", "batch"]
+      exporters: ["otlphttp"]
+    metrics:
+      receivers: ["otlp"]
+      processors: ["memory_limiter", "resource", "batch"]
+      exporters: ["otlphttp"]
+    logs:
+      receivers: ["otlp"]
+      processors: ["memory_limiter", "resource", "batch"]
+      exporters: ["otlphttp"]
diff --git a/otel.Dockerfile b/infra/otel/otel.Dockerfile
similarity index 78%
rename from otel.Dockerfile
rename to infra/otel/otel.Dockerfile
index 49b9243..5a69f5e 100644
--- a/otel.Dockerfile
+++ b/infra/otel/otel.Dockerfile
@@ -3,4 +3,4 @@ FROM otel/opentelemetry-collector-contrib:latest
 
 # Copy your existing config file (with ${VAR_NAME} placeholders)
 # into the image. Azure will inject the env vars at runtime.
-COPY otel-collector-config.yml /etc/otelcol-contrib/config.yaml
\ No newline at end of file
+COPY collector-config.yml /etc/otelcol-contrib/config.yaml
diff --git a/otel-collector-config.yml b/otel-collector-config.yml
deleted file mode 100644
index 74cec84..0000000
--- a/otel-collector-config.yml
+++ /dev/null
@@ -1,70 +0,0 @@
-receivers:
-  otlp:
-    protocols:
-      grpc:
-        endpoint: 0.0.0.0:4317 # App -> collector (gRPC)
-      http:
-        endpoint: 0.0.0.0:4318 # App -> collector (HTTP)
-
-processors:
-  batch: {}
-  resource:
-    attributes:
-      - key: "deployment.environment"
-        value: "${APP_ENVIRONMENT}"
-        action: upsert
-      - key: "service.name"
-        value: "${SERVICE_NAME}"
-        action: upsert
-      - key: "service.instance.id"
-        value: "${SERVICE_INSTANCE_ID}"
-        action: upsert
-
-exporters:
-  otlphttp:
-    endpoint: "${GRAFANA_OTLP_ENDPOINT}"
-    headers:
-      Authorization: "Basic ${GRAFANA_BASIC_AUTH}"
-    # Robustness tuning
-    timeout: 10s
-    compression: on
-    # Retry and queue settings help with transient network / rate-limit issues
-    retry_on_failure:
-      enabled: true
-      initial_interval: 5s
-      max_interval: 30s
-      max_elapsed_time: 300s
-    sending_queue:
-      enabled: true
-      num_consumers: 2
-      queue_size: 5000
-
-  logging:
-    loglevel: debug
-
-extensions:
-  health_check:
-    endpoint: 0.0.0.0:13133
-
-service:
-  telemetry:
-    logs:
-      level: "debug"
-
-  extensions: [health_check]
-
-  pipelines:
-    traces:
-      receivers: [otlp]
-      processors: [batch, resource]
-      exporters: [otlphttp, logging]
-
-    metrics:
-      receivers: [otlp]
-      processors: [batch, resource]
-      exporters: [otlphttp, logging]
-
-    logs:
-      receivers: [otlp]
-      processors: [batch, resource]
-      exporters: [otlphttp, logging]
diff --git a/pm2-logrotate.config.js b/pm2-logrotate.config.js
deleted file mode 100644
index 5b7bed6..0000000
--- a/pm2-logrotate.config.js
+++ /dev/null
@@ -1,31 +0,0 @@
-module.exports = {
-  apps: [
-    {
-      name: "ai_trader",
-      script: "uvicorn",
-      args: "app.main:app --host 0.0.0.0 --port 8000 --reload",
-      interpreter: "python3",
-      env: {
-        PYTHONPATH: ".",
-        ENV: "production",
-      },
-    },
-  ],
-
-  // PM2 Logrotate module configuration
-  deploy: {},
-  pm2: {
-    modules: {
-      "pm2-logrotate": {
-        max_size: "20M",             // rotate when logs reach 20MB
-        retain: 7,                   // keep 7 rotated logs
-        compress: true,              // compress old logs
-        dateFormat: "YYYY-MM-DD_HH-mm-ss",
-        workerInterval: 60,          // check every 60 seconds
-        rotateInterval: "0 0 * * *", // rotate daily at midnight
-        rotateModule: true,          // rotate module logs too
-        keepDays: 7,                 // remove logs older than 7 days
-      },
-    },
-  },
-};
\ No newline at end of file
diff --git a/project_structure.txt b/project_structure.txt
index ce22d56..8317164 100644
--- a/project_structure.txt
+++ b/project_structure.txt
@@ -1,9 +1,10 @@
 .
 ├── .DS_Store
 ├── .codex
-│   └── backlog.yaml
 ├── .dockerignore
-├── .env.example
+├── .env.dev
+├── .gemini
+│   └── settings.json
 ├── .github
 │   └── workflows
 │       ├── ci-deploy.yml
@@ -25,12 +26,13 @@
 │   └── config.toml
 ├── .tool-versions
 ├── .vscode
+│   ├── mcp.json
 │   └── settings.json
 ├── AGENTS.md
-├── Dockerfile
+├── CHANGELOG.md
 ├── Makefile
 ├── README.md
-├── ai_trader_logs
+├── ai-trader-logs
 ├── alembic.ini
 ├── app
 │   ├── __init__.py
@@ -49,8 +51,6 @@
 │   │   │   ├── __init__.py
 │   │   │   └── azure_blob.py
 │   │   └── telemetry
-│   │       ├── __init__.py
-│   │       └── logging.py
 │   ├── agent
 │   │   ├── __init__.py
 │   │   ├── policy.py
@@ -98,10 +98,13 @@
 │   │   ├── __init__.py
 │   │   ├── base.py
 │   │   └── volatility.py
+│   ├── logging_utils.py
 │   ├── main.py
 │   ├── monitoring
 │   │   ├── __init__.py
 │   │   └── dashboard.py
+│   ├── observability
+│   │   └── __init__.py
 │   ├── probability
 │   │   └── __init__.py
 │   ├── providers
@@ -121,6 +124,7 @@
 │   │   ├── __init__.py
 │   │   ├── session_clock.py
 │   │   └── session_metrics.py
+│   ├── settings.py
 │   ├── sources
 │   │   ├── __init__.py
 │   │   ├── finviz_source.py
@@ -145,14 +149,10 @@
 │       └── __init__.py
 ├── artifacts
 │   └── __init__.py
-├── backtest_AAPL.csv
-├── backtest_OKLO.csv
 ├── config
 │   ├── __init__.py
 │   ├── config.yaml
 │   └── watchlist_source.yaml
-├── connect.py
-├── debug_webhook_call.py
 ├── docs
 │   ├── architecture
 │   │   ├── ARCHITECTURE.md
@@ -161,6 +161,7 @@
 │   │   ├── PHASE3_BACKTESTING.md
 │   │   ├── PHASE4_EXECUTION.md
 │   │   └── PHASE5_CODEX_DOCS.md
+│   ├── architecture.md
 │   ├── operations
 │   │   ├── CI_CD.md
 │   │   ├── ROADMAP.md
@@ -179,12 +180,20 @@
 │       ├── RISK_POLICY.md
 │       ├── SESSION_MODEL.md
 │       ├── SOURCES.md
-│       └── WATCHLIST_SPEC.md
-├── dump_json
-├── ecosystem.config.cjs
+│       ├── WATCHLIST_SPEC.md
+│       └── data_layer.md
 ├── infra
 │   ├── README.md
-│   └── __init__.py
+│   ├── __init__.py
+│   ├── azure
+│   │   ├── containers.appservice.json
+│   │   └── render_sidecar.py
+│   ├── docker
+│   │   ├── Dockerfile
+│   │   └── docker-compose.yml
+│   └── otel
+│       ├── collector-config.yml
+│       └── otel.Dockerfile
 ├── migrations
 │   ├── README
 │   ├── __init__.py
@@ -192,11 +201,21 @@
 │   ├── script.py.mako
 │   └── versions
 │       └── 337e81e1d35c_initial_schema.py
-├── pm2-logrotate.config.js
 ├── project_structure.txt
 ├── pyproject.toml
 ├── pytest.ini
 ├── requirements.txt
+├── research-docs
+│   ├── AI Agents for Probabilistic Trading.pdf
+│   ├── AI_Trader_Handover.pdf
+│   ├── AI_Trader_Mind_Map.png
+│   ├── Autonomous Probabilistic Trading Agent (AI Trader) - Project Plan.pdf
+│   ├── Financial Data Source Comparison.pdf
+│   ├── M1 – Foundational Architecture & Data Layer.md
+│   ├── Phase 1 - Foundational Hardening & Data Resilience (P0).md
+│   ├── Phase 2 - Probabilistic Core Integration (P1).md
+│   ├── Phase 3 - Orchestration & Risk Control (P2P3).md
+│   └── Phase 4 - Final Validation and Scaling (P4).md
 ├── runtime.txt
 ├── scripts
 │   ├── __init__.py
@@ -206,19 +225,11 @@
 │   ├── schema.sql
 │   ├── set_webhook.sh
 │   └── strat_preview.py
-├── signals_debug_tail_AAPL.csv
-├── signals_debug_tail_OKLO.csv
-├── signals_events_AAPL.csv
-├── signals_events_OKLO.csv
-├── signals_flat_debug_AAPL.csv
-├── signals_flat_debug_OKLO.csv
-├── signals_tail_AAPL.csv
-├── signals_tail_OKLO.csv
-├── test_format.py
 └── tests
     ├── __init__.py
     ├── adapters
-    │   └── test_azure_blob_adapter.py
+    │   ├── test_azure_blob_adapter.py
+    │   └── test_telegram_http.py
     ├── api
     ├── backtest
     │   └── test_metrics.py
@@ -227,12 +238,16 @@
     ├── conftest.py
     ├── data
     │   └── test_data_client.py
+    ├── execution
+    │   └── test_alpaca_client_http.py
     ├── filters
     │   ├── __init__.py
     │   └── test_volatility_filter.py
     ├── integration
     │   ├── test_app.py
     │   └── test_telegram_smoke.py
+    ├── observability
+    │   └── test_observability.py
     ├── probability
     │   └── __init__.py
     ├── providers
@@ -244,7 +259,12 @@
     │   ├── test_breakout.py
     │   ├── test_common.py
     │   └── test_momentum.py
+    ├── support
+    │   ├── __init__.py
+    │   └── telegram_sink.py
     ├── test_config.py
+    ├── test_logging_utils.py
+    ├── test_settings.py
     ├── test_smoke.py
     ├── unit
     │   ├── test_health_config.py
@@ -266,4 +286,4 @@
     │   └── test_normalize.py
     └── wiring
 
-66 directories, 201 files
+74 directories, 213 files
diff --git a/pytest.ini b/pytest.ini
index 4a132ee..61c0941 100644
--- a/pytest.ini
+++ b/pytest.ini
@@ -1,3 +1,6 @@
 [pytest]
 addopts = -q
-pythonpath = .
\ No newline at end of file
+pythonpath = .
+filterwarnings =
+    ignore:.*asyncio\.iscoroutinefunction.*:DeprecationWarning:sentry_sdk.integrations.fastapi
+    ignore:.*asyncio\.iscoroutinefunction.*:DeprecationWarning:sentry_sdk.integrations.starlette
diff --git a/refactor: finalize architecture diagrams and DAL.md b/refactor: finalize architecture diagrams and DAL.md
deleted file mode 100644
index 10be1b6..0000000
--- a/refactor: finalize architecture diagrams and DAL.md	
+++ /dev/null
@@ -1,11 +0,0 @@
-This issue tracks the work to finalize the architecture diagrams and the Data Abstraction Layer (DAL).
-
-**Acceptance Criteria:**
-
-- [ ] The architecture diagram in `docs/architecture.md` is up-to-date and accurately reflects the current state of the codebase.
-- [ ] The Data Abstraction Layer (DAL) is formalized and integrated with the Probabilistic Core.
-- [ ] The `app/sources/` directory is updated to remove Finviz and other deprecated sources.
-
-**Depends on:**
-
-* [ZIS-60](https://linear.app/zishanmalik/issue/ZIS-60/m1-foundational-architecture-and-data-layer)
\ No newline at end of file
diff --git a/requirements.txt b/requirements.txt
index ff0cb43..392c38e 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -61,10 +61,11 @@ pluggy==1.6.0
 propcache==0.4.1
 protobuf==6.33.0
 psycopg2-binary==2.9.11
-pyarrow==21.0.0
+pyarrow==22.0.0
 pycparser==2.23
 pydantic==2.12.2
 pydantic_core==2.41.4
+pydantic-settings==2.5.2
 pydeck==0.9.1
 Pygments==2.19.2
 PyJWT==2.10.1
@@ -102,7 +103,7 @@ uvicorn==0.38.0
 uvloop==0.22.1
 watchfiles==1.1.1
 websocket-client==1.9.0
-websockets==13.0
+websockets==15.0.1
 xgboost==3.1.1
 yarl==1.22.0
 yfinance==0.2.66
@@ -119,3 +120,10 @@ openai>=1.0.0
 joblib>=1.3.0
 sentry-sdk[fastapi]
 opentelemetry-instrumentation-logging
+pydantic>=2.0
+pydantic-settings>=2.2
+opentelemetry-sdk>=1.26
+opentelemetry-exporter-otlp-proto-http
+opentelemetry-instrumentation-fastapi
+opentelemetry-instrumentation-sqlalchemy
+opentelemetry-instrumentation-httpx
diff --git "a/M1 \342\200\223 Foundational Architecture & Data Layer.md" "b/research-docs/M1 \342\200\223 Foundational Architecture & Data Layer.md"
similarity index 100%
rename from "M1 \342\200\223 Foundational Architecture & Data Layer.md"
rename to "research-docs/M1 \342\200\223 Foundational Architecture & Data Layer.md"
diff --git a/Phase 1 - Foundational Hardening & Data Resilience (P0).md b/research-docs/Phase 1 - Foundational Hardening & Data Resilience (P0).md
similarity index 100%
rename from Phase 1 - Foundational Hardening & Data Resilience (P0).md
rename to research-docs/Phase 1 - Foundational Hardening & Data Resilience (P0).md
diff --git a/Phase 2 - Probabilistic Core Integration (P1).md b/research-docs/Phase 2 - Probabilistic Core Integration (P1).md
similarity index 100%
rename from Phase 2 - Probabilistic Core Integration (P1).md
rename to research-docs/Phase 2 - Probabilistic Core Integration (P1).md
diff --git a/Phase 3 - Orchestration & Risk Control (P2P3).md b/research-docs/Phase 3 - Orchestration & Risk Control (P2P3).md
similarity index 100%
rename from Phase 3 - Orchestration & Risk Control (P2P3).md
rename to research-docs/Phase 3 - Orchestration & Risk Control (P2P3).md
diff --git a/Phase 4 - Final Validation and Scaling (P4).md b/research-docs/Phase 4 - Final Validation and Scaling (P4).md
similarity index 100%
rename from Phase 4 - Final Validation and Scaling (P4).md
rename to research-docs/Phase 4 - Final Validation and Scaling (P4).md
diff --git a/runtime.txt b/runtime.txt
index 4937ae1..069d643 100644
--- a/runtime.txt
+++ b/runtime.txt
@@ -1 +1 @@
-python-3.12.6
\ No newline at end of file
+python-3.13.9
\ No newline at end of file
diff --git a/scripts/strat_preview.py b/scripts/strat_preview.py
index f6552a6..1b2e5ab 100644
--- a/scripts/strat_preview.py
+++ b/scripts/strat_preview.py
@@ -2,12 +2,12 @@
 from __future__ import annotations
 
 import argparse
-import logging
 from datetime import date
 from pathlib import Path
 
 import numpy as np
 import pandas as pd
+from loguru import logger
 
 # Try to import your provider; otherwise fallback
 try:
@@ -21,8 +21,10 @@ from app.strats import (
     breakout_signals,
     momentum_signals,
 )
+from app.logging_utils import setup_logging
 
-log = logging.getLogger(__name__)
+
+log = logger
 
 
 def _load_data(symbol: str, start: str | None, end: str | None) -> pd.DataFrame:
@@ -76,10 +78,7 @@ def main() -> None:
     ap.add_argument("--debug", action="store_true")
     args = ap.parse_args()
 
-    logging.basicConfig(
-        level=logging.DEBUG if args.debug else logging.INFO,
-        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
-    )
+    setup_logging(force=True, level="DEBUG" if args.debug else "INFO")
 
     log.info("Loading %s from %s → %s", args.symbol, args.start, args.end or "today")
     df = _load_data(args.symbol, args.start, args.end)
diff --git a/test_format.py b/test_format.py
deleted file mode 100644
index 16c1ef6..0000000
--- a/test_format.py
+++ /dev/null
@@ -1,2 +0,0 @@
-x = 1
-print(x)
diff --git a/tests/adapters/test_telegram_http.py b/tests/adapters/test_telegram_http.py
new file mode 100644
index 0000000..6414994
--- /dev/null
+++ b/tests/adapters/test_telegram_http.py
@@ -0,0 +1,81 @@
+from __future__ import annotations
+
+import importlib
+from typing import Any, Dict, List
+
+
+def _reload_telegram(monkeypatch):
+    monkeypatch.setenv("HTTP_TIMEOUT", "11")
+    monkeypatch.setenv("HTTP_RETRIES", "2")
+    monkeypatch.setenv("HTTP_BACKOFF", "1.1")
+    monkeypatch.setenv("TELEGRAM_TIMEOUT_SECS", "9")
+    import app.utils.env as env_module
+
+    importlib.reload(env_module)
+
+    import app.adapters.notifiers.telegram as telegram_module
+
+    return importlib.reload(telegram_module)
+
+
+def test_telegram_request_applies_env(monkeypatch):
+    telegram_module = _reload_telegram(monkeypatch)
+
+    calls: List[Dict[str, Any]] = []
+    responses: List[Any] = [
+        telegram_module.requests.RequestException("boom"),
+        type(
+            "Resp429",
+            (),
+            {
+                "status_code": 429,
+                "headers": {"Retry-After": "1"},
+                "text": "Edge: Too Many Requests",
+                "json": lambda self: {"ok": False},
+            },
+        )(),
+        type(
+            "RespOk",
+            (),
+            {
+                "status_code": 200,
+                "headers": {},
+                "text": "",
+                "json": lambda self: {"ok": True},
+            },
+        )(),
+    ]
+
+    def fake_request(method, url, **kwargs):
+        calls.append({"method": method, "url": url, **kwargs})
+        outcome = responses.pop(0)
+        if isinstance(outcome, Exception):
+            raise outcome
+        return outcome
+
+    delays: List[Any] = []
+
+    def fake_backoff(attempt, backoff, retry_after):
+        delays.append((attempt, backoff, retry_after))
+        return 0.0
+
+    client = telegram_module.TelegramClient("token")
+
+    monkeypatch.setattr(telegram_module.requests, "request", fake_request)
+    monkeypatch.setattr(telegram_module, "compute_backoff_delay", fake_backoff)
+    monkeypatch.setattr(telegram_module.time, "sleep", lambda *_: None)
+
+    resp = client._request("POST", "sendMessage", json={"chat_id": 1})
+    assert resp is not None
+    assert resp.status_code == 200
+
+    assert client.timeout == 9
+    assert client.retries == 2
+    assert client.backoff == 1.1
+
+    assert len(delays) == 2
+    assert delays[0] == (0, 1.1, None)
+    assert delays[1] == (1, 1.1, "1")
+
+    assert len(calls) == 3
+    assert calls[0]["timeout"] == 9.0
diff --git a/tests/backtest/test_run_breakout_probabilistic.py b/tests/backtest/test_run_breakout_probabilistic.py
new file mode 100644
index 0000000..8a46691
--- /dev/null
+++ b/tests/backtest/test_run_breakout_probabilistic.py
@@ -0,0 +1,249 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+from typing import Any, Dict, Iterable, List
+
+import numpy as np
+import pandas as pd
+import pytest
+
+from app.agent.probabilistic.regime import RegimeSnapshot
+from app.backtest.run_breakout import run as breakout_run
+from app.dal.results import ProbabilisticBatch
+from app.dal.schemas import Bar, Bars, SignalFrame
+from app.dal.vendors.base import FetchRequest, VendorClient
+
+
+@dataclass
+class _FakeMetrics:
+    sharpe: float = 1.0
+    sortino: float = 1.0
+
+
+class _FakeBetaWinrate:
+    def __init__(self) -> None:
+        self.fmax = 0.5
+
+    def kelly_fraction(self) -> float:
+        return 0.2
+
+
+class _FakeBacktestEngine:
+    def __init__(self) -> None:
+        self.last_kwargs: Dict[str, Any] | None = None
+
+    def __call__(self, **kwargs: Any) -> Dict[str, Any]:
+        self.last_kwargs = kwargs
+        index = pd.date_range("2021-01-01", periods=5, freq="D")
+        equity = pd.Series(np.linspace(100.0, 105.0, len(index)), index=index, name="equity")
+        return {"equity": equity, "trades": []}
+
+
+class _HybridVendor(VendorClient):
+    def __init__(self, bars: Bars, signals: List[SignalFrame], regimes: List[RegimeSnapshot]) -> None:
+        super().__init__("hybrid")
+        self._batch = ProbabilisticBatch(bars=bars, signals=signals, regimes=regimes)
+        self.fetch_called_with: Dict[str, Any] | None = None
+
+    def fetch_bars(self, request: FetchRequest) -> Bars:
+        self.fetch_called_with = {
+            "symbol": request.symbol,
+            "start": request.start,
+            "end": request.end,
+            "interval": request.interval,
+        }
+        return self._batch.bars
+
+    def to_probabilistic_batch(self) -> ProbabilisticBatch:
+        return self._batch
+
+
+class _FakeMarketDataDAL:
+    def __init__(self, *args: Any, **kwargs: Any) -> None:
+        self.vendor: _HybridVendor = kwargs.pop("vendor")  # injected via monkeypatch closure
+
+    def fetch_bars(
+        self,
+        symbol: str,
+        *,
+        start: datetime | None = None,
+        end: datetime | None = None,
+        interval: str = "1Day",
+        vendor: str = "alpaca",
+        limit: int | None = None,
+    ) -> ProbabilisticBatch:
+        self.vendor.fetch_called_with = {
+            "symbol": symbol,
+            "start": start,
+            "end": end,
+            "interval": interval,
+            "vendor": vendor,
+            "limit": limit,
+        }
+        return self.vendor.to_probabilistic_batch()
+
+
+def _build_bars(symbol: str, vendor: str, start: datetime, count: int) -> Bars:
+    bars = Bars(symbol=symbol, vendor=vendor, timezone="UTC")
+    for idx in range(count):
+        ts = start + timedelta(days=idx)
+        bars.append(
+            Bar(
+                symbol=symbol,
+                vendor=vendor,
+                timestamp=ts,
+                open=100 + idx,
+                high=101 + idx,
+                low=99 + idx,
+                close=100 + idx,
+                volume=1_000 + idx,
+                timezone="UTC",
+                source="historical",
+            )
+        )
+    return bars
+
+
+def _build_signals(symbol: str, vendor: str, start: datetime, count: int) -> list[SignalFrame]:
+    signals: list[SignalFrame] = []
+    for idx in range(count):
+        ts = start + timedelta(days=idx)
+        signals.append(
+            SignalFrame(
+                symbol=symbol,
+                vendor=vendor,
+                timestamp=ts,
+                price=100.0 + idx,
+                volume=1_000 + idx,
+                filtered_price=100.0 + idx * 0.5,
+                velocity=0.1 * idx,
+                uncertainty=0.02,
+                butterworth_price=100.0 + idx * 0.4,
+                ema_price=100.0 + idx * 0.3,
+            )
+        )
+    return signals
+
+
+def _build_regimes(symbol: str, start: datetime, count: int) -> list[RegimeSnapshot]:
+    regimes: list[RegimeSnapshot] = []
+    labels = ["trend_up", "sideways", "trend_down", "high_volatility", "uncertain"]
+    for idx in range(count):
+        ts = start + timedelta(days=idx)
+        label = labels[idx % len(labels)]
+        regimes.append(
+            RegimeSnapshot(
+                symbol=symbol,
+                timestamp=ts,
+                regime=label,
+                volatility=0.01 * (idx + 1),
+                uncertainty=0.02 * (idx + 1),
+                momentum=0.001 * idx,
+            )
+        )
+    return regimes
+
+
+@pytest.mark.parametrize("use_probabilistic, regime_aware", [(True, True), (True, False)])
+def test_breakout_run_probabilistic_smoke(monkeypatch: pytest.MonkeyPatch, tmp_path: Path, use_probabilistic: bool, regime_aware: bool):
+    symbol = "AAPL"
+    start_str = "2021-01-01"
+    end_str = "2021-01-05"
+    start_dt = datetime(2021, 1, 1, tzinfo=timezone.utc)
+
+    dates = pd.date_range(start=start_str, end=end_str, freq="D")
+    history_df = pd.DataFrame(
+        {
+            "Open": 100 + np.arange(len(dates)),
+            "High": 101 + np.arange(len(dates)),
+            "Low": 99 + np.arange(len(dates)),
+            "Close": 100 + np.arange(len(dates)),
+            "Volume": np.full(len(dates), 1_000.0),
+        },
+        index=dates,
+    )
+
+    def fake_get_history_daily(symbol_: str, start, end):
+        assert symbol_ == symbol
+        return history_df.copy()
+
+    def fake_generate_signals(df: pd.DataFrame, params: Dict[str, Any]) -> pd.DataFrame:
+        base = pd.DataFrame(
+            {
+                "open": df["Open"],
+                "high": df["High"],
+                "low": df["Low"],
+                "close": df["Close"],
+                "long_entry": [True] + [False] * (len(df) - 1),
+                "long_exit": [False] * (len(df) - 1) + [True],
+                "atr": np.full(len(df), 1.5),
+                "atr_ok": True,
+            },
+            index=df.index,
+        )
+        return base
+
+    fake_beta = _FakeBetaWinrate()
+    engine_stub = _FakeBacktestEngine()
+
+    bars = _build_bars(symbol, "hybrid", start_dt, len(dates))
+    signals = _build_signals(symbol, "hybrid", start_dt, len(dates))
+    regimes = _build_regimes(symbol, start_dt, len(dates))
+    if regime_aware:
+        regimes[-1] = RegimeSnapshot(
+            symbol=symbol,
+            timestamp=start_dt + timedelta(days=len(dates) - 1),
+            regime="trend_down",
+            volatility=0.05,
+            uncertainty=0.08,
+            momentum=-0.002,
+        )
+
+    vendor = _HybridVendor(bars=bars, signals=signals, regimes=regimes)
+
+    def fake_market_data_dal_factory(*args: Any, **kwargs: Any):
+        kwargs["vendor"] = vendor
+        return _FakeMarketDataDAL(*args, **kwargs)
+
+    monkeypatch.setenv("BACKTEST_NO_SAVE", "1")
+    monkeypatch.setenv("BACKTEST_OUT_DIR", str(tmp_path))
+
+    monkeypatch.setattr("app.backtest.run_breakout.get_history_daily", fake_get_history_daily)
+    monkeypatch.setattr("app.backtest.run_breakout.generate_signals", fake_generate_signals)
+    monkeypatch.setattr("app.backtest.run_breakout.BetaWinrate", lambda: fake_beta)
+    monkeypatch.setattr("app.backtest.run_breakout.backtest_long_only", engine_stub)
+    monkeypatch.setattr("app.backtest.run_breakout.bt_metrics.equity_stats", lambda equity, use_mtm=True: _FakeMetrics())
+    monkeypatch.setattr("app.backtest.run_breakout.MarketDataDAL", fake_market_data_dal_factory)
+
+    breakout_run(
+        symbol=symbol,
+        start=start_str,
+        end=end_str,
+        params_kwargs={"lookback": 3},
+        slippage_bps=1.0,
+        fee_per_share=0.0,
+        min_notional=50.0,
+        debug=False,
+        debug_signals=False,
+        debug_entries=False,
+        regime_aware_sizing=regime_aware,
+        use_probabilistic=use_probabilistic,
+        dal_vendor="hybrid",
+        dal_interval="1Day",
+        export_csv=None,
+    )
+
+    assert engine_stub.last_kwargs is not None, "Backtest engine did not receive call"
+    risk_frac_used = engine_stub.last_kwargs["risk_frac"]
+
+    base_risk = 0.01 * fake_beta.kelly_fraction() / fake_beta.fmax
+    if use_probabilistic and regime_aware:
+        expected_multiplier = 0.6 * 0.7  # trend_down scaling with uncertainty > 0.05
+    else:
+        expected_multiplier = 1.0
+    assert pytest.approx(risk_frac_used, rel=1e-5) == base_risk * expected_multiplier
+
+    if use_probabilistic:
+        assert vendor.fetch_called_with is not None
diff --git a/tests/conftest.py b/tests/conftest.py
index 89fa654..c8d39ed 100644
--- a/tests/conftest.py
+++ b/tests/conftest.py
@@ -4,6 +4,7 @@ from __future__ import annotations
 import os
 import sys
 import importlib
+import warnings
 from typing import Any, Dict, List
 
 import pytest
@@ -15,7 +16,37 @@ except ImportError:
 
 from fastapi.testclient import TestClient
 
-from app.adapters.telemetry.loguru import configure_test_logging
+from app.logging_utils import setup_test_logging
+from tests.support import telegram_sink
+
+# Silence third-party DeprecationWarnings (Python 3.14 compatibility noise)
+warnings.filterwarnings(
+    "ignore",
+    category=DeprecationWarning,
+    module="sentry_sdk.integrations.fastapi",
+)
+warnings.filterwarnings(
+    "ignore",
+    category=DeprecationWarning,
+    module="sentry_sdk.integrations.starlette",
+)
+warnings.filterwarnings(
+    "ignore",
+    category=DeprecationWarning,
+    message=".*asyncio\\.iscoroutinefunction.*",
+)
+
+def pytest_configure(config):
+    warnings.filterwarnings(
+        "ignore",
+        category=DeprecationWarning,
+        module=r"sentry_sdk\.integrations\.fastapi",
+    )
+    warnings.filterwarnings(
+        "ignore",
+        category=DeprecationWarning,
+        message=".*asyncio\\.iscoroutinefunction.*",
+    )
 
 # -----------------------------------------------------------------------------
 # Test env — set BEFORE importing the app so the adapter boots predictably
@@ -49,19 +80,16 @@ def pytest_sessionstart(session):
     """
     from pathlib import Path
     log_path = Path("ai-trader-logs/")
-    configure_test_logging(log_path)
+    setup_test_logging(log_path)
 
 # -----------------------------------------------------------------------------
 # Local sink for Telegram messages (fully under test control)
 # -----------------------------------------------------------------------------
-_FAKE_TG_SINK: List[Dict[str, Any]] = []
-_HTTP_OUTBOX: list[dict] = []
-
 def _sink_clear() -> None:
-    _FAKE_TG_SINK.clear()
+    telegram_sink.sink_clear()
 
 def _sink_snapshot() -> List[Dict[str, Any]]:
-    return list(_FAKE_TG_SINK)
+    return telegram_sink.sink_snapshot()
 
 # Expose helpers expected by tests (return just text strings)
 def _outbox():
@@ -72,17 +100,14 @@ def _outbox():
       3) Patched requests.post capture (HTTP)
     """
     from app.adapters.notifiers.telegram import test_outbox
-    sink_msgs = [m.get("text", "") for m in _FAKE_TG_SINK]
-    adapter_msgs = [m.get("text", "") for m in test_outbox()]
-    http_msgs = [m.get("text", "") for m in _HTTP_OUTBOX]
-    # preserve intuitive order: sink -> adapter -> http
-    return sink_msgs + adapter_msgs + http_msgs
+    adapter_msgs = list(test_outbox())
+    return telegram_sink.merged_outbox(adapter_msgs)
 
 def _clear_outbox():
     from app.adapters.notifiers.telegram import test_outbox_clear
     test_outbox_clear()
-    _HTTP_OUTBOX.clear()
-    _FAKE_TG_SINK.clear()
+    telegram_sink.http_clear()
+    telegram_sink.sink_clear()
 
 # Helper to extract the actual dependency callable from an Annotated alias like TelegramDep
 def _dep_callable_from_alias(alias):
@@ -129,14 +154,11 @@ if _requests:
                 elif "data" in kwargs and isinstance(kwargs["data"], dict):
                     payload = dict(kwargs["data"])
                 # Capture to HTTP outbox for visibility
-                try:
-                    _HTTP_OUTBOX.append({
-                        "chat_id": payload.get("chat_id"),
-                        "text": payload.get("text", ""),
-                        "parse_mode": payload.get("parse_mode")
-                    })
-                except Exception:
-                    pass
+                telegram_sink.http_append(
+                    payload.get("chat_id"),
+                    payload.get("text", ""),
+                    payload.get("parse_mode"),
+                )
                 return _FakeResp(
                     200,
                     json_body={
@@ -281,7 +303,7 @@ def _telegram_fake_layer():
     _orig_send_message = getattr(tgmod.TelegramClient, "send_message", None)
 
     def _sink_append(chat_id: int | str, text: str, parse_mode: str | None):
-        _FAKE_TG_SINK.append({"chat_id": chat_id, "text": text or "", "parse_mode": parse_mode})
+        telegram_sink.sink_append(chat_id, text, parse_mode)
 
     def smart_send_stub(self, chat_id, text, *, parse_mode=None, mode=None, chunk_size=3500, retries=2, **_kw):
         eff_mode = parse_mode or mode
@@ -366,11 +388,7 @@ def _reassert_telegram_override_per_test():
 
     # Local sink appender
     def _sink_append(chat_id, text, parse_mode):
-        _FAKE_TG_SINK.append({
-            "chat_id": chat_id,
-            "text": text or "",
-            "parse_mode": parse_mode,
-        })
+        telegram_sink.sink_append(chat_id, text, parse_mode)
 
     # Stub TelegramClient methods (covers Fake/Real instances)
     def _smart_send(self, chat_id, text, *, parse_mode=None, mode=None, chunk_size=3500, **_kw):
diff --git a/tests/dal/test_kalman.py b/tests/dal/test_kalman.py
new file mode 100644
index 0000000..e56770b
--- /dev/null
+++ b/tests/dal/test_kalman.py
@@ -0,0 +1,14 @@
+from __future__ import annotations
+
+from app.dal.kalman import KalmanConfig, KalmanFilter1D
+
+
+def test_kalman_filter_velocity_positive():
+    filt = KalmanFilter1D(KalmanConfig(process_variance=1e-4, measurement_variance=1e-3))
+    prices = [10 + i * 0.5 for i in range(10)]
+    last_velocity = None
+    for price in prices:
+        _, velocity, _ = filt.step(price)
+        last_velocity = velocity
+    assert last_velocity is not None
+    assert last_velocity > 0
diff --git a/tests/dal/test_manager.py b/tests/dal/test_manager.py
new file mode 100644
index 0000000..6b48623
--- /dev/null
+++ b/tests/dal/test_manager.py
@@ -0,0 +1,125 @@
+from __future__ import annotations
+
+import asyncio
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+
+import pytest
+
+from app.dal.manager import MarketDataDAL
+from app.dal.results import ProbabilisticStreamFrame
+from app.dal.schemas import Bar, Bars
+from app.dal.vendors.base import FetchRequest, VendorClient
+
+
+class FakeVendor(VendorClient):
+    def __init__(self) -> None:
+        super().__init__("fake")
+
+    def fetch_bars(self, request: FetchRequest) -> Bars:
+        bars = Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+        base = datetime(2024, 1, 1, tzinfo=timezone.utc)
+        for idx in range(3):
+            bars.append(
+                Bar(
+                    symbol=request.symbol.upper(),
+                    vendor=self.name,
+                    timestamp=base + timedelta(minutes=idx),
+                    open=100 + idx,
+                    high=101 + idx,
+                    low=99 + idx,
+                    close=100 + idx,
+                    volume=1_000 + idx,
+                    timezone="UTC",
+                    source="test",
+                )
+            )
+        return bars
+
+
+class StreamingVendor(VendorClient):
+    def __init__(self, events: list[dict]) -> None:
+        super().__init__("stream_fake")
+        self._events = events
+
+    def fetch_bars(self, request: FetchRequest) -> Bars:
+        bars = Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+        base = request.start or datetime.now(timezone.utc)
+        bars.append(
+            Bar(
+                symbol=request.symbol.upper(),
+                vendor=self.name,
+                timestamp=base + timedelta(seconds=30),
+                open=200,
+                high=201,
+                low=199,
+                close=200,
+                volume=5_000,
+                timezone="UTC",
+                source="backfill",
+            )
+        )
+        return bars
+
+    def supports_streaming(self) -> bool:
+        return True
+
+    async def stream_bars(self, symbols, interval: str):  # type: ignore[override]
+        for payload in self._events:
+            await asyncio.sleep(0)
+            yield payload
+
+
+def test_market_data_dal_fetch(tmp_path: Path, monkeypatch):
+    dal = MarketDataDAL(
+        cache_dir=tmp_path,
+        vendor_clients={"fake": FakeVendor()},
+        enable_postgres_metadata=False,
+    )
+    batch = dal.fetch_bars("AAPL", vendor="fake")
+    assert batch.bars.symbol == "AAPL"
+    assert len(batch.bars.data) == 3
+    assert len(batch.signals) == 3
+    assert len(batch.regimes) == 3
+    assert "bars" in batch.cache_paths
+    assert "signals" in batch.cache_paths
+    assert "regimes" in batch.cache_paths
+    parquet_files = list(tmp_path.glob("*.parquet"))
+    assert parquet_files, "expected cached parquet output"
+
+
+@pytest.mark.asyncio
+async def test_market_data_dal_stream_backfill(tmp_path: Path):
+    base = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    events = [
+        {
+            "symbol": "AAPL",
+            "timestamp": base,
+            "price": 210.0,
+            "volume": 1000,
+        },
+        {
+            # gap that should trigger backfill
+            "symbol": "AAPL",
+            "timestamp": base + timedelta(minutes=5),
+            "price": 215.0,
+            "volume": 1200,
+        },
+    ]
+    vendor = StreamingVendor(events)
+    dal = MarketDataDAL(
+        cache_dir=tmp_path,
+        vendor_clients={"stream": vendor},
+        enable_postgres_metadata=False,
+    )
+
+    frames: list[ProbabilisticStreamFrame] = []
+    async for frame in dal.stream_bars(["AAPL"], vendor="stream", interval="1Min"):
+        frames.append(frame)
+        if len(frames) >= 3:
+            break
+
+    assert frames, "expected streamed frames"
+    symbols = {frame.signal.symbol for frame in frames}
+    assert symbols == {"AAPL"}
+    assert all(frame.regime.symbol == "AAPL" for frame in frames)
diff --git a/tests/dal/test_probabilistic_pipeline.py b/tests/dal/test_probabilistic_pipeline.py
new file mode 100644
index 0000000..1a062fa
--- /dev/null
+++ b/tests/dal/test_probabilistic_pipeline.py
@@ -0,0 +1,112 @@
+from __future__ import annotations
+
+import asyncio
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+from typing import Iterable, List
+
+import pytest
+
+from app.dal.manager import MarketDataDAL
+from app.dal.results import ProbabilisticStreamFrame
+from app.dal.schemas import Bar, Bars
+from app.dal.vendors.base import FetchRequest, VendorClient
+
+
+class HybridVendor(VendorClient):
+    def __init__(self, bars: List[Bar], stream_events: List[dict]) -> None:
+        super().__init__("hybrid")
+        self._bars = bars
+        self._stream_events = stream_events
+
+    def fetch_bars(self, request: FetchRequest) -> Bars:
+        out = Bars(symbol=request.symbol.upper(), vendor=self.name, timezone="UTC")
+        for bar in self._bars:
+            if request.start and bar.timestamp < request.start:
+                continue
+            if request.end and bar.timestamp > request.end:
+                continue
+            out.append(bar)
+            if request.limit and len(out.data) >= request.limit:
+                break
+        if not out.data:
+            # provide minimal response so consumers still get structure
+            for bar in self._bars:
+                out.append(bar)
+                break
+        return out
+
+    def supports_streaming(self) -> bool:
+        return True
+
+    async def stream_bars(self, symbols: Iterable[str], interval: str):  # type: ignore[override]
+        for payload in self._stream_events:
+            await asyncio.sleep(0)
+            yield payload
+
+
+@pytest.mark.asyncio
+async def test_probabilistic_pipeline_end_to_end(tmp_path: Path):
+    base = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    historical_bars = [
+        Bar(
+            symbol="AAPL",
+            vendor="hybrid",
+            timestamp=base + timedelta(minutes=idx),
+            open=100 + idx,
+            high=101 + idx,
+            low=99 + idx,
+            close=100 + idx,
+            volume=1_000 + idx * 10,
+            timezone="UTC",
+            source="historical",
+        )
+        for idx in range(10)
+    ]
+
+    stream_events = [
+        {
+            "symbol": "AAPL",
+            "timestamp": base + timedelta(minutes=5),
+            "price": 105.0,
+            "volume": 1_200,
+        },
+        {
+            # skip ahead to trigger deterministic backfill
+            "symbol": "AAPL",
+            "timestamp": base + timedelta(minutes=9),
+            "price": 109.0,
+            "volume": 1_300,
+        },
+    ]
+
+    vendor = HybridVendor(historical_bars, stream_events)
+    dal = MarketDataDAL(
+        cache_dir=tmp_path,
+        vendor_clients={"hybrid": vendor},
+        enable_postgres_metadata=False,
+    )
+
+    batch = dal.fetch_bars("AAPL", vendor="hybrid", limit=5)
+    assert batch.bars.symbol == "AAPL"
+    assert len(batch.signals) == 5
+    assert len(batch.regimes) == 5
+    assert all(frame.butterworth_price is not None for frame in batch.signals)
+    assert all(frame.ema_price is not None for frame in batch.signals)
+    assert batch.regimes[-1].regime in {"trend_up", "sideways", "calm"}
+
+    stream_frames: list[ProbabilisticStreamFrame] = []
+    target_ts = stream_events[-1]["timestamp"]
+    async for payload in dal.stream_bars(["AAPL"], vendor="hybrid", interval="1Min"):
+        stream_frames.append(payload)
+        if payload.signal.timestamp >= target_ts:
+            break
+        if len(stream_frames) > 12:
+            break
+
+    assert len(stream_frames) >= len(stream_events)
+    assert stream_frames[0].signal.timestamp > batch.bars.data[0].timestamp
+    assert stream_frames[-1].signal.timestamp >= target_ts
+    assert all(frame.signal.butterworth_price is not None for frame in stream_frames)
+    assert all(frame.regime.symbol == "AAPL" for frame in stream_frames)
+    assert stream_frames[-1].regime.regime in {"trend_up", "sideways", "calm", "high_volatility"}
diff --git a/tests/execution/test_alpaca_client_http.py b/tests/execution/test_alpaca_client_http.py
new file mode 100644
index 0000000..31e0ebe
--- /dev/null
+++ b/tests/execution/test_alpaca_client_http.py
@@ -0,0 +1,71 @@
+from __future__ import annotations
+
+import importlib
+from typing import Any, Dict, List
+
+
+def _reload_env_and_module(monkeypatch, *, timeout="20", retries="3", backoff="2.5"):
+    monkeypatch.setenv("HTTP_TIMEOUT", timeout)
+    monkeypatch.setenv("HTTP_RETRIES", retries)
+    monkeypatch.setenv("HTTP_BACKOFF", backoff)
+    import app.utils.env as env_module
+
+    env = importlib.reload(env_module)
+
+    import app.execution.alpaca_client as alpaca_module
+
+    module = importlib.reload(alpaca_module)
+    return env, module
+
+
+def test_alpaca_client_uses_env_defaults(monkeypatch):
+    _env, alpaca_module = _reload_env_and_module(
+        monkeypatch, timeout="25", retries="4", backoff="3.0"
+    )
+
+    calls: List[Dict[str, Any]] = []
+    responses: List[Any] = [
+        alpaca_module.requests.RequestException("boom"),
+        type(
+            "Resp",
+            (),
+            {
+                "status_code": 200,
+                "headers": {},
+                "json": lambda self: {"ok": True},
+                "text": "",
+            },
+        )(),
+    ]
+
+    def fake_request(method, url, **kwargs):
+        calls.append({"method": method, "url": url, **kwargs})
+        outcome = responses.pop(0)
+        if isinstance(outcome, Exception):
+            raise outcome
+        return outcome
+
+    delays: List[Any] = []
+
+    def fake_backoff(attempt, backoff, retry_after):
+        delays.append((attempt, backoff, retry_after))
+        return 0.0
+
+    monkeypatch.setattr(alpaca_module.requests, "request", fake_request)
+    monkeypatch.setattr(alpaca_module, "compute_backoff_delay", fake_backoff)
+    monkeypatch.setattr(alpaca_module.time, "sleep", lambda *_: None)
+
+    client = alpaca_module.AlpacaClient(
+        "key", "secret", "https://example.com", data_url="https://example.com/data"
+    )
+
+    assert client.timeout == 25.0
+    assert client.retries == 4
+    assert client.backoff == 3.0
+
+    resp = client._request("GET", "https://example.com/test")
+    assert resp.status_code == 200
+
+    assert len(delays) == 1
+    assert delays[0] == (0, 3.0, None)
+    assert calls[0]["timeout"] == 25.0
diff --git a/tests/integration/test_app.py b/tests/integration/test_app.py
index e203579..95c0ffe 100644
--- a/tests/integration/test_app.py
+++ b/tests/integration/test_app.py
@@ -33,15 +33,20 @@ def make_client(monkeypatch):
 
     fake_db.make_engine = lambda: DummyEngine()
     fake_db.ping = lambda retries=1: True
-    sys.modules["app.adapters.db.postgres"] = fake_db
-    sys.modules.setdefault("app.adapters.db", types.ModuleType("app.adapters.db"))
-    fake_loguru = types.ModuleType("loguru")
-    fake_loguru.logger = types.SimpleNamespace(debug=lambda *_, **__: None)
-    sys.modules["loguru"] = fake_loguru
+    monkeypatch.setitem(sys.modules, "app.adapters.db.postgres", fake_db)
+    monkeypatch.setitem(
+        sys.modules,
+        "app.adapters.db",
+        sys.modules.get("app.adapters.db", types.ModuleType("app.adapters.db")),
+    )
 
     fake_notifier = types.ModuleType("app.adapters.notifiers.telegram_notifier")
     fake_notifier.TelegramNotifier = object
-    sys.modules["app.adapters.notifiers.telegram_notifier"] = fake_notifier
+    monkeypatch.setitem(
+        sys.modules,
+        "app.adapters.notifiers.telegram_notifier",
+        fake_notifier,
+    )
 
     import app.main as main_module
 
diff --git a/tests/observability/test_observability.py b/tests/observability/test_observability.py
new file mode 100644
index 0000000..ef90842
--- /dev/null
+++ b/tests/observability/test_observability.py
@@ -0,0 +1,49 @@
+from __future__ import annotations
+
+import importlib
+from typing import Iterable
+
+import pytest
+
+
+_OTEL_ENV_FLAGS = (
+    "OTEL_EXPORTER_OTLP_ENDPOINT",
+    "OTEL_EXPORTER_OTLP_TRACES_ENDPOINT",
+    "OTEL_TRACES_EXPORTER",
+    "OTEL_EXPORTER_OTLP_METRICS_ENDPOINT",
+    "OTEL_METRICS_EXPORTER",
+    "OTEL_EXPORTER_OTLP_LOGS_ENDPOINT",
+    "OTEL_LOGS_EXPORTER",
+)
+
+
+def _reload_module():
+    import app.observability as observability
+
+    return importlib.reload(observability)
+
+
+def _clear_env(monkeypatch, keys: Iterable[str]):
+    for key in keys:
+        monkeypatch.delenv(key, raising=False)
+
+
+def test_configure_observability_without_env(monkeypatch):
+    _clear_env(monkeypatch, _OTEL_ENV_FLAGS)
+    module = _reload_module()
+
+    assert module.configure_observability() is False
+
+
+def test_configure_tracing_with_env(monkeypatch):
+    pytest.importorskip("opentelemetry.sdk.trace")
+    pytest.importorskip("opentelemetry.exporter.otlp.proto.grpc.trace_exporter")
+
+    _clear_env(monkeypatch, _OTEL_ENV_FLAGS)
+    module = _reload_module()
+    monkeypatch.setenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://collector:4317")
+    monkeypatch.setenv("OTEL_SERVICE_NAME", "ai-trader-tests")
+
+    assert module.configure_tracing() is True
+    # Once configured, repeated calls should short-circuit.
+    assert module.configure_tracing() is True
diff --git a/tests/probability/test_regime_analysis_agent.py b/tests/probability/test_regime_analysis_agent.py
new file mode 100644
index 0000000..e383adf
--- /dev/null
+++ b/tests/probability/test_regime_analysis_agent.py
@@ -0,0 +1,63 @@
+from __future__ import annotations
+
+from datetime import datetime, timedelta, timezone
+from typing import Iterable
+
+from app.agent.probabilistic.regime import RegimeAnalysisAgent
+from app.dal.schemas import SignalFrame
+
+
+def _build_frames(
+    prices: Iterable[float],
+    *,
+    symbol: str = "AAPL",
+    uncertainty: float = 0.01,
+) -> list[SignalFrame]:
+    frames: list[SignalFrame] = []
+    ts_base = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    for idx, price in enumerate(prices):
+        frames.append(
+            SignalFrame(
+                symbol=symbol,
+                vendor="alpaca",
+                timestamp=ts_base + timedelta(minutes=idx),
+                price=float(price),
+                volume=1_000.0,
+                filtered_price=float(price),
+                velocity=0.0,
+                uncertainty=uncertainty,
+                butterworth_price=float(price),
+                ema_price=float(price),
+            )
+        )
+    return frames
+
+
+def test_regime_analysis_agent_marks_uncertain_when_uncertainty_spikes() -> None:
+    prices = [100.0 + 0.1 * idx for idx in range(15)]
+    frames = _build_frames(prices, uncertainty=0.2)
+
+    agent = RegimeAnalysisAgent(window=5)
+
+    snapshots = agent.classify(frames)
+
+    assert all(snapshot.regime == "uncertain" for snapshot in snapshots)
+
+
+def test_regime_analysis_agent_detects_market_states() -> None:
+    agent = RegimeAnalysisAgent(window=10, high_vol_threshold=0.03)
+
+    trend_up_prices = [100.0 + 0.3 * idx for idx in range(40)]
+    trend_down_prices = [100.0 - 0.3 * idx for idx in range(40)]
+    sideways_prices = [100.0 + (-1.0) ** idx * 1.0 for idx in range(60)]
+    high_vol_prices = [100.0 + (-1.0) ** idx * 5.0 for idx in range(60)]
+
+    trend_up = agent.classify(_build_frames(trend_up_prices))
+    trend_down = agent.classify(_build_frames(trend_down_prices))
+    sideways = agent.classify(_build_frames(sideways_prices))
+    high_vol = agent.classify(_build_frames(high_vol_prices))
+
+    assert trend_up[-1].regime == "trend_up"
+    assert trend_down[-1].regime == "trend_down"
+    assert sideways[-1].regime == "sideways"
+    assert high_vol[-1].regime == "high_volatility"
diff --git a/tests/probability/test_signal_filter_agent.py b/tests/probability/test_signal_filter_agent.py
new file mode 100644
index 0000000..7178efb
--- /dev/null
+++ b/tests/probability/test_signal_filter_agent.py
@@ -0,0 +1,72 @@
+from __future__ import annotations
+
+from datetime import datetime, timedelta, timezone
+
+import numpy as np
+import pytest
+
+from app.agent.probabilistic.signal_filter import FilterConfig, SignalFilteringAgent
+from app.dal.schemas import Bar, Bars
+
+
+def _build_bars(prices: list[float], *, symbol: str = "AAPL") -> Bars:
+    ts_base = datetime(2024, 1, 1, tzinfo=timezone.utc)
+    bars = Bars(symbol=symbol, vendor="alpaca", timezone="UTC")
+    for idx, price in enumerate(prices):
+        timestamp = ts_base + timedelta(minutes=idx)
+        bars.append(
+            Bar(
+                symbol=symbol,
+                vendor="alpaca",
+                timestamp=timestamp,
+                open=price,
+                high=price,
+                low=price,
+                close=price,
+                volume=1_000.0,
+            )
+        )
+    return bars
+
+
+def test_signal_filtering_agent_returns_empty_list_for_empty_bars() -> None:
+    agent = SignalFilteringAgent()
+    empty_bars = Bars(symbol="AAPL", vendor="alpaca", timezone="UTC")
+
+    frames = agent.run(empty_bars)
+
+    assert frames == []
+
+
+def test_signal_filtering_agent_smooths_high_frequency_noise() -> None:
+    trend = [100.0 + 0.5 * idx for idx in range(60)]
+    noise = [(-1.0) ** idx * 2.0 for idx in range(60)]
+    prices = [t + n for t, n in zip(trend, noise, strict=True)]
+    bars = _build_bars(prices)
+
+    config = FilterConfig(butterworth_cutoff=0.15, butterworth_order=2, ema_span=8)
+    agent = SignalFilteringAgent(config)
+
+    frames = agent.run(bars)
+
+    assert len(frames) == len(prices)
+    butterworth = np.array([frame.butterworth_price for frame in frames], dtype=float)
+    ema = np.array([frame.ema_price for frame in frames], dtype=float)
+    filtered = np.array([frame.filtered_price for frame in frames], dtype=float)
+    uncertainties = np.array([frame.uncertainty for frame in frames], dtype=float)
+    velocities = np.array([frame.velocity for frame in frames], dtype=float)
+
+    trend_arr = np.array(trend, dtype=float)
+    prices_arr = np.array(prices, dtype=float)
+
+    price_step_variation = float(np.mean(np.abs(np.diff(prices_arr))))
+    butterworth_step_variation = float(np.mean(np.abs(np.diff(butterworth))))
+    ema_step_variation = float(np.mean(np.abs(np.diff(ema))))
+
+    assert np.all(np.isfinite(butterworth))
+    assert np.all(np.isfinite(ema))
+    assert butterworth_step_variation < price_step_variation
+    assert ema_step_variation < price_step_variation
+    assert np.all(uncertainties >= 0.0)
+    assert velocities[-1] > 0.0
+    assert filtered[-1] == pytest.approx(trend_arr[-1], abs=2.0)
diff --git a/tests/providers/test_yahoo_provider.py b/tests/providers/test_yahoo_provider.py
index 314b227..c82dc00 100644
--- a/tests/providers/test_yahoo_provider.py
+++ b/tests/providers/test_yahoo_provider.py
@@ -1,5 +1,9 @@
 from __future__ import annotations
 
+import importlib
+
+import types
+
 from loguru import logger
 
 from app.providers import yahoo_provider
@@ -16,18 +20,92 @@ def test_fetch_chart_history_non_200(monkeypatch, caplog):
         return 500, {"chart": {"error": "nope"}}
 
     monkeypatch.setattr(yahoo_provider, "http_get", fake_http_get)
-    logger.remove()
-    logger.add(caplog.handler, level="WARNING")
+    handler_id = logger.add(caplog.handler, level="WARNING")
+
+    try:
+        data = yahoo_provider._fetch_chart_history("AAPL", "2024-01-01", "2024-01-10")
+
+        assert data == {}
+        assert "period1" in captured["params"]
+        assert captured["kwargs"]["retries"] == ENV.HTTP_RETRIES
+        assert captured["kwargs"]["backoff"] == ENV.HTTP_BACKOFF
+        assert captured["kwargs"]["timeout"] == ENV.HTTP_TIMEOUT
+
+        assert any("yahoo history fetch failed" in rec.message for rec in caplog.records)
+        record = caplog.records[0]
+        assert getattr(record, "provider", None) == "yahoo"
+        assert getattr(record, "status", None) == 500
+    finally:
+        logger.remove(handler_id)
+
+
+def test_yahoo_request_uses_env(monkeypatch):
+    monkeypatch.setenv("HTTP_TIMEOUT", "12")
+    monkeypatch.setenv("HTTP_RETRIES", "2")
+    monkeypatch.setenv("HTTP_BACKOFF", "1.7")
+
+    import app.utils.env as env_module
+    importlib.reload(env_module)
+
+    import app.providers.yahoo_provider as module
+    module = importlib.reload(module)
+
+    calls = []
+    outcomes = [
+        module.requests.RequestException("boom"),
+        type(
+            "Resp429",
+            (),
+            {
+                "status_code": 429,
+                "headers": {"Retry-After": "1"},
+                "text": "Edge: Too Many Requests",
+                "json": lambda self: {},
+            },
+        )(),
+        type(
+            "RespOk",
+            (),
+            {
+                "status_code": 200,
+                "headers": {},
+                "text": "",
+                "json": lambda self: {"chart": {"result": True}},
+            },
+        )(),
+    ]
+
+    def fake_get(url, **kwargs):
+        calls.append({"url": url, **kwargs})
+        outcome = outcomes.pop(0)
+        if isinstance(outcome, Exception):
+            raise outcome
+        return outcome
+
+    delays = []
+
+    def fake_backoff(attempt, backoff, retry_after):
+        delays.append((attempt, backoff, retry_after))
+        return 0.0
+
+    if not hasattr(module.logger, "warning"):
+        module.logger = types.SimpleNamespace(
+            warning=lambda *_, **__: None,
+            debug=lambda *_, **__: None,
+            info=lambda *_, **__: None,
+            error=lambda *_, **__: None,
+        )
 
-    data = yahoo_provider._fetch_chart_history("AAPL", "2024-01-01", "2024-01-10")
+    monkeypatch.setattr(module.requests, "get", fake_get)
+    monkeypatch.setattr(module, "compute_backoff_delay", fake_backoff)
+    monkeypatch.setattr(module.time, "sleep", lambda *_: None)
 
-    assert data == {}
-    assert "period1" in captured["params"]
-    assert captured["kwargs"]["retries"] == ENV.HTTP_RETRIES
-    assert captured["kwargs"]["backoff"] == ENV.HTTP_BACKOFF
-    assert captured["kwargs"]["timeout"] == ENV.HTTP_TIMEOUT
+    status, data = module._yahoo_request("https://example.com")
+    assert status == 200
+    assert data == {"chart": {"result": True}}
 
-    assert any("yahoo history fetch failed" in rec.message for rec in caplog.records)
-    record = caplog.records[0]
-    assert record.extra["provider"] == "yahoo"
-    assert record.extra["status"] == 500
+    assert len(calls) == 3
+    assert calls[0]["timeout"] == 12.0
+    assert len(delays) == 2
+    assert delays[0] == (0, 1.7, None)
+    assert delays[1] == (1, 1.7, "1")
diff --git a/tests/support/__init__.py b/tests/support/__init__.py
new file mode 100644
index 0000000..5a6392c
--- /dev/null
+++ b/tests/support/__init__.py
@@ -0,0 +1 @@
+"""Shared test helpers."""
diff --git a/tests/support/telegram_sink.py b/tests/support/telegram_sink.py
new file mode 100644
index 0000000..7ad3def
--- /dev/null
+++ b/tests/support/telegram_sink.py
@@ -0,0 +1,61 @@
+from __future__ import annotations
+
+from typing import Any, Dict, List
+
+_FAKE_TG_SINK: List[Dict[str, Any]] = []
+_HTTP_OUTBOX: List[Dict[str, Any]] = []
+
+
+def sink_clear() -> None:
+    """Clear all captured Telegram messages."""
+    _FAKE_TG_SINK.clear()
+
+
+def sink_snapshot() -> List[Dict[str, Any]]:
+    """Return a shallow copy of the captured Telegram sink."""
+    return list(_FAKE_TG_SINK)
+
+
+def sink_append(chat_id: int | str, text: str, parse_mode: str | None) -> None:
+    """Add a message to the sink."""
+    _FAKE_TG_SINK.append(
+        {
+            "chat_id": chat_id,
+            "text": text or "",
+            "parse_mode": parse_mode,
+        }
+    )
+
+
+def http_append(chat_id: int | str, text: str, parse_mode: str | None) -> None:
+    """Record a Telegram HTTP payload (e.g., via requests.post)."""
+    _HTTP_OUTBOX.append(
+        {
+            "chat_id": chat_id,
+            "text": text or "",
+            "parse_mode": parse_mode,
+        }
+    )
+
+
+def http_clear() -> None:
+    """Clear captured HTTP payloads."""
+    _HTTP_OUTBOX.clear()
+
+
+def merged_outbox(adapter_messages: List[Dict[str, Any]]) -> List[str]:
+    """Aggregate sink, adapter, and HTTP payloads into a unified list of messages."""
+    sink_msgs = [m.get("text", "") for m in _FAKE_TG_SINK]
+    adapter_msgs = [m.get("text", "") for m in adapter_messages]
+    http_msgs = [m.get("text", "") for m in _HTTP_OUTBOX]
+    return sink_msgs + adapter_msgs + http_msgs
+
+
+__all__ = [
+    "sink_clear",
+    "sink_snapshot",
+    "sink_append",
+    "http_append",
+    "http_clear",
+    "merged_outbox",
+]
diff --git a/tests/test_logging_utils.py b/tests/test_logging_utils.py
new file mode 100644
index 0000000..b9407ba
--- /dev/null
+++ b/tests/test_logging_utils.py
@@ -0,0 +1,63 @@
+from __future__ import annotations
+
+import logging
+from contextlib import contextmanager
+
+import pytest
+from loguru import logger
+
+from app.config import settings
+from app.logging_utils import setup_logging
+
+
+@pytest.fixture(autouse=True)
+def _reset_logging():
+    setup_logging(force=True, level="INFO")
+    yield
+    setup_logging(force=True, level="INFO")
+
+
+@contextmanager
+def capture_records(level=logging.INFO):
+    records = []
+
+    class ListHandler(logging.Handler):
+        def emit(self, record):
+            records.append(record)
+
+    handler = ListHandler()
+    handler.setLevel(level)
+    root = logging.getLogger()
+    prev_level = root.level
+    root.setLevel(level)
+    root.addHandler(handler)
+    try:
+        yield records
+    finally:
+        root.removeHandler(handler)
+        root.setLevel(prev_level)
+
+
+def test_setup_logging_attaches_metadata(monkeypatch):
+    monkeypatch.setenv("ENV", "staging")
+    monkeypatch.setenv("GIT_SHA", "abc123")
+
+    setup_logging(force=True, level="INFO")
+
+    with capture_records() as records:
+        logger.info("hello world")
+
+    record = records[-1]
+    assert record.environment == "staging"
+    assert record.git_sha == "abc123"
+    assert record.service_version == settings.VERSION
+    assert record.request_id == "-"
+
+
+def test_contextual_request_id():
+    with capture_records() as records:
+        with logger.contextualize(request_id="req-1"):
+            logger.info("with request id")
+
+    record = records[-1]
+    assert record.request_id == "req-1"
diff --git a/tests/test_settings.py b/tests/test_settings.py
new file mode 100644
index 0000000..1d98b68
--- /dev/null
+++ b/tests/test_settings.py
@@ -0,0 +1,85 @@
+from __future__ import annotations
+
+from app import settings as settings_module
+
+
+def test_otel_settings_flags(monkeypatch):
+    for key in (
+        "OTEL_EXPORTER_OTLP_ENDPOINT",
+        "OTEL_EXPORTER_OTLP_TRACES_ENDPOINT",
+        "OTEL_TRACES_EXPORTER",
+        "OTEL_EXPORTER_OTLP_METRICS_ENDPOINT",
+        "OTEL_METRICS_EXPORTER",
+        "OTEL_EXPORTER_OTLP_LOGS_ENDPOINT",
+        "OTEL_LOGS_EXPORTER",
+    ):
+        monkeypatch.delenv(key, raising=False)
+
+    settings_module.reload_settings()
+    otel = settings_module.get_otel_settings()
+    assert otel.traces_enabled is False
+    assert otel.metrics_enabled is False
+    assert otel.logs_enabled is False
+
+    monkeypatch.setenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://collector:4317")
+    monkeypatch.setenv("OTEL_RESOURCE_ATTRIBUTES", "service.namespace=ai,env=local")
+    settings_module.reload_settings()
+    otel = settings_module.get_otel_settings()
+
+    assert otel.traces_enabled is True
+    assert otel.metrics_enabled is True
+    assert otel.logs_enabled is True
+    assert ("service.namespace", "ai") in otel.resource_attributes_map
+    assert ("env", "local") in otel.resource_attributes_map
+
+
+def test_telegram_settings_parses_allowlist(monkeypatch):
+    monkeypatch.setenv("TELEGRAM_ALLOWED_USER_IDS", "123 , 456,notint, 789 ")
+    monkeypatch.setenv("TELEGRAM_TIMEOUT_SECS", "15")
+    monkeypatch.setenv("TELEGRAM_FAKE", "1")
+
+    settings_module.reload_settings()
+    telegram = settings_module.get_telegram_settings()
+    assert telegram.allowed_user_ids == (123, 456, 789)
+    assert telegram.timeout_secs == 15
+    assert telegram.fake_mode is True
+
+
+def test_database_settings_fallback(monkeypatch):
+    for key in ("DATABASE_URL", "TEST_DATABASE_URL"):
+        monkeypatch.delenv(key, raising=False)
+    monkeypatch.setenv("PGUSER", "user")
+    monkeypatch.setenv("PGPASSWORD", "p@ss word")
+    monkeypatch.setenv("PGHOST", "db.local")
+    monkeypatch.setenv("PGPORT", "6543")
+    monkeypatch.setenv("PGDATABASE", "trader")
+    monkeypatch.setenv("PGSSLMODE", "require")
+
+    settings_module.reload_settings()
+    db = settings_module.get_database_settings()
+    assert db.primary_dsn is None
+    assembled = db.assembled_dsn
+    assert "user" in assembled
+    assert "p%40ss+word" in assembled
+    assert "db.local:6543" in assembled
+    assert db.effective_dsn() == assembled
+
+
+def test_reload_settings_returns_fresh_instance(monkeypatch):
+    monkeypatch.setenv("SENTRY_DSN", "https://public@sentry.example/1")
+    s1 = settings_module.get_settings()
+    s2 = settings_module.reload_settings()
+    assert s1.sentry.dsn == "https://public@sentry.example/1"
+    assert s2.sentry.dsn == "https://public@sentry.example/1"
+    assert s1 is not s2
+
+
+def test_market_data_settings(monkeypatch):
+    monkeypatch.setenv("ALPHAVANTAGE_API_KEY", "alpha-key")
+    monkeypatch.setenv("FINNHUB_API_KEY", "finn-key")
+    settings_module.reload_settings()
+    market = settings_module.get_market_data_settings()
+    assert market.alphavantage_key == "alpha-key"
+    assert market.finnhub_key == "finn-key"
+    assert market.has_alphavantage is True
+    assert market.has_finnhub is True
diff --git a/tests/unit/test_health_config.py b/tests/unit/test_health_config.py
index 2433ee1..d4058a5 100644
--- a/tests/unit/test_health_config.py
+++ b/tests/unit/test_health_config.py
@@ -1,6 +1,12 @@
+import logging
+import os
+
+import logging
+
 from starlette.testclient import TestClient
+
+from app.logging_utils import setup_logging
 from app.main import app
-import os
 
 client = TestClient(app)
 
@@ -27,4 +33,33 @@ def test_health_config_masks_and_status(monkeypatch):
     masked = data["env"]["TELEGRAM_BOT_TOKEN"]
     assert masked.startswith("AA")
     assert masked.endswith("ZZ")
-    assert "*" in masked
\ No newline at end of file
+    assert "*" in masked
+
+
+def test_request_logging_carries_request_id(caplog):
+    setup_logging(force=True, level="INFO")
+    request_id = "req-test-123"
+
+    records = []
+
+    class ListHandler(logging.Handler):
+        def emit(self, record):
+            records.append(record)
+
+    handler = ListHandler()
+    handler.setLevel(logging.INFO)
+    root = logging.getLogger()
+    prev_level = root.level
+    root.setLevel(logging.INFO)
+    root.addHandler(handler)
+    try:
+        resp = client.get("/health/live", headers={"X-Request-ID": request_id})
+
+        assert resp.status_code == 200
+        assert resp.headers["X-Request-ID"] == request_id
+
+        captured = [rec for rec in records if getattr(rec, "request_id", None) == request_id]
+        assert captured, "expected log record with request_id"
+    finally:
+        root.removeHandler(handler)
+        root.setLevel(prev_level)
diff --git a/tests/utils/test_env.py b/tests/utils/test_env.py
index f960204..b6ec336 100644
--- a/tests/utils/test_env.py
+++ b/tests/utils/test_env.py
@@ -25,6 +25,9 @@ def test_env_defaults_when_missing(monkeypatch):
         "TRADING_ENABLED",
         "PRICE_PROVIDERS",
         "HTTP_TIMEOUT_SECS",
+        "HTTP_TIMEOUT",
+        "HTTP_RETRIES",
+        "HTTP_BACKOFF",
     ]:
         monkeypatch.delenv(key, raising=False)
 
@@ -33,16 +36,40 @@ def test_env_defaults_when_missing(monkeypatch):
     assert env_module.TZ == "America/Los_Angeles"
     assert env_module.TRADING_ENABLED is False
     assert env_module.HTTP_TIMEOUT_SECS == 10
+    assert env_module.HTTP_TIMEOUT == 10
+    assert env_module.HTTP_RETRIES == 2
+    assert env_module.HTTP_BACKOFF == 1.5
     assert env_module.PRICE_PROVIDERS == ["alpaca", "yahoo"]
 
 
 def test_env_overrides(monkeypatch):
     monkeypatch.setenv("TRADING_ENABLED", "yes")
     monkeypatch.setenv("PRICE_PROVIDERS", "alpaca")
-    monkeypatch.setenv("HTTP_TIMEOUT_SECS", "15")
+    monkeypatch.setenv("HTTP_TIMEOUT", "15")
+    monkeypatch.setenv("HTTP_RETRIES", "5")
+    monkeypatch.setenv("HTTP_BACKOFF", "2.0")
+    monkeypatch.setenv("HTTP_TIMEOUT_SECS", "99")
 
     env_module = reload_env()
 
     assert env_module.TRADING_ENABLED is True
     assert env_module.PRICE_PROVIDERS == ["alpaca"]
     assert env_module.HTTP_TIMEOUT_SECS == 15
+    assert env_module.HTTP_TIMEOUT == 15
+    assert env_module.HTTP_RETRIES == 5
+    assert env_module.HTTP_BACKOFF == 2.0
+
+
+def test_env_legacy_http_settings(monkeypatch):
+    monkeypatch.delenv("HTTP_TIMEOUT", raising=False)
+    monkeypatch.delenv("HTTP_RETRIES", raising=False)
+    monkeypatch.delenv("HTTP_BACKOFF", raising=False)
+    monkeypatch.setenv("HTTP_TIMEOUT_SECS", "25")
+    monkeypatch.setenv("HTTP_RETRY_ATTEMPTS", "4")
+    monkeypatch.setenv("HTTP_RETRY_BACKOFF_SEC", "3.5")
+
+    env_module = reload_env()
+
+    assert env_module.HTTP_TIMEOUT == 25
+    assert env_module.HTTP_RETRIES == 4
+    assert env_module.HTTP_BACKOFF == 3.5
diff --git a/tests/utils/test_http.py b/tests/utils/test_http.py
index 0603521..bad4581 100644
--- a/tests/utils/test_http.py
+++ b/tests/utils/test_http.py
@@ -47,16 +47,18 @@ def test_http_get_retries_and_backoff(monkeypatch, caplog):
     perf_values = iter([0.0, 0.01, 1.0, 1.05])
     monkeypatch.setattr(http.time, "perf_counter", lambda: next(perf_values))
 
-    logger.remove()
-    logger.add(caplog.handler, level="INFO")
-
-    status, data = http.http_get("https://example.com/api")
-
-    assert status == 200
-    assert data == {"ok": True}
-    assert len(fake_requests.calls) == 2
-    assert sleeps == [1.5]
-    assert any(
-        "method=GET url=https://example.com/api status=500" in record.message
-        for record in caplog.records
-    )
+    handler_id = logger.add(caplog.handler, level="INFO")
+
+    try:
+        status, data = http.http_get("https://example.com/api")
+
+        assert status == 200
+        assert data == {"ok": True}
+        assert len(fake_requests.calls) == 2
+        assert sleeps == [1.5]
+        assert any(
+            "method=GET url=https://example.com/api status=500" in record.message
+            for record in caplog.records
+        )
+    finally:
+        logger.remove(handler_id)
diff --git a/ui/pages/Monitoring Dashboard.py b/ui/pages/Monitoring Dashboard.py
new file mode 100644
index 0000000..2dac876
--- /dev/null
+++ b/ui/pages/Monitoring Dashboard.py	
@@ -0,0 +1,18 @@
+from __future__ import annotations
+
+import runpy
+
+
+def main() -> None:
+    """
+    Thin wrapper that renders the existing monitoring dashboard as a Streamlit page.
+
+    This keeps the dashboard logic in `app/monitoring/dashboard.py` unchanged while
+    exposing it through Streamlit's native multipage routing.
+    """
+
+    runpy.run_module("app.monitoring.dashboard", run_name="__main__")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/ui/streamlit_app.py b/ui/streamlit_app.py
new file mode 100644
index 0000000..7bef7dd
--- /dev/null
+++ b/ui/streamlit_app.py
@@ -0,0 +1,565 @@
+from __future__ import annotations
+
+from datetime import datetime, timedelta, timezone
+from pathlib import Path
+from typing import Dict, Iterable, List, Optional, Tuple
+
+import altair as alt
+import numpy as np
+import pandas as pd
+import requests
+import streamlit as st
+
+try:
+    from dotenv import load_dotenv  # type: ignore
+except ImportError:
+    load_dotenv = None
+
+try:
+    import yfinance as yf  # type: ignore
+except ImportError:  # pragma: no cover - optional dependency
+    yf = None
+
+# Load local dotenv files before importing app configuration so ENV picks up overrides.
+if load_dotenv:
+    for candidate in (Path(".env.dev"), Path(".env")):
+        if candidate.exists():
+            load_dotenv(candidate, override=False)
+
+from app.adapters.market.alpaca_client import AlpacaAuthError, AlpacaMarketClient
+from app.services.watchlist_service import build_watchlist
+from app.utils import env as ENV
+
+FALLBACK_SYMBOLS = ["AAPL", "MSFT", "NVDA", "SPY", "QQQ"]
+
+# ---------------------------------------------------------------------------
+# Streamlit configuration
+# ---------------------------------------------------------------------------
+st.set_page_config(
+    page_title="AI Trader Console",
+    page_icon="🤖",
+    layout="wide",
+    initial_sidebar_state="expanded",
+)
+
+st.markdown(
+    """
+    <style>
+    :root { --accent-color: #38bdf8; }
+    html, body, [data-testid="stAppViewContainer"] {
+        background: linear-gradient(135deg, #0d1117 0%, #111a2b 45%, #0a0f1a 100%);
+        color: #e2e8f0 !important;
+    }
+    [data-testid="stSidebar"] {
+        background-color: rgba(15, 23, 42, 0.92) !important;
+        border-right: 1px solid rgba(148, 163, 184, 0.12) !important;
+    }
+    h1, h2, h3, h4, h5, h6, .stMarkdown p {
+        color: #f8fafc !important;
+    }
+    .glass-card {
+        background: rgba(17, 25, 40, 0.85);
+        border: 1px solid rgba(148, 163, 184, 0.12);
+        border-radius: 18px;
+        padding: 1.2rem 1.4rem;
+        box-shadow: 0 8px 28px rgba(8, 15, 30, 0.42);
+        backdrop-filter: blur(16px);
+    }
+    .stSelectbox, .stTextInput, .stTextArea, .stNumberInput, .stButton>button {
+        color: #f8fafc !important;
+        background-color: rgba(30, 41, 59, 0.8) !important;
+        border: 1px solid rgba(148, 163, 184, 0.32) !important;
+        border-radius: 10px !important;
+    }
+    .stButton>button:hover {
+        border-color: var(--accent-color) !important;
+        background-color: rgba(56, 189, 248, 0.25) !important;
+    }
+    .stDataFrame div, .stTable, .stMetric, .stCaption, .stTextInput>div>div>input {
+        color: #f8fafc !important;
+    }
+    .stTabs [role="tab"] {
+        color: rgba(226, 232, 240, 0.7) !important;
+        border-radius: 12px 12px 0 0 !important;
+        padding: 0.55rem 0.9rem !important;
+    }
+    .stTabs [role="tab"][aria-selected="true"] {
+        background-color: rgba(56, 189, 248, 0.18) !important;
+        color: #f8fafc !important;
+        border-bottom: 2px solid var(--accent-color) !important;
+    }
+    .summary-label {
+        text-transform: uppercase;
+        font-size: 0.75rem;
+        letter-spacing: 0.12em;
+        color: rgba(148, 163, 184, 0.9);
+    }
+    .summary-value {
+        font-size: 1.8rem;
+        font-weight: 700;
+        color: #f8fafc;
+    }
+    .summary-note {
+        font-size: 0.75rem;
+        color: rgba(148, 163, 184, 0.65);
+    }
+    .badge {
+        display: inline-flex;
+        padding: 0.28rem 0.65rem;
+        border-radius: 999px;
+        background: rgba(56, 189, 248, 0.2);
+        color: #bae6fd;
+        font-size: 0.7rem;
+        font-weight: 600;
+        letter-spacing: 0.08em;
+        text-transform: uppercase;
+    }
+    </style>
+    """,
+    unsafe_allow_html=True,
+)
+
+
+# ---------------------------------------------------------------------------
+# Helpers
+# ---------------------------------------------------------------------------
+def _alpaca_headers() -> Optional[Dict[str, str]]:
+    key = ENV.ALPACA_API_KEY
+    secret = ENV.ALPACA_API_SECRET
+    if not key or not secret:
+        return None
+    return {"APCA-API-KEY-ID": key, "APCA-API-SECRET-KEY": secret}
+
+
+def _format_timestamp(ts: Optional[str | int | float]) -> Optional[datetime]:
+    if ts is None:
+        return None
+    try:
+        if isinstance(ts, str):
+            return datetime.fromisoformat(ts.replace("Z", "+00:00")).astimezone(
+                timezone.utc
+            )
+        if isinstance(ts, (int, float)):
+            return datetime.fromtimestamp(float(ts), tz=timezone.utc)
+    except Exception:
+        return None
+    return None
+
+
+def _dedupe_manual(entries: str) -> List[str]:
+    parts = [token.strip().upper() for token in entries.split(",") if token.strip()]
+    seen = set()
+    ordered: List[str] = []
+    for sym in parts:
+        if sym in seen:
+            continue
+        seen.add(sym)
+        ordered.append(sym)
+    return ordered
+
+
+def _build_watchlist_frame(symbols: Iterable[str], snapshots: Dict[str, dict]) -> pd.DataFrame:
+    rows = []
+    for sym in symbols:
+        snap = snapshots.get(sym, {})
+        trade = snap.get("latestTrade") or {}
+        daily = snap.get("dailyBar") or {}
+        price = trade.get("price") or daily.get("c")
+        open_price = daily.get("o")
+
+        change = None
+        change_pct = None
+        if price is not None and open_price:
+            try:
+                change = float(price) - float(open_price)
+                change_pct = (change / float(open_price)) * 100.0 if open_price else None
+            except Exception:
+                change = None
+                change_pct = None
+
+        rows.append(
+            {
+                "Symbol": sym,
+                "Last Price": float(price) if price is not None else np.nan,
+                "Change": float(change) if change is not None else np.nan,
+                "% Change": float(change_pct) if change_pct is not None else np.nan,
+                "Updated": _format_timestamp(trade.get("timestamp")),
+            }
+        )
+    df = pd.DataFrame(rows)
+    return df
+
+
+def _render_watchlist_table(df: pd.DataFrame) -> None:
+    if df.empty:
+        st.info("Watchlist is empty. Adjust filters or verify the data source.")
+        return
+    styled = df.set_index("Symbol").style.format(
+        {
+            "Last Price": lambda v: "—" if pd.isna(v) else f"${v:,.2f}",
+            "Change": lambda v: "—" if pd.isna(v) else f"{v:+.2f}",
+            "% Change": lambda v: "—" if pd.isna(v) else f"{v:+.2f}%",
+            "Updated": lambda v: v.isoformat() if isinstance(v, datetime) else "—",
+        }
+    )
+
+    def _delta_style(value: float) -> Optional[str]:
+        if pd.isna(value) or value == 0:
+            return None
+        return "color:#34d399;" if value > 0 else "color:#f87171;"
+
+    styled = styled.map(_delta_style, subset=pd.IndexSlice[:, ["Change", "% Change"]])
+    st.dataframe(styled, width="stretch", height=380)
+
+
+def _render_symbol_detail(symbol: str, row: pd.Series, history: pd.DataFrame) -> None:
+    last_price_val = row.get("Last Price", np.nan)
+    last_price_display = "—" if pd.isna(last_price_val) else f"${last_price_val:,.2f}"
+    updated_val = row.get("Updated")
+    updated_display = (
+        updated_val.strftime("%Y-%m-%d %H:%M")
+        if isinstance(updated_val, datetime)
+        else "Timestamp n/a"
+    )
+
+    st.markdown(
+        f"""
+        <div class="glass-card" style="margin-bottom:1rem;">
+            <div style="display:flex;justify-content:space-between;align-items:center;">
+                <div>
+                    <span class="badge">Detail View</span>
+                    <h2 style="margin:0.35rem 0 0;">{symbol}</h2>
+                </div>
+                <div style="text-align:right;">
+                    <div class="summary-label">Last Price</div>
+                    <div class="summary-value">{last_price_display}</div>
+                    <div class="metric-subtext">{updated_display}</div>
+                </div>
+            </div>
+        </div>
+        """,
+        unsafe_allow_html=True,
+    )
+
+    cols = st.columns([2.2, 1])
+    with cols[0]:
+        timeframe = st.selectbox(
+            "Chart timeframe",
+            options=list(TIMEFRAME_CONFIG.keys()),
+            index=list(TIMEFRAME_CONFIG.keys()).index("1H"),
+            key=f"tf_{symbol}",
+        )
+        history = _load_bar_history(symbol, timeframe=timeframe)
+        if history.empty:
+            st.warning("No historical data available for this symbol.")
+        else:
+            history_plot = history.reset_index().rename(columns={"index": "timestamp"})
+            chart = (
+                alt.Chart(history_plot)
+                .mark_line(color="#60a5fa", strokeWidth=2)
+                .encode(
+                    x=alt.X("timestamp:T", title="Time"),
+                    y=alt.Y("close:Q", title="Close Price"),
+                    tooltip=[
+                        alt.Tooltip("timestamp:T", title="Timestamp"),
+                        alt.Tooltip("close:Q", title="Close", format=",.2f"),
+                    ],
+                )
+            )
+            st.altair_chart(chart.interactive(), width="stretch")
+    with cols[1]:
+        st.markdown("###### Session Snapshot")
+        change = row.get("Change", np.nan)
+        pct = row.get("% Change", np.nan)
+        st.metric(
+            label="Change",
+            value="—" if pd.isna(change) else f"{change:+.2f}",
+            delta="—" if pd.isna(pct) else f"{pct:+.2f}%",
+        )
+        st.caption(
+            "Charts sourced from Alpaca / Yahoo Finance fallback. "
+            "Historical candle resolution: 1H (configurable)."
+        )
+
+
+@st.cache_data(show_spinner=False, ttl=30)
+def _load_snapshots(symbols: Tuple[str, ...]) -> Tuple[Dict[str, dict], str]:
+    if not symbols:
+        return {}, "none"
+    snapshots, summary = get_market_snapshots(symbols)
+    return snapshots, summary
+
+
+TIMEFRAME_CONFIG = {
+    "1H": ("1Hour", 120),
+    "15m": ("15Min", 200),
+    "5m": ("5Min", 240),
+}
+
+
+@st.cache_data(show_spinner=False, ttl=300)
+def _load_bar_history(symbol: str, *, timeframe: str = "1H") -> pd.DataFrame:
+    df = get_intraday_bars(symbol, timeframe=timeframe)
+    if df.empty:
+        interval_minutes = 60 if timeframe == "1H" else 15 if timeframe == "15m" else 5
+        limit = TIMEFRAME_CONFIG.get(timeframe, ("1Hour", 120))[1]
+        index = pd.date_range(
+            datetime.now(timezone.utc) - timedelta(minutes=(limit - 1) * interval_minutes),
+            periods=limit,
+            freq=f"{interval_minutes}T",
+        )
+        base = 100.0 + np.linspace(0, 5, limit)
+        noise = np.random.normal(0, 0.7, size=limit)
+        df = pd.DataFrame({"close": base + noise}, index=index)
+    return df
+
+
+# ---------------------------------------------------------------------------
+# Sidebar watchlist builder
+# ---------------------------------------------------------------------------
+
+if "watchlist_state" not in st.session_state:
+    st.session_state.watchlist_state = {
+        "source": "auto",
+        "symbols": [],
+        "manual_raw": "",
+        "scanner": "",
+        "sort": "none",
+        "limit": min(ENV.MAX_WATCHLIST or 50, 25),
+    }
+
+state = st.session_state.watchlist_state
+
+st.sidebar.header("Watchlist Builder")
+source = st.sidebar.selectbox(
+    "Select source",
+    options=["auto", "finviz", "textlist", "manual"],
+    index=["auto", "finviz", "textlist", "manual"].index(state.get("source", "auto")),
+    format_func=lambda x: x.title(),
+    help="Choose how to construct the current watchlist.",
+)
+
+limit = st.sidebar.number_input(
+    "Max symbols",
+    min_value=5,
+    max_value=ENV.MAX_WATCHLIST or 100,
+    value=int(state.get("limit") or min(ENV.MAX_WATCHLIST or 50, 25)),
+    step=5,
+    help="Upper bound after deduping and filtering.",
+)
+
+sort = st.sidebar.selectbox(
+    "Sort order",
+    options=["none", "alpha"],
+    index=["none", "alpha"].index(state.get("sort", "none")),
+    help="Optional alphabetical sort before display.",
+)
+
+scanner = ""
+manual_input = ""
+if source in {"auto", "finviz", "textlist"}:
+    scanner = st.sidebar.text_input(
+        "Scanner (optional)",
+        value=state.get("scanner", ""),
+        help="Finviz/Textlist scanners are defined in app configuration.",
+    )
+else:
+    manual_input = st.sidebar.text_area(
+        "Manual symbols",
+        value=state.get("manual_raw", ""),
+        height=100,
+        help="Comma-separated tickers, e.g. AAPL, MSFT, NVDA.",
+    )
+
+apply_watchlist = st.sidebar.button("Apply Watchlist")
+
+def _resolve_watchlist(
+    selected_source: str,
+    *,
+    scanner_value: str,
+    manual_value: str,
+    limit_value: int | None,
+    sort_value: str,
+) -> Tuple[List[str], Optional[str]]:
+    symbols: List[str] = []
+    limit_norm = limit_value if limit_value and limit_value > 0 else None
+    note: Optional[str] = None
+
+    if selected_source == "manual":
+        symbols = _dedupe_manual(manual_value)
+    else:
+        symbols = build_watchlist(
+            source=selected_source,
+            scanner=scanner_value or None,
+            limit=limit_norm,
+            sort="alpha" if sort_value == "alpha" else None,
+        )
+
+    if selected_source == "manual" and sort_value == "alpha":
+        symbols = sorted(symbols)
+
+    if limit_norm:
+        symbols = symbols[:limit_norm]
+
+    if not symbols and selected_source != "manual":
+        fallback = FALLBACK_SYMBOLS[: limit_norm] if limit_norm else FALLBACK_SYMBOLS
+        symbols = fallback
+        note = (
+            "Watchlist source returned no symbols. Using demo tickers: "
+            + ", ".join(fallback)
+        )
+
+    return symbols, note
+
+
+source_note: Optional[str] = None
+if apply_watchlist or not state.get("symbols"):
+    symbols, source_note = _resolve_watchlist(
+        source,
+        scanner_value=scanner,
+        manual_value=manual_input,
+        limit_value=int(limit),
+        sort_value=sort,
+    )
+    state.update(
+        {
+            "source": source,
+            "symbols": symbols,
+            "scanner": scanner,
+            "manual_raw": manual_input,
+            "sort": sort,
+            "limit": int(limit),
+        }
+    )
+    _load_snapshots.clear()
+    _load_bar_history.clear()
+
+watchlist = state.get("symbols", [])
+
+if source_note:
+    st.sidebar.warning(source_note)
+
+st.sidebar.markdown("---")
+with st.sidebar.expander("Current Selection", expanded=True):
+    st.write(f"Source: `{state.get('source', 'auto')}`")
+    if state.get("source") == "manual":
+        st.caption("Manual entries applied.")
+    elif state.get("scanner"):
+        st.caption(f"Scanner preset: `{state.get('scanner')}`")
+    st.write(f"Symbols ({len(watchlist)}):")
+    st.code(", ".join(watchlist) if watchlist else "—")
+
+# Manual refresh button to re-pull snapshots/bars without changing config.
+refresh_data = st.sidebar.button("Refresh Market Data")
+if refresh_data:
+    _load_snapshots.clear()
+    _load_bar_history.clear()
+
+# ---------------------------------------------------------------------------
+# Data hydration & header
+# ---------------------------------------------------------------------------
+snapshots, snapshot_note = _load_snapshots(tuple(watchlist))
+watchlist_df = _build_watchlist_frame(watchlist, snapshots)
+
+if snapshot_note and "Yahoo Finance" in snapshot_note:
+    provider_label = "Yahoo Fallback"
+elif snapshot_note and "Alpaca" in snapshot_note:
+    provider_label = "Alpaca (degraded)"
+else:
+    provider_label = "Alpaca"
+
+if not watchlist_df.empty and watchlist_df["Updated"].notna().any():
+    last_updated = (
+        watchlist_df["Updated"].dropna().max()
+        if watchlist_df["Updated"].notna().any()
+        else datetime.now(timezone.utc)
+    )
+else:
+    last_updated = datetime.now(timezone.utc)
+
+header_note = snapshot_note or source_note
+
+st.markdown(
+    f"""
+    <div class="glass-card" style="margin-bottom:1.1rem;">
+        <div style="display:flex;justify-content:space-between;align-items:center;">
+            <div>
+                <span class="badge">Operations Console</span>
+                <h1 style="margin:0.35rem 0 0;">AI Trader — Market Pulse</h1>
+                <div class="summary-note" style="margin-top:0.25rem;">
+                    Curated watchlists, live snapshots, and symbol intelligence for intraday decisioning.
+                </div>
+            </div>
+            <div style="text-align:right;">
+                <div class="summary-note">Data source</div>
+                <div class="badge" style="margin-bottom:0.4rem;">{provider_label}</div>
+                <div class="summary-label">Last snapshot (UTC)</div>
+                <div class="summary-value" style="font-size:1.1rem;">{last_updated.strftime('%Y-%m-%d %H:%M:%S')}</div>
+            </div>
+        </div>
+    </div>
+    """,
+    unsafe_allow_html=True,
+)
+
+if header_note:
+    st.warning(header_note)
+
+stats_cols = st.columns(3)
+with stats_cols[0]:
+    st.markdown(
+        f'<div class="glass-card"><div class="summary-label">Tracked Symbols</div>'
+        f'<div class="summary-value">{len(watchlist_df)}</div>'
+        f'<div class="summary-note">Active in current watchlist</div></div>',
+        unsafe_allow_html=True,
+    )
+with stats_cols[1]:
+    coverage = watchlist_df["Last Price"].notna().sum()
+    st.markdown(
+        f'<div class="glass-card"><div class="summary-label">Price Coverage</div>'
+        f'<div class="summary-value">{coverage}</div>'
+        f'<div class="summary-note">Symbols with recent quotes</div></div>',
+        unsafe_allow_html=True,
+    )
+with stats_cols[2]:
+    mean_move = watchlist_df["% Change"].dropna().mean() if not watchlist_df.empty else 0.0
+    st.markdown(
+        f'<div class="glass-card"><div class="summary-label">Avg Session Move</div>'
+        f'<div class="summary-value">{mean_move:+.2f}%</div>'
+        f'<div class="summary-note">Across symbols with price data</div></div>',
+        unsafe_allow_html=True,
+    )
+
+# ---------------------------------------------------------------------------
+# Main tabs
+# ---------------------------------------------------------------------------
+tabs = st.tabs(["Watchlist", "Detail View", "Orders • Roadmap"])
+
+with tabs[0]:
+    st.markdown("#### Current Symbols")
+    _render_watchlist_table(watchlist_df)
+
+with tabs[1]:
+    if watchlist_df.empty:
+        st.info("Watchlist is empty. Populate via the sidebar to inspect details.")
+    else:
+        symbol = st.selectbox("Symbol focus", options=list(watchlist_df["Symbol"]), index=0)
+        row = watchlist_df.set_index("Symbol").loc[symbol]
+        history = _load_bar_history(symbol)
+        _render_symbol_detail(symbol, row, history)
+
+with tabs[2]:
+    st.markdown("#### Orders & Routing")
+    st.info(
+        "Order routing and execution preview will land here. "
+        "Integrate with OMS/EMS endpoints to surface staged orders and fills."
+    )
+    st.caption(
+        "Planned widgets: staged orders list, risk review, manual override toggles."
+    )
+
+st.sidebar.markdown("---")
+st.sidebar.caption(
+    "Environment sourced from Azure Key Vault references or local .env/.env.dev files. "
+    "Alpaca keys enable live market data; Yahoo fallback triggers when unavailable."
+)
